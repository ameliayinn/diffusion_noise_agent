bash scripts/script_normal_dif_test.sh 
[?2004l[2025-04-01 04:10:19,053] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:10:22,105] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 04:10:22,105] [INFO] [runner.py:605:main] cmd = /hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None main.py --image_size 8 --batch_size 256 --num_epochs 200 --timesteps 1000 --lr 2e-4 --time_emb_dim 256 --mu1 4 --sigma1 1 --num1 9000 --mu2 10 --sigma2 4 --num2 1000 --samples_dir ./samples --checkpoints_dir ./checkpoints --fp16 --mode train --model_path  --num_images 10000 --simulation_distribution normal --use_different_noise --use_moe --num_experts 4 --moe_hidden_dim 64 --moe_tau 0.1
[2025-04-01 04:10:23,846] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:10:26,070] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 04:10:26,070] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 04:10:26,070] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 04:10:26,070] [INFO] [launch.py:164:main] dist_world_size=1
[2025-04-01 04:10:26,070] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 04:10:26,187] [INFO] [launch.py:256:main] process 66018 spawned with command: ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1']
[2025-04-01 04:10:29,561] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Traceback (most recent call last):
  File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/main.py", line 9, in <module>
    train_deepspeed(cfg)
  File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py", line 52, in train_deepspeed
    model = UNetSimulation(time_emb_dim=config.time_emb_dim, image_size=config.image_size, num_experts=config.num_experts, moe_hidden_dim=config.moe_hidden_dim, moe_tau=config.moe_tau)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: UNetSimulation.__init__() got an unexpected keyword argument 'num_experts'
[2025-04-01 04:10:33,189] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 66018
[2025-04-01 04:10:33,190] [ERROR] [launch.py:325:sigkill_handler] ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1'] exits with return code = 1
[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ bash scripts/script_normal_dif_test.sh 
[?2004l[2025-04-01 04:11:06,523] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:11:08,795] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 04:11:08,795] [INFO] [runner.py:605:main] cmd = /hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None main.py --image_size 8 --batch_size 256 --num_epochs 200 --timesteps 1000 --lr 2e-4 --time_emb_dim 256 --mu1 4 --sigma1 1 --num1 9000 --mu2 10 --sigma2 4 --num2 1000 --samples_dir ./samples --checkpoints_dir ./checkpoints --fp16 --mode train --model_path  --num_images 10000 --simulation_distribution normal --use_different_noise --use_moe --num_experts 4 --moe_hidden_dim 64 --moe_tau 0.1
[2025-04-01 04:11:10,552] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:11:12,814] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 04:11:12,814] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 04:11:12,814] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 04:11:12,814] [INFO] [launch.py:164:main] dist_world_size=1
[2025-04-01 04:11:12,814] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 04:11:12,931] [INFO] [launch.py:256:main] process 66470 spawned with command: ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1']
[2025-04-01 04:11:15,718] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:11:18,939] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.16.5, git-hash=unknown, git-branch=unknown
[2025-04-01 04:11:18,939] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-01 04:11:18,939] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-04-01 04:11:18,942] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 1
[2025-04-01 04:11:20,121] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124/fused_adam/build.ninja...
/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.24651598930358887 seconds
[2025-04-01 04:11:20,372] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-04-01 04:11:20,372] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-04-01 04:11:20,377] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-04-01 04:11:20,377] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-04-01 04:11:20,377] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2025-04-01 04:11:20,378] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000
[2025-04-01 04:11:20,378] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000
[2025-04-01 04:11:20,378] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-04-01 04:11:20,378] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-04-01 04:11:20,761] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-04-01 04:11:20,762] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:11:20,762] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 66.99 GB, percent = 6.6%
[2025-04-01 04:11:20,882] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-04-01 04:11:20,883] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:11:20,883] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.0 GB, percent = 6.6%
[2025-04-01 04:11:20,883] [INFO] [stage_1_and_2.py:556:__init__] optimizer state initialized
[2025-04-01 04:11:21,001] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-04-01 04:11:21,002] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:11:21,002] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.0 GB, percent = 6.6%
[2025-04-01 04:11:21,004] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-04-01 04:11:21,004] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-04-01 04:11:21,004] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-04-01 04:11:21,004] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:11:21,004] [INFO] [config.py:1000:print] DeepSpeedEngine configuration:
[2025-04-01 04:11:21,005] [INFO] [config.py:1004:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-04-01 04:11:21,005] [INFO] [config.py:1004:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-04-01 04:11:21,005] [INFO] [config.py:1004:print]   amp_enabled .................. False
[2025-04-01 04:11:21,005] [INFO] [config.py:1004:print]   amp_params ................... False
[2025-04-01 04:11:21,005] [INFO] [config.py:1004:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-01 04:11:21,005] [INFO] [config.py:1004:print]   bfloat16_enabled ............. False
[2025-04-01 04:11:21,005] [INFO] [config.py:1004:print]   bfloat16_immediate_grad_update  False
[2025-04-01 04:11:21,005] [INFO] [config.py:1004:print]   checkpoint_parallel_write_pipeline  False
[2025-04-01 04:11:21,005] [INFO] [config.py:1004:print]   checkpoint_tag_validation_enabled  True
[2025-04-01 04:11:21,005] [INFO] [config.py:1004:print]   checkpoint_tag_validation_fail  False
[2025-04-01 04:11:21,005] [INFO] [config.py:1004:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f2ec490c440>
[2025-04-01 04:11:21,005] [INFO] [config.py:1004:print]   communication_data_type ...... None
[2025-04-01 04:11:21,005] [INFO] [config.py:1004:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-01 04:11:21,005] [INFO] [config.py:1004:print]   curriculum_enabled_legacy .... False
[2025-04-01 04:11:21,005] [INFO] [config.py:1004:print]   curriculum_params_legacy ..... False
[2025-04-01 04:11:21,005] [INFO] [config.py:1004:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-01 04:11:21,005] [INFO] [config.py:1004:print]   data_efficiency_enabled ...... False
[2025-04-01 04:11:21,005] [INFO] [config.py:1004:print]   dataloader_drop_last ......... False
[2025-04-01 04:11:21,005] [INFO] [config.py:1004:print]   disable_allgather ............ False
[2025-04-01 04:11:21,005] [INFO] [config.py:1004:print]   dump_state ................... False
[2025-04-01 04:11:21,005] [INFO] [config.py:1004:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-04-01 04:11:21,005] [INFO] [config.py:1004:print]   eigenvalue_enabled ........... False
[2025-04-01 04:11:21,005] [INFO] [config.py:1004:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-01 04:11:21,005] [INFO] [config.py:1004:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-01 04:11:21,005] [INFO] [config.py:1004:print]   eigenvalue_layer_num ......... 0
[2025-04-01 04:11:21,005] [INFO] [config.py:1004:print]   eigenvalue_max_iter .......... 100
[2025-04-01 04:11:21,005] [INFO] [config.py:1004:print]   eigenvalue_stability ......... 1e-06
[2025-04-01 04:11:21,005] [INFO] [config.py:1004:print]   eigenvalue_tol ............... 0.01
[2025-04-01 04:11:21,005] [INFO] [config.py:1004:print]   eigenvalue_verbose ........... False
[2025-04-01 04:11:21,005] [INFO] [config.py:1004:print]   elasticity_enabled ........... False
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   fp16_auto_cast ............... False
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   fp16_enabled ................. True
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   fp16_master_weights_and_gradients  False
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   global_rank .................. 0
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   grad_accum_dtype ............. None
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   gradient_accumulation_steps .. 1
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   gradient_clipping ............ 1.0
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   gradient_predivide_factor .... 1.0
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   graph_harvesting ............. False
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   initial_dynamic_scale ........ 65536
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   load_universal_checkpoint .... False
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   loss_scale ................... 0
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   memory_breakdown ............. False
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   mics_hierarchial_params_gather  False
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   mics_shard_size .............. -1
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   optimizer_legacy_fusion ...... False
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   optimizer_name ............... adamw
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   optimizer_params ............. {'lr': 0.0002, 'weight_decay': 0.01}
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   pld_enabled .................. False
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   pld_params ................... False
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   prescale_gradients ........... False
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   scheduler_name ............... None
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   scheduler_params ............. None
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   seq_parallel_communication_data_type  torch.float32
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   sparse_attention ............. None
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   sparse_gradients_enabled ..... False
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   steps_per_print .............. 50
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   timers_config ................ enabled=True synchronized=True
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   train_batch_size ............. 256
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   train_micro_batch_size_per_gpu  256
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   use_data_before_expert_parallel_  False
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   use_node_local_storage ....... False
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   wall_clock_breakdown ......... False
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   weight_quantization_config ... None
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   world_size ................... 1
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   zero_allow_untested_optimizer  False
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   zero_enabled ................. True
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-01 04:11:21,006] [INFO] [config.py:1004:print]   zero_optimization_stage ...... 2
[2025-04-01 04:11:21,007] [INFO] [config.py:990:print_user_config]   json = {
    "train_batch_size": 256, 
    "gradient_accumulation_steps": 1, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0002, 
            "weight_decay": 0.01
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 1000
    }, 
    "zero_optimization": {
        "stage": 2, 
        "contiguous_gradients": true, 
        "overlap_comm": true
    }, 
    "steps_per_print": 50, 
    "gradient_clipping": 1.0
}
****START TRAINING****
image_size: 8, batch_size: 256, timesteps: 1000, time_emb_dim: 256
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/main.py", line 9, in <module>
[rank0]:     train_deepspeed(cfg)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py", line 155, in train_deepspeed
[rank0]:     noisy_images, noise = forward_diffusion_with_moe(
[rank0]:                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/diffusion.py", line 95, in forward_diffusion_with_moe
[rank0]:     assert x0.dim() == 4, "输入必须是4D张量[B,C,H,W]"
[rank0]: AssertionError: 输入必须是4D张量[B,C,H,W]
[rank0]:[W401 04:11:21.729387750 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-04-01 04:11:22,932] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 66470
[2025-04-01 04:11:22,933] [ERROR] [launch.py:325:sigkill_handler] ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1'] exits with return code = 1
[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ bash scripts/script_normal_dif_test.sh 
[?2004l[2025-04-01 04:11:41,695] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:11:43,956] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 04:11:43,956] [INFO] [runner.py:605:main] cmd = /hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None main.py --image_size 8 --batch_size 256 --num_epochs 200 --timesteps 1000 --lr 2e-4 --time_emb_dim 256 --mu1 4 --sigma1 1 --num1 9000 --mu2 10 --sigma2 4 --num2 1000 --samples_dir ./samples --checkpoints_dir ./checkpoints --fp16 --mode train --model_path  --num_images 10000 --simulation_distribution normal --use_different_noise --use_moe --num_experts 4 --moe_hidden_dim 64 --moe_tau 0.1
[2025-04-01 04:11:45,655] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:11:47,855] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 04:11:47,855] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 04:11:47,855] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 04:11:47,855] [INFO] [launch.py:164:main] dist_world_size=1
[2025-04-01 04:11:47,855] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 04:11:47,969] [INFO] [launch.py:256:main] process 67002 spawned with command: ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1']
[2025-04-01 04:11:50,739] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:11:53,537] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.16.5, git-hash=unknown, git-branch=unknown
[2025-04-01 04:11:53,537] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-01 04:11:53,537] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-04-01 04:11:53,540] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 1
[2025-04-01 04:11:54,700] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124/fused_adam/build.ninja...
/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.19716668128967285 seconds
[2025-04-01 04:11:54,902] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-04-01 04:11:54,903] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-04-01 04:11:54,905] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-04-01 04:11:54,905] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-04-01 04:11:54,906] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2025-04-01 04:11:54,906] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000
[2025-04-01 04:11:54,906] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000
[2025-04-01 04:11:54,906] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-04-01 04:11:54,906] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-04-01 04:11:55,282] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-04-01 04:11:55,283] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:11:55,283] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 66.96 GB, percent = 6.6%
[2025-04-01 04:11:55,401] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-04-01 04:11:55,401] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:11:55,401] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 66.97 GB, percent = 6.6%
[2025-04-01 04:11:55,401] [INFO] [stage_1_and_2.py:556:__init__] optimizer state initialized
[2025-04-01 04:11:55,512] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-04-01 04:11:55,513] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:11:55,513] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 66.97 GB, percent = 6.6%
[2025-04-01 04:11:55,514] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-04-01 04:11:55,514] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-04-01 04:11:55,515] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-04-01 04:11:55,515] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:11:55,515] [INFO] [config.py:1000:print] DeepSpeedEngine configuration:
[2025-04-01 04:11:55,515] [INFO] [config.py:1004:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-04-01 04:11:55,515] [INFO] [config.py:1004:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-04-01 04:11:55,515] [INFO] [config.py:1004:print]   amp_enabled .................. False
[2025-04-01 04:11:55,515] [INFO] [config.py:1004:print]   amp_params ................... False
[2025-04-01 04:11:55,515] [INFO] [config.py:1004:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-01 04:11:55,515] [INFO] [config.py:1004:print]   bfloat16_enabled ............. False
[2025-04-01 04:11:55,515] [INFO] [config.py:1004:print]   bfloat16_immediate_grad_update  False
[2025-04-01 04:11:55,515] [INFO] [config.py:1004:print]   checkpoint_parallel_write_pipeline  False
[2025-04-01 04:11:55,515] [INFO] [config.py:1004:print]   checkpoint_tag_validation_enabled  True
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   checkpoint_tag_validation_fail  False
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fedd160a270>
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   communication_data_type ...... None
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   curriculum_enabled_legacy .... False
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   curriculum_params_legacy ..... False
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   data_efficiency_enabled ...... False
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   dataloader_drop_last ......... False
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   disable_allgather ............ False
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   dump_state ................... False
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   eigenvalue_enabled ........... False
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   eigenvalue_layer_num ......... 0
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   eigenvalue_max_iter .......... 100
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   eigenvalue_stability ......... 1e-06
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   eigenvalue_tol ............... 0.01
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   eigenvalue_verbose ........... False
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   elasticity_enabled ........... False
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   fp16_auto_cast ............... False
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   fp16_enabled ................. True
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   fp16_master_weights_and_gradients  False
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   global_rank .................. 0
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   grad_accum_dtype ............. None
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   gradient_accumulation_steps .. 1
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   gradient_clipping ............ 1.0
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   gradient_predivide_factor .... 1.0
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   graph_harvesting ............. False
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   initial_dynamic_scale ........ 65536
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   load_universal_checkpoint .... False
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   loss_scale ................... 0
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   memory_breakdown ............. False
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   mics_hierarchial_params_gather  False
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   mics_shard_size .............. -1
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   optimizer_legacy_fusion ...... False
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   optimizer_name ............... adamw
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   optimizer_params ............. {'lr': 0.0002, 'weight_decay': 0.01}
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   pld_enabled .................. False
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   pld_params ................... False
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   prescale_gradients ........... False
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   scheduler_name ............... None
[2025-04-01 04:11:55,516] [INFO] [config.py:1004:print]   scheduler_params ............. None
[2025-04-01 04:11:55,517] [INFO] [config.py:1004:print]   seq_parallel_communication_data_type  torch.float32
[2025-04-01 04:11:55,517] [INFO] [config.py:1004:print]   sparse_attention ............. None
[2025-04-01 04:11:55,517] [INFO] [config.py:1004:print]   sparse_gradients_enabled ..... False
[2025-04-01 04:11:55,517] [INFO] [config.py:1004:print]   steps_per_print .............. 50
[2025-04-01 04:11:55,517] [INFO] [config.py:1004:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-04-01 04:11:55,517] [INFO] [config.py:1004:print]   timers_config ................ enabled=True synchronized=True
[2025-04-01 04:11:55,517] [INFO] [config.py:1004:print]   train_batch_size ............. 256
[2025-04-01 04:11:55,517] [INFO] [config.py:1004:print]   train_micro_batch_size_per_gpu  256
[2025-04-01 04:11:55,517] [INFO] [config.py:1004:print]   use_data_before_expert_parallel_  False
[2025-04-01 04:11:55,517] [INFO] [config.py:1004:print]   use_node_local_storage ....... False
[2025-04-01 04:11:55,517] [INFO] [config.py:1004:print]   wall_clock_breakdown ......... False
[2025-04-01 04:11:55,517] [INFO] [config.py:1004:print]   weight_quantization_config ... None
[2025-04-01 04:11:55,517] [INFO] [config.py:1004:print]   world_size ................... 1
[2025-04-01 04:11:55,517] [INFO] [config.py:1004:print]   zero_allow_untested_optimizer  False
[2025-04-01 04:11:55,517] [INFO] [config.py:1004:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-04-01 04:11:55,517] [INFO] [config.py:1004:print]   zero_enabled ................. True
[2025-04-01 04:11:55,517] [INFO] [config.py:1004:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-01 04:11:55,517] [INFO] [config.py:1004:print]   zero_optimization_stage ...... 2
[2025-04-01 04:11:55,517] [INFO] [config.py:990:print_user_config]   json = {
    "train_batch_size": 256, 
    "gradient_accumulation_steps": 1, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0002, 
            "weight_decay": 0.01
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 1000
    }, 
    "zero_optimization": {
        "stage": 2, 
        "contiguous_gradients": true, 
        "overlap_comm": true
    }, 
    "steps_per_print": 50, 
    "gradient_clipping": 1.0
}
****START TRAINING****
image_size: 8, batch_size: 256, timesteps: 1000, time_emb_dim: 256
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/main.py", line 9, in <module>
[rank0]:     train_deepspeed(cfg)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py", line 155, in train_deepspeed
[rank0]:     noisy_images, noise = forward_diffusion_with_moe(
[rank0]:                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/diffusion.py", line 95, in forward_diffusion_with_moe
[rank0]:     assert x0.dim() == 4, "输入必须是4D张量[B,C,H,W]"
[rank0]: AssertionError: 输入必须是4D张量[B,C,H,W]
[rank0]:[W401 04:11:56.212157689 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-04-01 04:11:56,971] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 67002
[2025-04-01 04:11:56,972] [ERROR] [launch.py:325:sigkill_handler] ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1'] exits with return code = 1
[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ bash scripts/script_normal_dif_test.sh 
[?2004l[2025-04-01 04:13:06,838] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:13:09,151] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 04:13:09,152] [INFO] [runner.py:605:main] cmd = /hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None main.py --image_size 8 --batch_size 256 --num_epochs 200 --timesteps 1000 --lr 2e-4 --time_emb_dim 256 --mu1 4 --sigma1 1 --num1 9000 --mu2 10 --sigma2 4 --num2 1000 --samples_dir ./samples --checkpoints_dir ./checkpoints --fp16 --mode train --model_path  --num_images 10000 --simulation_distribution normal --use_different_noise --use_moe --num_experts 4 --moe_hidden_dim 64 --moe_tau 0.1
[2025-04-01 04:13:10,930] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:13:13,138] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 04:13:13,138] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 04:13:13,138] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 04:13:13,138] [INFO] [launch.py:164:main] dist_world_size=1
[2025-04-01 04:13:13,138] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 04:13:13,251] [INFO] [launch.py:256:main] process 67612 spawned with command: ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1']
[2025-04-01 04:13:15,996] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:13:18,933] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.16.5, git-hash=unknown, git-branch=unknown
[2025-04-01 04:13:18,933] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-01 04:13:18,933] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-04-01 04:13:18,936] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 1
[2025-04-01 04:13:19,989] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124/fused_adam/build.ninja...
/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.19227075576782227 seconds
[2025-04-01 04:13:20,187] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-04-01 04:13:20,187] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-04-01 04:13:20,190] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-04-01 04:13:20,190] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-04-01 04:13:20,190] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2025-04-01 04:13:20,191] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000
[2025-04-01 04:13:20,191] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000
[2025-04-01 04:13:20,191] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-04-01 04:13:20,191] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-04-01 04:13:20,480] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-04-01 04:13:20,480] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:13:20,480] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 66.97 GB, percent = 6.6%
[2025-04-01 04:13:20,590] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-04-01 04:13:20,590] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:13:20,590] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 66.97 GB, percent = 6.6%
[2025-04-01 04:13:20,590] [INFO] [stage_1_and_2.py:556:__init__] optimizer state initialized
[2025-04-01 04:13:20,694] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-04-01 04:13:20,694] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:13:20,694] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 66.97 GB, percent = 6.6%
[2025-04-01 04:13:20,696] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-04-01 04:13:20,696] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-04-01 04:13:20,696] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-04-01 04:13:20,696] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:13:20,696] [INFO] [config.py:1000:print] DeepSpeedEngine configuration:
[2025-04-01 04:13:20,696] [INFO] [config.py:1004:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-04-01 04:13:20,696] [INFO] [config.py:1004:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-04-01 04:13:20,696] [INFO] [config.py:1004:print]   amp_enabled .................. False
[2025-04-01 04:13:20,696] [INFO] [config.py:1004:print]   amp_params ................... False
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   bfloat16_enabled ............. False
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   bfloat16_immediate_grad_update  False
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   checkpoint_parallel_write_pipeline  False
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   checkpoint_tag_validation_enabled  True
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   checkpoint_tag_validation_fail  False
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f701c7f5820>
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   communication_data_type ...... None
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   curriculum_enabled_legacy .... False
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   curriculum_params_legacy ..... False
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   data_efficiency_enabled ...... False
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   dataloader_drop_last ......... False
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   disable_allgather ............ False
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   dump_state ................... False
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   eigenvalue_enabled ........... False
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   eigenvalue_layer_num ......... 0
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   eigenvalue_max_iter .......... 100
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   eigenvalue_stability ......... 1e-06
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   eigenvalue_tol ............... 0.01
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   eigenvalue_verbose ........... False
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   elasticity_enabled ........... False
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   fp16_auto_cast ............... False
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   fp16_enabled ................. True
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   fp16_master_weights_and_gradients  False
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   global_rank .................. 0
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   grad_accum_dtype ............. None
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   gradient_accumulation_steps .. 1
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   gradient_clipping ............ 1.0
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   gradient_predivide_factor .... 1.0
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   graph_harvesting ............. False
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   initial_dynamic_scale ........ 65536
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   load_universal_checkpoint .... False
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   loss_scale ................... 0
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   memory_breakdown ............. False
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   mics_hierarchial_params_gather  False
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   mics_shard_size .............. -1
[2025-04-01 04:13:20,697] [INFO] [config.py:1004:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-04-01 04:13:20,698] [INFO] [config.py:1004:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-04-01 04:13:20,698] [INFO] [config.py:1004:print]   optimizer_legacy_fusion ...... False
[2025-04-01 04:13:20,698] [INFO] [config.py:1004:print]   optimizer_name ............... adamw
[2025-04-01 04:13:20,698] [INFO] [config.py:1004:print]   optimizer_params ............. {'lr': 0.0002, 'weight_decay': 0.01}
[2025-04-01 04:13:20,698] [INFO] [config.py:1004:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-04-01 04:13:20,698] [INFO] [config.py:1004:print]   pld_enabled .................. False
[2025-04-01 04:13:20,698] [INFO] [config.py:1004:print]   pld_params ................... False
[2025-04-01 04:13:20,698] [INFO] [config.py:1004:print]   prescale_gradients ........... False
[2025-04-01 04:13:20,698] [INFO] [config.py:1004:print]   scheduler_name ............... None
[2025-04-01 04:13:20,698] [INFO] [config.py:1004:print]   scheduler_params ............. None
[2025-04-01 04:13:20,698] [INFO] [config.py:1004:print]   seq_parallel_communication_data_type  torch.float32
[2025-04-01 04:13:20,698] [INFO] [config.py:1004:print]   sparse_attention ............. None
[2025-04-01 04:13:20,698] [INFO] [config.py:1004:print]   sparse_gradients_enabled ..... False
[2025-04-01 04:13:20,698] [INFO] [config.py:1004:print]   steps_per_print .............. 50
[2025-04-01 04:13:20,698] [INFO] [config.py:1004:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-04-01 04:13:20,698] [INFO] [config.py:1004:print]   timers_config ................ enabled=True synchronized=True
[2025-04-01 04:13:20,698] [INFO] [config.py:1004:print]   train_batch_size ............. 256
[2025-04-01 04:13:20,698] [INFO] [config.py:1004:print]   train_micro_batch_size_per_gpu  256
[2025-04-01 04:13:20,698] [INFO] [config.py:1004:print]   use_data_before_expert_parallel_  False
[2025-04-01 04:13:20,698] [INFO] [config.py:1004:print]   use_node_local_storage ....... False
[2025-04-01 04:13:20,698] [INFO] [config.py:1004:print]   wall_clock_breakdown ......... False
[2025-04-01 04:13:20,698] [INFO] [config.py:1004:print]   weight_quantization_config ... None
[2025-04-01 04:13:20,698] [INFO] [config.py:1004:print]   world_size ................... 1
[2025-04-01 04:13:20,698] [INFO] [config.py:1004:print]   zero_allow_untested_optimizer  False
[2025-04-01 04:13:20,698] [INFO] [config.py:1004:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-04-01 04:13:20,698] [INFO] [config.py:1004:print]   zero_enabled ................. True
[2025-04-01 04:13:20,698] [INFO] [config.py:1004:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-01 04:13:20,698] [INFO] [config.py:1004:print]   zero_optimization_stage ...... 2
[2025-04-01 04:13:20,698] [INFO] [config.py:990:print_user_config]   json = {
    "train_batch_size": 256, 
    "gradient_accumulation_steps": 1, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0002, 
            "weight_decay": 0.01
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 1000
    }, 
    "zero_optimization": {
        "stage": 2, 
        "contiguous_gradients": true, 
        "overlap_comm": true
    }, 
    "steps_per_print": 50, 
    "gradient_clipping": 1.0
}
****START TRAINING****
image_size: 8, batch_size: 256, timesteps: 1000, time_emb_dim: 256
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s][2025-04-01 04:13:21,725] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:03,  1.02it/s][2025-04-01 04:13:21,778] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:01<00:00,  3.02it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.84it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.56it/s]
/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py:190: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True) # 使用 pd.concat 追加数据
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.79it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00, 10.08it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.39it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.76it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.67it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.75it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.26it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.62it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.74it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.83it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.32it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.68it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.76it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.75it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.16it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.59it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.84it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.84it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.23it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.65it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.83it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00, 10.03it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.44it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.79it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.76it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.77it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.28it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.65it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.72it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00, 10.07it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.22it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.68it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.79it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.90it/s][2025-04-01 04:13:26,956] [INFO] [logging.py:107:log_dist] [Rank 0] step=50, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:13:26,958] [INFO] [timer.py:264:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=3067.1148365341505, CurrSamplesPerSec=4240.31482292166, MemAllocated=0.02GB, MaxMemAllocated=1.9GB
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.35it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.72it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.77it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.85it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.24it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.66it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.59it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.69it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.10it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.50it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.76it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00, 10.04it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.48it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.80it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.77it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.78it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.19it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.60it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.85it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.79it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.20it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.63it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.66it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.65it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.08it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.50it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.59it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.84it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.14it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.53it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.76it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.87it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.35it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.70it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.79it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.77it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.27it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.65it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.77it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.85it/s][2025-04-01 04:13:32,397] [INFO] [logging.py:107:log_dist] [Rank 0] step=100, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:13:32,399] [INFO] [timer.py:264:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=3096.451403388583, CurrSamplesPerSec=4074.470105051667, MemAllocated=0.02GB, MaxMemAllocated=1.9GB
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.31it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.69it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/main.py", line 9, in <module>
[rank0]:     train_deepspeed(cfg)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py", line 210, in train_deepspeed
[rank0]:     generate_during_training(model_engine, sample_dir, config, epoch, num_images=config.num_images//config.image_size)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/generate.py", line 227, in generate_during_training_simulation_dif
[rank0]:     x = model_engine.module.moe.sample_initial_noise(num_images, config)
[rank0]:         ^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1928, in __getattr__
[rank0]:     raise AttributeError(
[rank0]: AttributeError: 'UNetSimulation' object has no attribute 'moe'
[rank0]:[W401 04:13:33.019571513 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-04-01 04:13:34,254] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 67612
[2025-04-01 04:13:34,255] [ERROR] [launch.py:325:sigkill_handler] ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1'] exits with return code = 1
[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ bash scripts/script_normal_dif_test.sh 
[?2004l[2025-04-01 04:16:17,010] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:16:19,260] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 04:16:19,260] [INFO] [runner.py:605:main] cmd = /hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None main.py --image_size 8 --batch_size 256 --num_epochs 200 --timesteps 1000 --lr 2e-4 --time_emb_dim 256 --mu1 4 --sigma1 1 --num1 9000 --mu2 10 --sigma2 4 --num2 1000 --samples_dir ./samples --checkpoints_dir ./checkpoints --fp16 --mode train --model_path  --num_images 10000 --simulation_distribution normal --use_different_noise --use_moe --num_experts 4 --moe_hidden_dim 64 --moe_tau 0.1
[2025-04-01 04:16:21,067] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:16:23,283] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 04:16:23,283] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 04:16:23,283] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 04:16:23,283] [INFO] [launch.py:164:main] dist_world_size=1
[2025-04-01 04:16:23,283] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 04:16:23,397] [INFO] [launch.py:256:main] process 68529 spawned with command: ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1']
[2025-04-01 04:16:26,083] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:16:29,134] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.16.5, git-hash=unknown, git-branch=unknown
[2025-04-01 04:16:29,135] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-01 04:16:29,135] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-04-01 04:16:29,138] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 1
[2025-04-01 04:16:30,351] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124/fused_adam/build.ninja...
/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.190338134765625 seconds
[2025-04-01 04:16:30,547] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-04-01 04:16:30,548] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-04-01 04:16:30,552] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-04-01 04:16:30,553] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-04-01 04:16:30,553] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2025-04-01 04:16:30,553] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000
[2025-04-01 04:16:30,553] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000
[2025-04-01 04:16:30,553] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-04-01 04:16:30,554] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-04-01 04:16:30,976] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-04-01 04:16:30,977] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:16:30,977] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 66.94 GB, percent = 6.6%
[2025-04-01 04:16:31,097] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-04-01 04:16:31,098] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:16:31,098] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 66.95 GB, percent = 6.6%
[2025-04-01 04:16:31,098] [INFO] [stage_1_and_2.py:556:__init__] optimizer state initialized
[2025-04-01 04:16:31,212] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-04-01 04:16:31,213] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:16:31,213] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 66.95 GB, percent = 6.6%
[2025-04-01 04:16:31,215] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-04-01 04:16:31,215] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-04-01 04:16:31,215] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-04-01 04:16:31,215] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:16:31,215] [INFO] [config.py:1000:print] DeepSpeedEngine configuration:
[2025-04-01 04:16:31,215] [INFO] [config.py:1004:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-04-01 04:16:31,215] [INFO] [config.py:1004:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-04-01 04:16:31,215] [INFO] [config.py:1004:print]   amp_enabled .................. False
[2025-04-01 04:16:31,215] [INFO] [config.py:1004:print]   amp_params ................... False
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   bfloat16_enabled ............. False
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   bfloat16_immediate_grad_update  False
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   checkpoint_parallel_write_pipeline  False
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   checkpoint_tag_validation_enabled  True
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   checkpoint_tag_validation_fail  False
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fb9593093d0>
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   communication_data_type ...... None
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   curriculum_enabled_legacy .... False
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   curriculum_params_legacy ..... False
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   data_efficiency_enabled ...... False
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   dataloader_drop_last ......... False
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   disable_allgather ............ False
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   dump_state ................... False
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   eigenvalue_enabled ........... False
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   eigenvalue_layer_num ......... 0
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   eigenvalue_max_iter .......... 100
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   eigenvalue_stability ......... 1e-06
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   eigenvalue_tol ............... 0.01
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   eigenvalue_verbose ........... False
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   elasticity_enabled ........... False
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   fp16_auto_cast ............... False
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   fp16_enabled ................. True
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   fp16_master_weights_and_gradients  False
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   global_rank .................. 0
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   grad_accum_dtype ............. None
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   gradient_accumulation_steps .. 1
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   gradient_clipping ............ 1.0
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   gradient_predivide_factor .... 1.0
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   graph_harvesting ............. False
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   initial_dynamic_scale ........ 65536
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   load_universal_checkpoint .... False
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   loss_scale ................... 0
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   memory_breakdown ............. False
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   mics_hierarchial_params_gather  False
[2025-04-01 04:16:31,216] [INFO] [config.py:1004:print]   mics_shard_size .............. -1
[2025-04-01 04:16:31,217] [INFO] [config.py:1004:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-04-01 04:16:31,217] [INFO] [config.py:1004:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-04-01 04:16:31,217] [INFO] [config.py:1004:print]   optimizer_legacy_fusion ...... False
[2025-04-01 04:16:31,217] [INFO] [config.py:1004:print]   optimizer_name ............... adamw
[2025-04-01 04:16:31,217] [INFO] [config.py:1004:print]   optimizer_params ............. {'lr': 0.0002, 'weight_decay': 0.01}
[2025-04-01 04:16:31,217] [INFO] [config.py:1004:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-04-01 04:16:31,217] [INFO] [config.py:1004:print]   pld_enabled .................. False
[2025-04-01 04:16:31,217] [INFO] [config.py:1004:print]   pld_params ................... False
[2025-04-01 04:16:31,217] [INFO] [config.py:1004:print]   prescale_gradients ........... False
[2025-04-01 04:16:31,217] [INFO] [config.py:1004:print]   scheduler_name ............... None
[2025-04-01 04:16:31,217] [INFO] [config.py:1004:print]   scheduler_params ............. None
[2025-04-01 04:16:31,217] [INFO] [config.py:1004:print]   seq_parallel_communication_data_type  torch.float32
[2025-04-01 04:16:31,217] [INFO] [config.py:1004:print]   sparse_attention ............. None
[2025-04-01 04:16:31,217] [INFO] [config.py:1004:print]   sparse_gradients_enabled ..... False
[2025-04-01 04:16:31,217] [INFO] [config.py:1004:print]   steps_per_print .............. 50
[2025-04-01 04:16:31,217] [INFO] [config.py:1004:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-04-01 04:16:31,217] [INFO] [config.py:1004:print]   timers_config ................ enabled=True synchronized=True
[2025-04-01 04:16:31,217] [INFO] [config.py:1004:print]   train_batch_size ............. 256
[2025-04-01 04:16:31,217] [INFO] [config.py:1004:print]   train_micro_batch_size_per_gpu  256
[2025-04-01 04:16:31,217] [INFO] [config.py:1004:print]   use_data_before_expert_parallel_  False
[2025-04-01 04:16:31,217] [INFO] [config.py:1004:print]   use_node_local_storage ....... False
[2025-04-01 04:16:31,217] [INFO] [config.py:1004:print]   wall_clock_breakdown ......... False
[2025-04-01 04:16:31,217] [INFO] [config.py:1004:print]   weight_quantization_config ... None
[2025-04-01 04:16:31,217] [INFO] [config.py:1004:print]   world_size ................... 1
[2025-04-01 04:16:31,217] [INFO] [config.py:1004:print]   zero_allow_untested_optimizer  False
[2025-04-01 04:16:31,217] [INFO] [config.py:1004:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-04-01 04:16:31,217] [INFO] [config.py:1004:print]   zero_enabled ................. True
[2025-04-01 04:16:31,217] [INFO] [config.py:1004:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-01 04:16:31,217] [INFO] [config.py:1004:print]   zero_optimization_stage ...... 2
[2025-04-01 04:16:31,217] [INFO] [config.py:990:print_user_config]   json = {
    "train_batch_size": 256, 
    "gradient_accumulation_steps": 1, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0002, 
            "weight_decay": 0.01
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 1000
    }, 
    "zero_optimization": {
        "stage": 2, 
        "contiguous_gradients": true, 
        "overlap_comm": true
    }, 
    "steps_per_print": 50, 
    "gradient_clipping": 1.0
}
****START TRAINING****
image_size: 8, batch_size: 256, timesteps: 1000, time_emb_dim: 256
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s][2025-04-01 04:16:32,253] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:03,  1.03it/s][2025-04-01 04:16:32,319] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:01<00:00,  3.08it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.77it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.55it/s]
/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py:190: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True) # 使用 pd.concat 追加数据
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.32it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.63it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.47it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.41it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.40it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.63it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.61it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.49it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.40it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.64it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.55it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.44it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.33it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.74it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.68it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.51it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.33it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.69it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.65it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.50it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.37it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.68it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.43it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.39it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.25it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.49it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.51it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.38it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.60it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.78it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.71it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.60it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.26it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.44it/s][2025-04-01 04:16:38,190] [INFO] [logging.py:107:log_dist] [Rank 0] step=50, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:16:38,191] [INFO] [timer.py:264:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=2649.576113534738, CurrSamplesPerSec=3306.494737387013, MemAllocated=0.02GB, MaxMemAllocated=1.9GB
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.32it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.30it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.28it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.59it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.52it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.39it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.32it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.57it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.58it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.46it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.37it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.69it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.65it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.53it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.37it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.68it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.70it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.55it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.49it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.81it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.74it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.79it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.44it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.70it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.51it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.47it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.42it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.60it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.62it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.50it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.28it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.47it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.56it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.41it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.38it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.01it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.81it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.64it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.46it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.70it/s][2025-04-01 04:16:44,322] [INFO] [logging.py:107:log_dist] [Rank 0] step=100, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:16:44,323] [INFO] [timer.py:264:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=2671.1629671730857, CurrSamplesPerSec=3171.7850101748604, MemAllocated=0.02GB, MaxMemAllocated=1.9GB
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.36it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.38it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/main.py", line 9, in <module>
[rank0]:     train_deepspeed(cfg)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py", line 210, in train_deepspeed
[rank0]:     generate_during_training(model_engine, sample_dir, config, epoch, num_images=config.num_images//config.image_size)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/generate.py", line 240, in generate_during_training_simulation_dif
[rank0]:     pred_noise = model_engine(x, t_batch)
[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/deepspeed/runtime/engine.py", line 2030, in forward
[rank0]:     loss = self.module(*inputs, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank0]:     return inner()
[rank0]:            ^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/unet.py", line 257, in forward
[rank0]:     assert x.dim() == 4, f"输入必须是4D张量，但得到 {x.shape}"
[rank0]: AssertionError: 输入必须是4D张量，但得到 torch.Size([1250, 1])
[rank0]:[W401 04:16:45.975249434 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-04-01 04:16:46,400] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 68529
[2025-04-01 04:16:46,401] [ERROR] [launch.py:325:sigkill_handler] ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1'] exits with return code = 1
[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ bash scripts/script_normal_dif_test.sh 
[?2004l[2025-04-01 04:17:49,716] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:17:52,016] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 04:17:52,017] [INFO] [runner.py:605:main] cmd = /hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None main.py --image_size 8 --batch_size 256 --num_epochs 200 --timesteps 1000 --lr 2e-4 --time_emb_dim 256 --mu1 4 --sigma1 1 --num1 9000 --mu2 10 --sigma2 4 --num2 1000 --samples_dir ./samples --checkpoints_dir ./checkpoints --fp16 --mode train --model_path  --num_images 10000 --simulation_distribution normal --use_different_noise --use_moe --num_experts 4 --moe_hidden_dim 64 --moe_tau 0.1
[2025-04-01 04:17:53,752] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:17:55,975] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 04:17:55,975] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 04:17:55,975] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 04:17:55,975] [INFO] [launch.py:164:main] dist_world_size=1
[2025-04-01 04:17:55,975] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 04:17:56,087] [INFO] [launch.py:256:main] process 69418 spawned with command: ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1']
[2025-04-01 04:17:58,841] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:18:01,840] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.16.5, git-hash=unknown, git-branch=unknown
[2025-04-01 04:18:01,840] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-01 04:18:01,840] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-04-01 04:18:01,844] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 1
[2025-04-01 04:18:03,042] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124/fused_adam/build.ninja...
/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.19310784339904785 seconds
[2025-04-01 04:18:03,240] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-04-01 04:18:03,240] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-04-01 04:18:03,245] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-04-01 04:18:03,245] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-04-01 04:18:03,245] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2025-04-01 04:18:03,246] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000
[2025-04-01 04:18:03,246] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000
[2025-04-01 04:18:03,246] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-04-01 04:18:03,246] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-04-01 04:18:03,742] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-04-01 04:18:03,743] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:18:03,743] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 66.98 GB, percent = 6.6%
[2025-04-01 04:18:03,863] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-04-01 04:18:03,864] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:18:03,864] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 66.99 GB, percent = 6.6%
[2025-04-01 04:18:03,864] [INFO] [stage_1_and_2.py:556:__init__] optimizer state initialized
[2025-04-01 04:18:03,993] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-04-01 04:18:03,994] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:18:03,994] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 66.99 GB, percent = 6.6%
[2025-04-01 04:18:03,996] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-04-01 04:18:03,996] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-04-01 04:18:03,996] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-04-01 04:18:03,996] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:18:03,996] [INFO] [config.py:1000:print] DeepSpeedEngine configuration:
[2025-04-01 04:18:03,996] [INFO] [config.py:1004:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-04-01 04:18:03,996] [INFO] [config.py:1004:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-04-01 04:18:03,996] [INFO] [config.py:1004:print]   amp_enabled .................. False
[2025-04-01 04:18:03,996] [INFO] [config.py:1004:print]   amp_params ................... False
[2025-04-01 04:18:03,997] [INFO] [config.py:1004:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-01 04:18:03,997] [INFO] [config.py:1004:print]   bfloat16_enabled ............. False
[2025-04-01 04:18:03,997] [INFO] [config.py:1004:print]   bfloat16_immediate_grad_update  False
[2025-04-01 04:18:03,997] [INFO] [config.py:1004:print]   checkpoint_parallel_write_pipeline  False
[2025-04-01 04:18:03,997] [INFO] [config.py:1004:print]   checkpoint_tag_validation_enabled  True
[2025-04-01 04:18:03,997] [INFO] [config.py:1004:print]   checkpoint_tag_validation_fail  False
[2025-04-01 04:18:03,997] [INFO] [config.py:1004:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7faad2925b20>
[2025-04-01 04:18:03,997] [INFO] [config.py:1004:print]   communication_data_type ...... None
[2025-04-01 04:18:03,997] [INFO] [config.py:1004:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-01 04:18:03,997] [INFO] [config.py:1004:print]   curriculum_enabled_legacy .... False
[2025-04-01 04:18:03,997] [INFO] [config.py:1004:print]   curriculum_params_legacy ..... False
[2025-04-01 04:18:03,997] [INFO] [config.py:1004:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-01 04:18:03,997] [INFO] [config.py:1004:print]   data_efficiency_enabled ...... False
[2025-04-01 04:18:03,997] [INFO] [config.py:1004:print]   dataloader_drop_last ......... False
[2025-04-01 04:18:03,997] [INFO] [config.py:1004:print]   disable_allgather ............ False
[2025-04-01 04:18:03,997] [INFO] [config.py:1004:print]   dump_state ................... False
[2025-04-01 04:18:03,997] [INFO] [config.py:1004:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-04-01 04:18:03,997] [INFO] [config.py:1004:print]   eigenvalue_enabled ........... False
[2025-04-01 04:18:03,997] [INFO] [config.py:1004:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-01 04:18:03,997] [INFO] [config.py:1004:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-01 04:18:03,997] [INFO] [config.py:1004:print]   eigenvalue_layer_num ......... 0
[2025-04-01 04:18:03,997] [INFO] [config.py:1004:print]   eigenvalue_max_iter .......... 100
[2025-04-01 04:18:03,997] [INFO] [config.py:1004:print]   eigenvalue_stability ......... 1e-06
[2025-04-01 04:18:03,997] [INFO] [config.py:1004:print]   eigenvalue_tol ............... 0.01
[2025-04-01 04:18:03,997] [INFO] [config.py:1004:print]   eigenvalue_verbose ........... False
[2025-04-01 04:18:03,997] [INFO] [config.py:1004:print]   elasticity_enabled ........... False
[2025-04-01 04:18:03,997] [INFO] [config.py:1004:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-04-01 04:18:03,997] [INFO] [config.py:1004:print]   fp16_auto_cast ............... False
[2025-04-01 04:18:03,997] [INFO] [config.py:1004:print]   fp16_enabled ................. True
[2025-04-01 04:18:03,997] [INFO] [config.py:1004:print]   fp16_master_weights_and_gradients  False
[2025-04-01 04:18:03,997] [INFO] [config.py:1004:print]   global_rank .................. 0
[2025-04-01 04:18:03,997] [INFO] [config.py:1004:print]   grad_accum_dtype ............. None
[2025-04-01 04:18:03,997] [INFO] [config.py:1004:print]   gradient_accumulation_steps .. 1
[2025-04-01 04:18:03,997] [INFO] [config.py:1004:print]   gradient_clipping ............ 1.0
[2025-04-01 04:18:03,997] [INFO] [config.py:1004:print]   gradient_predivide_factor .... 1.0
[2025-04-01 04:18:03,997] [INFO] [config.py:1004:print]   graph_harvesting ............. False
[2025-04-01 04:18:03,997] [INFO] [config.py:1004:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-01 04:18:03,997] [INFO] [config.py:1004:print]   initial_dynamic_scale ........ 65536
[2025-04-01 04:18:03,998] [INFO] [config.py:1004:print]   load_universal_checkpoint .... False
[2025-04-01 04:18:03,998] [INFO] [config.py:1004:print]   loss_scale ................... 0
[2025-04-01 04:18:03,998] [INFO] [config.py:1004:print]   memory_breakdown ............. False
[2025-04-01 04:18:03,998] [INFO] [config.py:1004:print]   mics_hierarchial_params_gather  False
[2025-04-01 04:18:03,998] [INFO] [config.py:1004:print]   mics_shard_size .............. -1
[2025-04-01 04:18:03,998] [INFO] [config.py:1004:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-04-01 04:18:03,998] [INFO] [config.py:1004:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-04-01 04:18:03,998] [INFO] [config.py:1004:print]   optimizer_legacy_fusion ...... False
[2025-04-01 04:18:03,998] [INFO] [config.py:1004:print]   optimizer_name ............... adamw
[2025-04-01 04:18:03,998] [INFO] [config.py:1004:print]   optimizer_params ............. {'lr': 0.0002, 'weight_decay': 0.01}
[2025-04-01 04:18:03,998] [INFO] [config.py:1004:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-04-01 04:18:03,998] [INFO] [config.py:1004:print]   pld_enabled .................. False
[2025-04-01 04:18:03,998] [INFO] [config.py:1004:print]   pld_params ................... False
[2025-04-01 04:18:03,998] [INFO] [config.py:1004:print]   prescale_gradients ........... False
[2025-04-01 04:18:03,998] [INFO] [config.py:1004:print]   scheduler_name ............... None
[2025-04-01 04:18:03,998] [INFO] [config.py:1004:print]   scheduler_params ............. None
[2025-04-01 04:18:03,998] [INFO] [config.py:1004:print]   seq_parallel_communication_data_type  torch.float32
[2025-04-01 04:18:03,998] [INFO] [config.py:1004:print]   sparse_attention ............. None
[2025-04-01 04:18:03,998] [INFO] [config.py:1004:print]   sparse_gradients_enabled ..... False
[2025-04-01 04:18:03,998] [INFO] [config.py:1004:print]   steps_per_print .............. 50
[2025-04-01 04:18:03,998] [INFO] [config.py:1004:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-04-01 04:18:03,998] [INFO] [config.py:1004:print]   timers_config ................ enabled=True synchronized=True
[2025-04-01 04:18:03,998] [INFO] [config.py:1004:print]   train_batch_size ............. 256
[2025-04-01 04:18:03,998] [INFO] [config.py:1004:print]   train_micro_batch_size_per_gpu  256
[2025-04-01 04:18:03,998] [INFO] [config.py:1004:print]   use_data_before_expert_parallel_  False
[2025-04-01 04:18:03,998] [INFO] [config.py:1004:print]   use_node_local_storage ....... False
[2025-04-01 04:18:03,998] [INFO] [config.py:1004:print]   wall_clock_breakdown ......... False
[2025-04-01 04:18:03,998] [INFO] [config.py:1004:print]   weight_quantization_config ... None
[2025-04-01 04:18:03,998] [INFO] [config.py:1004:print]   world_size ................... 1
[2025-04-01 04:18:03,998] [INFO] [config.py:1004:print]   zero_allow_untested_optimizer  False
[2025-04-01 04:18:03,998] [INFO] [config.py:1004:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-04-01 04:18:03,998] [INFO] [config.py:1004:print]   zero_enabled ................. True
[2025-04-01 04:18:03,998] [INFO] [config.py:1004:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-01 04:18:03,998] [INFO] [config.py:1004:print]   zero_optimization_stage ...... 2
[2025-04-01 04:18:03,998] [INFO] [config.py:990:print_user_config]   json = {
    "train_batch_size": 256, 
    "gradient_accumulation_steps": 1, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0002, 
            "weight_decay": 0.01
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 1000
    }, 
    "zero_optimization": {
        "stage": 2, 
        "contiguous_gradients": true, 
        "overlap_comm": true
    }, 
    "steps_per_print": 50, 
    "gradient_clipping": 1.0
}
****START TRAINING****
image_size: 8, batch_size: 256, timesteps: 1000, time_emb_dim: 256
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s][2025-04-01 04:18:05,034] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:03,  1.03it/s][2025-04-01 04:18:05,103] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:01<00:00,  3.02it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.68it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.49it/s]
/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py:190: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True) # 使用 pd.concat 追加数据
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.83it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.02it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.73it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.72it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.26it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.40it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.25it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.24it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.12it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.31it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.26it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.18it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.47it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.61it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.47it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.42it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.38it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.51it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.27it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.29it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.35it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.55it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.37it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.33it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.33it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.77it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.80it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.56it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.30it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.70it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.76it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.53it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.39it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.65it/s][2025-04-01 04:18:11,018] [INFO] [logging.py:107:log_dist] [Rank 0] step=50, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:18:11,019] [INFO] [timer.py:264:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=2637.580504109237, CurrSamplesPerSec=3341.740036744796, MemAllocated=0.02GB, MaxMemAllocated=1.9GB
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.62it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.47it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.31it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.61it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.34it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.41it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.32it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.52it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.27it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.30it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.39it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.59it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.40it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.36it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.35it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.49it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.52it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.40it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.15it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.34it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.15it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.17it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.36it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.76it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.79it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.55it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.36it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.54it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.55it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.41it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.82it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.05it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.71it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.69it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.36it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.60it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.33it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.33it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.25it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.39it/s][2025-04-01 04:18:17,250] [INFO] [logging.py:107:log_dist] [Rank 0] step=100, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:18:17,251] [INFO] [timer.py:264:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=2644.493010483458, CurrSamplesPerSec=3322.5890829660816, MemAllocated=0.02GB, MaxMemAllocated=1.9GB
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.23it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.29it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/main.py", line 9, in <module>
[rank0]:     train_deepspeed(cfg)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py", line 210, in train_deepspeed
[rank0]:     generate_during_training(model_engine, sample_dir, config, epoch, num_images=config.num_images//config.image_size)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/generate.py", line 240, in generate_during_training_simulation_dif
[rank0]:     pred_noise = model_engine(x, t_batch)
[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/deepspeed/runtime/engine.py", line 2030, in forward
[rank0]:     loss = self.module(*inputs, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank0]:     return inner()
[rank0]:            ^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/unet.py", line 264, in forward
[rank0]:     x1 = F.gelu(self.conv1(x))  # [B,64,64,64]
[rank0]:                 ^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/conv.py", line 554, in forward
[rank0]:     return self._conv_forward(input, self.weight, self.bias)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/conv.py", line 549, in _conv_forward
[rank0]:     return F.conv2d(
[rank0]:            ^^^^^^^^^
[rank0]: RuntimeError: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1250, 1]
[rank0]:[W401 04:18:17.907741725 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-04-01 04:18:19,090] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 69418
[2025-04-01 04:18:19,091] [ERROR] [launch.py:325:sigkill_handler] ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1'] exits with return code = 1
[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ bash scripts/script_normal_dif_test.sh 
[?2004l[2025-04-01 04:21:45,464] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:21:47,780] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 04:21:47,780] [INFO] [runner.py:605:main] cmd = /hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None main.py --image_size 8 --batch_size 256 --num_epochs 200 --timesteps 1000 --lr 2e-4 --time_emb_dim 256 --mu1 4 --sigma1 1 --num1 9000 --mu2 10 --sigma2 4 --num2 1000 --samples_dir ./samples --checkpoints_dir ./checkpoints --fp16 --mode train --model_path  --num_images 10000 --simulation_distribution normal --use_different_noise --use_moe --num_experts 4 --moe_hidden_dim 64 --moe_tau 0.1
[2025-04-01 04:21:49,522] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:21:51,861] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 04:21:51,861] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 04:21:51,861] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 04:21:51,861] [INFO] [launch.py:164:main] dist_world_size=1
[2025-04-01 04:21:51,861] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 04:21:51,978] [INFO] [launch.py:256:main] process 70364 spawned with command: ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1']
[2025-04-01 04:21:54,884] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:21:57,836] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.16.5, git-hash=unknown, git-branch=unknown
[2025-04-01 04:21:57,836] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-01 04:21:57,836] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-04-01 04:21:57,839] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 1
[2025-04-01 04:21:59,065] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124/fused_adam/build.ninja...
/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.19611620903015137 seconds
[2025-04-01 04:21:59,267] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-04-01 04:21:59,267] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-04-01 04:21:59,271] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-04-01 04:21:59,272] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-04-01 04:21:59,272] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2025-04-01 04:21:59,272] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000
[2025-04-01 04:21:59,272] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000
[2025-04-01 04:21:59,272] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-04-01 04:21:59,272] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-04-01 04:21:59,728] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-04-01 04:21:59,729] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:21:59,729] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.05 GB, percent = 6.7%
[2025-04-01 04:21:59,843] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-04-01 04:21:59,843] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:21:59,844] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.06 GB, percent = 6.7%
[2025-04-01 04:21:59,844] [INFO] [stage_1_and_2.py:556:__init__] optimizer state initialized
[2025-04-01 04:21:59,950] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-04-01 04:21:59,951] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:21:59,951] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.06 GB, percent = 6.7%
[2025-04-01 04:21:59,953] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-04-01 04:21:59,953] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-04-01 04:21:59,953] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-04-01 04:21:59,953] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:21:59,953] [INFO] [config.py:1000:print] DeepSpeedEngine configuration:
[2025-04-01 04:21:59,954] [INFO] [config.py:1004:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-04-01 04:21:59,954] [INFO] [config.py:1004:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-04-01 04:21:59,954] [INFO] [config.py:1004:print]   amp_enabled .................. False
[2025-04-01 04:21:59,954] [INFO] [config.py:1004:print]   amp_params ................... False
[2025-04-01 04:21:59,954] [INFO] [config.py:1004:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-01 04:21:59,954] [INFO] [config.py:1004:print]   bfloat16_enabled ............. False
[2025-04-01 04:21:59,954] [INFO] [config.py:1004:print]   bfloat16_immediate_grad_update  False
[2025-04-01 04:21:59,954] [INFO] [config.py:1004:print]   checkpoint_parallel_write_pipeline  False
[2025-04-01 04:21:59,954] [INFO] [config.py:1004:print]   checkpoint_tag_validation_enabled  True
[2025-04-01 04:21:59,954] [INFO] [config.py:1004:print]   checkpoint_tag_validation_fail  False
[2025-04-01 04:21:59,954] [INFO] [config.py:1004:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fd35d29ba10>
[2025-04-01 04:21:59,954] [INFO] [config.py:1004:print]   communication_data_type ...... None
[2025-04-01 04:21:59,954] [INFO] [config.py:1004:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-01 04:21:59,954] [INFO] [config.py:1004:print]   curriculum_enabled_legacy .... False
[2025-04-01 04:21:59,954] [INFO] [config.py:1004:print]   curriculum_params_legacy ..... False
[2025-04-01 04:21:59,954] [INFO] [config.py:1004:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-01 04:21:59,954] [INFO] [config.py:1004:print]   data_efficiency_enabled ...... False
[2025-04-01 04:21:59,954] [INFO] [config.py:1004:print]   dataloader_drop_last ......... False
[2025-04-01 04:21:59,954] [INFO] [config.py:1004:print]   disable_allgather ............ False
[2025-04-01 04:21:59,954] [INFO] [config.py:1004:print]   dump_state ................... False
[2025-04-01 04:21:59,954] [INFO] [config.py:1004:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-04-01 04:21:59,954] [INFO] [config.py:1004:print]   eigenvalue_enabled ........... False
[2025-04-01 04:21:59,954] [INFO] [config.py:1004:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-01 04:21:59,954] [INFO] [config.py:1004:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-01 04:21:59,954] [INFO] [config.py:1004:print]   eigenvalue_layer_num ......... 0
[2025-04-01 04:21:59,954] [INFO] [config.py:1004:print]   eigenvalue_max_iter .......... 100
[2025-04-01 04:21:59,954] [INFO] [config.py:1004:print]   eigenvalue_stability ......... 1e-06
[2025-04-01 04:21:59,954] [INFO] [config.py:1004:print]   eigenvalue_tol ............... 0.01
[2025-04-01 04:21:59,954] [INFO] [config.py:1004:print]   eigenvalue_verbose ........... False
[2025-04-01 04:21:59,954] [INFO] [config.py:1004:print]   elasticity_enabled ........... False
[2025-04-01 04:21:59,954] [INFO] [config.py:1004:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   fp16_auto_cast ............... False
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   fp16_enabled ................. True
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   fp16_master_weights_and_gradients  False
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   global_rank .................. 0
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   grad_accum_dtype ............. None
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   gradient_accumulation_steps .. 1
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   gradient_clipping ............ 1.0
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   gradient_predivide_factor .... 1.0
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   graph_harvesting ............. False
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   initial_dynamic_scale ........ 65536
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   load_universal_checkpoint .... False
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   loss_scale ................... 0
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   memory_breakdown ............. False
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   mics_hierarchial_params_gather  False
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   mics_shard_size .............. -1
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   optimizer_legacy_fusion ...... False
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   optimizer_name ............... adamw
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   optimizer_params ............. {'lr': 0.0002, 'weight_decay': 0.01}
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   pld_enabled .................. False
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   pld_params ................... False
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   prescale_gradients ........... False
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   scheduler_name ............... None
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   scheduler_params ............. None
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   seq_parallel_communication_data_type  torch.float32
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   sparse_attention ............. None
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   sparse_gradients_enabled ..... False
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   steps_per_print .............. 50
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   timers_config ................ enabled=True synchronized=True
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   train_batch_size ............. 256
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   train_micro_batch_size_per_gpu  256
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   use_data_before_expert_parallel_  False
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   use_node_local_storage ....... False
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   wall_clock_breakdown ......... False
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   weight_quantization_config ... None
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   world_size ................... 1
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   zero_allow_untested_optimizer  False
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   zero_enabled ................. True
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-01 04:21:59,955] [INFO] [config.py:1004:print]   zero_optimization_stage ...... 2
[2025-04-01 04:21:59,955] [INFO] [config.py:990:print_user_config]   json = {
    "train_batch_size": 256, 
    "gradient_accumulation_steps": 1, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0002, 
            "weight_decay": 0.01
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 1000
    }, 
    "zero_optimization": {
        "stage": 2, 
        "contiguous_gradients": true, 
        "overlap_comm": true
    }, 
    "steps_per_print": 50, 
    "gradient_clipping": 1.0
}
****START TRAINING****
image_size: 8, batch_size: 256, timesteps: 1000, time_emb_dim: 256
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/main.py", line 9, in <module>
[rank0]:     train_deepspeed(cfg)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py", line 155, in train_deepspeed
[rank0]:     noisy_images, noise = forward_diffusion_with_moe(
[rank0]:                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/diffusion.py", line 99, in forward_diffusion_with_moe
[rank0]:     pred_params = model(x0, t)  # 输出已包含混合高斯的采样结果
[rank0]:                   ^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank0]:     return inner()
[rank0]:            ^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/unet.py", line 300, in forward
[rank0]:     output = moe_out.reshape(B, H, W, C).permute(0, 3, 1, 2)  # [B, C, H, W]
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: RuntimeError: shape '[256, 8, 8, 1]' is invalid for input of size 256
[rank0]:[W401 04:22:01.198043251 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-04-01 04:22:01,980] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 70364
[2025-04-01 04:22:01,981] [ERROR] [launch.py:325:sigkill_handler] ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1'] exits with return code = 1
[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ bash scripts/script_normal_dif_test.sh 
[?2004l[2025-04-01 04:22:58,056] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:23:00,294] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 04:23:00,294] [INFO] [runner.py:605:main] cmd = /hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None main.py --image_size 8 --batch_size 256 --num_epochs 200 --timesteps 1000 --lr 2e-4 --time_emb_dim 256 --mu1 4 --sigma1 1 --num1 9000 --mu2 10 --sigma2 4 --num2 1000 --samples_dir ./samples --checkpoints_dir ./checkpoints --fp16 --mode train --model_path  --num_images 10000 --simulation_distribution normal --use_different_noise --use_moe --num_experts 4 --moe_hidden_dim 64 --moe_tau 0.1
[2025-04-01 04:23:02,025] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:23:04,393] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 04:23:04,393] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 04:23:04,393] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 04:23:04,393] [INFO] [launch.py:164:main] dist_world_size=1
[2025-04-01 04:23:04,394] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 04:23:04,512] [INFO] [launch.py:256:main] process 70918 spawned with command: ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1']
[2025-04-01 04:23:07,346] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:23:10,341] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.16.5, git-hash=unknown, git-branch=unknown
[2025-04-01 04:23:10,341] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-01 04:23:10,341] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-04-01 04:23:10,344] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 1
[2025-04-01 04:23:11,566] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124/fused_adam/build.ninja...
/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.17174077033996582 seconds
[2025-04-01 04:23:11,743] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-04-01 04:23:11,743] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-04-01 04:23:11,748] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-04-01 04:23:11,748] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-04-01 04:23:11,748] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2025-04-01 04:23:11,749] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000
[2025-04-01 04:23:11,749] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000
[2025-04-01 04:23:11,749] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-04-01 04:23:11,749] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-04-01 04:23:12,168] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-04-01 04:23:12,169] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:23:12,169] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.02 GB, percent = 6.7%
[2025-04-01 04:23:12,284] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-04-01 04:23:12,285] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:23:12,285] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.02 GB, percent = 6.7%
[2025-04-01 04:23:12,285] [INFO] [stage_1_and_2.py:556:__init__] optimizer state initialized
[2025-04-01 04:23:12,402] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-04-01 04:23:12,402] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:23:12,403] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.02 GB, percent = 6.7%
[2025-04-01 04:23:12,404] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-04-01 04:23:12,404] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-04-01 04:23:12,404] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-04-01 04:23:12,404] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:23:12,405] [INFO] [config.py:1000:print] DeepSpeedEngine configuration:
[2025-04-01 04:23:12,405] [INFO] [config.py:1004:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-04-01 04:23:12,405] [INFO] [config.py:1004:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-04-01 04:23:12,405] [INFO] [config.py:1004:print]   amp_enabled .................. False
[2025-04-01 04:23:12,405] [INFO] [config.py:1004:print]   amp_params ................... False
[2025-04-01 04:23:12,405] [INFO] [config.py:1004:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-01 04:23:12,405] [INFO] [config.py:1004:print]   bfloat16_enabled ............. False
[2025-04-01 04:23:12,405] [INFO] [config.py:1004:print]   bfloat16_immediate_grad_update  False
[2025-04-01 04:23:12,405] [INFO] [config.py:1004:print]   checkpoint_parallel_write_pipeline  False
[2025-04-01 04:23:12,405] [INFO] [config.py:1004:print]   checkpoint_tag_validation_enabled  True
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   checkpoint_tag_validation_fail  False
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f77b5e1a0c0>
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   communication_data_type ...... None
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   curriculum_enabled_legacy .... False
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   curriculum_params_legacy ..... False
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   data_efficiency_enabled ...... False
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   dataloader_drop_last ......... False
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   disable_allgather ............ False
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   dump_state ................... False
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   eigenvalue_enabled ........... False
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   eigenvalue_layer_num ......... 0
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   eigenvalue_max_iter .......... 100
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   eigenvalue_stability ......... 1e-06
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   eigenvalue_tol ............... 0.01
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   eigenvalue_verbose ........... False
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   elasticity_enabled ........... False
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   fp16_auto_cast ............... False
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   fp16_enabled ................. True
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   fp16_master_weights_and_gradients  False
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   global_rank .................. 0
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   grad_accum_dtype ............. None
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   gradient_accumulation_steps .. 1
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   gradient_clipping ............ 1.0
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   gradient_predivide_factor .... 1.0
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   graph_harvesting ............. False
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   initial_dynamic_scale ........ 65536
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   load_universal_checkpoint .... False
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   loss_scale ................... 0
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   memory_breakdown ............. False
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   mics_hierarchial_params_gather  False
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   mics_shard_size .............. -1
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   optimizer_legacy_fusion ...... False
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   optimizer_name ............... adamw
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   optimizer_params ............. {'lr': 0.0002, 'weight_decay': 0.01}
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   pld_enabled .................. False
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   pld_params ................... False
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   prescale_gradients ........... False
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   scheduler_name ............... None
[2025-04-01 04:23:12,406] [INFO] [config.py:1004:print]   scheduler_params ............. None
[2025-04-01 04:23:12,407] [INFO] [config.py:1004:print]   seq_parallel_communication_data_type  torch.float32
[2025-04-01 04:23:12,407] [INFO] [config.py:1004:print]   sparse_attention ............. None
[2025-04-01 04:23:12,407] [INFO] [config.py:1004:print]   sparse_gradients_enabled ..... False
[2025-04-01 04:23:12,407] [INFO] [config.py:1004:print]   steps_per_print .............. 50
[2025-04-01 04:23:12,407] [INFO] [config.py:1004:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-04-01 04:23:12,407] [INFO] [config.py:1004:print]   timers_config ................ enabled=True synchronized=True
[2025-04-01 04:23:12,407] [INFO] [config.py:1004:print]   train_batch_size ............. 256
[2025-04-01 04:23:12,407] [INFO] [config.py:1004:print]   train_micro_batch_size_per_gpu  256
[2025-04-01 04:23:12,407] [INFO] [config.py:1004:print]   use_data_before_expert_parallel_  False
[2025-04-01 04:23:12,407] [INFO] [config.py:1004:print]   use_node_local_storage ....... False
[2025-04-01 04:23:12,407] [INFO] [config.py:1004:print]   wall_clock_breakdown ......... False
[2025-04-01 04:23:12,407] [INFO] [config.py:1004:print]   weight_quantization_config ... None
[2025-04-01 04:23:12,407] [INFO] [config.py:1004:print]   world_size ................... 1
[2025-04-01 04:23:12,407] [INFO] [config.py:1004:print]   zero_allow_untested_optimizer  False
[2025-04-01 04:23:12,407] [INFO] [config.py:1004:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-04-01 04:23:12,407] [INFO] [config.py:1004:print]   zero_enabled ................. True
[2025-04-01 04:23:12,407] [INFO] [config.py:1004:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-01 04:23:12,407] [INFO] [config.py:1004:print]   zero_optimization_stage ...... 2
[2025-04-01 04:23:12,407] [INFO] [config.py:990:print_user_config]   json = {
    "train_batch_size": 256, 
    "gradient_accumulation_steps": 1, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0002, 
            "weight_decay": 0.01
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 1000
    }, 
    "zero_optimization": {
        "stage": 2, 
        "contiguous_gradients": true, 
        "overlap_comm": true
    }, 
    "steps_per_print": 50, 
    "gradient_clipping": 1.0
}
****START TRAINING****
image_size: 8, batch_size: 256, timesteps: 1000, time_emb_dim: 256
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/main.py", line 9, in <module>
[rank0]:     train_deepspeed(cfg)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py", line 155, in train_deepspeed
[rank0]:     noisy_images, noise = forward_diffusion_with_moe(
[rank0]:                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/diffusion.py", line 99, in forward_diffusion_with_moe
[rank0]:     pred_params = model(x0, t)  # 输出已包含混合高斯的采样结果
[rank0]:                   ^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank0]:     return inner()
[rank0]:            ^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/unet.py", line 300, in forward
[rank0]:     output = moe_out.reshape(B, H, W, C).permute(0, 3, 1, 2)  # [B, C, H, W]
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: RuntimeError: shape '[256, 8, 8, 1]' is invalid for input of size 256
[rank0]:[W401 04:23:13.617293283 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-04-01 04:23:14,514] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 70918
[2025-04-01 04:23:14,515] [ERROR] [launch.py:325:sigkill_handler] ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1'] exits with return code = 1
[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ bash scripts/script_normal_dif_test.sh 
[?2004l[2025-04-01 04:23:43,365] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:23:45,608] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 04:23:45,608] [INFO] [runner.py:605:main] cmd = /hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None main.py --image_size 8 --batch_size 256 --num_epochs 200 --timesteps 1000 --lr 2e-4 --time_emb_dim 256 --mu1 4 --sigma1 1 --num1 9000 --mu2 10 --sigma2 4 --num2 1000 --samples_dir ./samples --checkpoints_dir ./checkpoints --fp16 --mode train --model_path  --num_images 10000 --simulation_distribution normal --use_different_noise --use_moe --num_experts 4 --moe_hidden_dim 64 --moe_tau 0.1
[2025-04-01 04:23:47,404] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:23:49,602] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 04:23:49,602] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 04:23:49,602] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 04:23:49,602] [INFO] [launch.py:164:main] dist_world_size=1
[2025-04-01 04:23:49,602] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 04:23:49,716] [INFO] [launch.py:256:main] process 71457 spawned with command: ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1']
[2025-04-01 04:23:52,538] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:23:55,340] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.16.5, git-hash=unknown, git-branch=unknown
[2025-04-01 04:23:55,340] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-01 04:23:55,340] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-04-01 04:23:55,350] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 1
[2025-04-01 04:23:56,548] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124/fused_adam/build.ninja...
/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.18926620483398438 seconds
[2025-04-01 04:23:56,742] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-04-01 04:23:56,743] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-04-01 04:23:56,747] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-04-01 04:23:56,748] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-04-01 04:23:56,748] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2025-04-01 04:23:56,748] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000
[2025-04-01 04:23:56,748] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000
[2025-04-01 04:23:56,748] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-04-01 04:23:56,748] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-04-01 04:23:57,177] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-04-01 04:23:57,178] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:23:57,178] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.04 GB, percent = 6.7%
[2025-04-01 04:23:57,307] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-04-01 04:23:57,308] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:23:57,308] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.04 GB, percent = 6.7%
[2025-04-01 04:23:57,308] [INFO] [stage_1_and_2.py:556:__init__] optimizer state initialized
[2025-04-01 04:23:57,423] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-04-01 04:23:57,423] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:23:57,423] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.05 GB, percent = 6.7%
[2025-04-01 04:23:57,425] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-04-01 04:23:57,425] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-04-01 04:23:57,425] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-04-01 04:23:57,425] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:23:57,426] [INFO] [config.py:1000:print] DeepSpeedEngine configuration:
[2025-04-01 04:23:57,426] [INFO] [config.py:1004:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-04-01 04:23:57,426] [INFO] [config.py:1004:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-04-01 04:23:57,426] [INFO] [config.py:1004:print]   amp_enabled .................. False
[2025-04-01 04:23:57,426] [INFO] [config.py:1004:print]   amp_params ................... False
[2025-04-01 04:23:57,426] [INFO] [config.py:1004:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-01 04:23:57,426] [INFO] [config.py:1004:print]   bfloat16_enabled ............. False
[2025-04-01 04:23:57,426] [INFO] [config.py:1004:print]   bfloat16_immediate_grad_update  False
[2025-04-01 04:23:57,426] [INFO] [config.py:1004:print]   checkpoint_parallel_write_pipeline  False
[2025-04-01 04:23:57,426] [INFO] [config.py:1004:print]   checkpoint_tag_validation_enabled  True
[2025-04-01 04:23:57,426] [INFO] [config.py:1004:print]   checkpoint_tag_validation_fail  False
[2025-04-01 04:23:57,426] [INFO] [config.py:1004:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fbd1cf9db20>
[2025-04-01 04:23:57,426] [INFO] [config.py:1004:print]   communication_data_type ...... None
[2025-04-01 04:23:57,426] [INFO] [config.py:1004:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-01 04:23:57,426] [INFO] [config.py:1004:print]   curriculum_enabled_legacy .... False
[2025-04-01 04:23:57,426] [INFO] [config.py:1004:print]   curriculum_params_legacy ..... False
[2025-04-01 04:23:57,426] [INFO] [config.py:1004:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   data_efficiency_enabled ...... False
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   dataloader_drop_last ......... False
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   disable_allgather ............ False
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   dump_state ................... False
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   eigenvalue_enabled ........... False
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   eigenvalue_layer_num ......... 0
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   eigenvalue_max_iter .......... 100
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   eigenvalue_stability ......... 1e-06
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   eigenvalue_tol ............... 0.01
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   eigenvalue_verbose ........... False
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   elasticity_enabled ........... False
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   fp16_auto_cast ............... False
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   fp16_enabled ................. True
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   fp16_master_weights_and_gradients  False
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   global_rank .................. 0
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   grad_accum_dtype ............. None
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   gradient_accumulation_steps .. 1
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   gradient_clipping ............ 1.0
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   gradient_predivide_factor .... 1.0
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   graph_harvesting ............. False
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   initial_dynamic_scale ........ 65536
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   load_universal_checkpoint .... False
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   loss_scale ................... 0
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   memory_breakdown ............. False
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   mics_hierarchial_params_gather  False
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   mics_shard_size .............. -1
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   optimizer_legacy_fusion ...... False
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   optimizer_name ............... adamw
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   optimizer_params ............. {'lr': 0.0002, 'weight_decay': 0.01}
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   pld_enabled .................. False
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   pld_params ................... False
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   prescale_gradients ........... False
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   scheduler_name ............... None
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   scheduler_params ............. None
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   seq_parallel_communication_data_type  torch.float32
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   sparse_attention ............. None
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   sparse_gradients_enabled ..... False
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   steps_per_print .............. 50
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-04-01 04:23:57,427] [INFO] [config.py:1004:print]   timers_config ................ enabled=True synchronized=True
[2025-04-01 04:23:57,428] [INFO] [config.py:1004:print]   train_batch_size ............. 256
[2025-04-01 04:23:57,428] [INFO] [config.py:1004:print]   train_micro_batch_size_per_gpu  256
[2025-04-01 04:23:57,428] [INFO] [config.py:1004:print]   use_data_before_expert_parallel_  False
[2025-04-01 04:23:57,428] [INFO] [config.py:1004:print]   use_node_local_storage ....... False
[2025-04-01 04:23:57,428] [INFO] [config.py:1004:print]   wall_clock_breakdown ......... False
[2025-04-01 04:23:57,428] [INFO] [config.py:1004:print]   weight_quantization_config ... None
[2025-04-01 04:23:57,428] [INFO] [config.py:1004:print]   world_size ................... 1
[2025-04-01 04:23:57,428] [INFO] [config.py:1004:print]   zero_allow_untested_optimizer  False
[2025-04-01 04:23:57,428] [INFO] [config.py:1004:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-04-01 04:23:57,428] [INFO] [config.py:1004:print]   zero_enabled ................. True
[2025-04-01 04:23:57,428] [INFO] [config.py:1004:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-01 04:23:57,428] [INFO] [config.py:1004:print]   zero_optimization_stage ...... 2
[2025-04-01 04:23:57,428] [INFO] [config.py:990:print_user_config]   json = {
    "train_batch_size": 256, 
    "gradient_accumulation_steps": 1, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0002, 
            "weight_decay": 0.01
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 1000
    }, 
    "zero_optimization": {
        "stage": 2, 
        "contiguous_gradients": true, 
        "overlap_comm": true
    }, 
    "steps_per_print": 50, 
    "gradient_clipping": 1.0
}
****START TRAINING****
image_size: 8, batch_size: 256, timesteps: 1000, time_emb_dim: 256
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s][2025-04-01 04:23:58,343] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:03,  1.16it/s][2025-04-01 04:23:58,397] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:01<00:00,  3.33it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  5.18it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.86it/s]
/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py:190: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True) # 使用 pd.concat 追加数据
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.66it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.79it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.35it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.54it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.70it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.69it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.36it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.53it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  5.19it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00, 10.58it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.94it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.41it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.69it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00, 10.02it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.33it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.57it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.67it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.85it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.02it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.41it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.73it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.87it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.41it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.42it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.58it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.81it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.34it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.53it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.51it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.62it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.22it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.42it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.64it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.85it/s][2025-04-01 04:24:03,657] [INFO] [logging.py:107:log_dist] [Rank 0] step=50, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:24:03,659] [INFO] [timer.py:264:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=3076.5600425349307, CurrSamplesPerSec=4209.378931958759, MemAllocated=0.02GB, MaxMemAllocated=1.9GB
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.38it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.59it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.65it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.88it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.34it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.55it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.72it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.87it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.32it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.56it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.70it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.86it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.31it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.59it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.56it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.84it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.20it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.46it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.59it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.73it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.30it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.50it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.69it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.81it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.22it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.52it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.71it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.79it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.17it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.52it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.71it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.92it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.54it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.75it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.67it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.82it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.44it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.69it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.66it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.99it/s][2025-04-01 04:24:09,147] [INFO] [logging.py:107:log_dist] [Rank 0] step=100, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:24:09,148] [INFO] [timer.py:264:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=3101.908839281945, CurrSamplesPerSec=4239.578151954888, MemAllocated=0.02GB, MaxMemAllocated=1.9GB
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.38it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.61it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/main.py", line 9, in <module>
[rank0]:     train_deepspeed(cfg)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py", line 210, in train_deepspeed
[rank0]:     generate_during_training(model_engine, sample_dir, config, epoch, num_images=config.num_images//config.image_size)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/generate.py", line 240, in generate_during_training_simulation_dif
[rank0]:     pred_noise = model_engine(x, t_batch)
[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/deepspeed/runtime/engine.py", line 2030, in forward
[rank0]:     loss = self.module(*inputs, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank0]:     return inner()
[rank0]:            ^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/unet.py", line 257, in forward
[rank0]:     assert x.dim() == 4, f"输入必须是4D张量，但得到 {x.shape}"
[rank0]: AssertionError: 输入必须是4D张量，但得到 torch.Size([1250, 1])
[rank0]:[W401 04:24:09.863022200 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-04-01 04:24:10,719] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 71457
[2025-04-01 04:24:10,719] [ERROR] [launch.py:325:sigkill_handler] ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1'] exits with return code = 1
[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ bash scripts/script_normal_dif_test.sh 
[?2004l[2025-04-01 04:25:41,446] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:25:43,727] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 04:25:43,728] [INFO] [runner.py:605:main] cmd = /hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None main.py --image_size 8 --batch_size 256 --num_epochs 200 --timesteps 1000 --lr 2e-4 --time_emb_dim 256 --mu1 4 --sigma1 1 --num1 9000 --mu2 10 --sigma2 4 --num2 1000 --samples_dir ./samples --checkpoints_dir ./checkpoints --fp16 --mode train --model_path  --num_images 10000 --simulation_distribution normal --use_different_noise --use_moe --num_experts 4 --moe_hidden_dim 64 --moe_tau 0.1
[2025-04-01 04:25:45,488] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:25:47,685] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 04:25:47,685] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 04:25:47,685] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 04:25:47,685] [INFO] [launch.py:164:main] dist_world_size=1
[2025-04-01 04:25:47,685] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 04:25:47,799] [INFO] [launch.py:256:main] process 72302 spawned with command: ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1']
[2025-04-01 04:25:50,471] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:25:53,538] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.16.5, git-hash=unknown, git-branch=unknown
[2025-04-01 04:25:53,538] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-01 04:25:53,538] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-04-01 04:25:53,543] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 1
[2025-04-01 04:25:54,686] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124/fused_adam/build.ninja...
/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.1917574405670166 seconds
[2025-04-01 04:25:54,883] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-04-01 04:25:54,883] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-04-01 04:25:54,888] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-04-01 04:25:54,888] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-04-01 04:25:54,888] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2025-04-01 04:25:54,889] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000
[2025-04-01 04:25:54,889] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000
[2025-04-01 04:25:54,889] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-04-01 04:25:54,889] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-04-01 04:25:55,344] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-04-01 04:25:55,345] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:25:55,345] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.02 GB, percent = 6.7%
[2025-04-01 04:25:55,455] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-04-01 04:25:55,455] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:25:55,456] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.02 GB, percent = 6.7%
[2025-04-01 04:25:55,456] [INFO] [stage_1_and_2.py:556:__init__] optimizer state initialized
[2025-04-01 04:25:55,552] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-04-01 04:25:55,553] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:25:55,553] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.02 GB, percent = 6.7%
[2025-04-01 04:25:55,554] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-04-01 04:25:55,555] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-04-01 04:25:55,555] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-04-01 04:25:55,555] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:25:55,555] [INFO] [config.py:1000:print] DeepSpeedEngine configuration:
[2025-04-01 04:25:55,555] [INFO] [config.py:1004:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-04-01 04:25:55,555] [INFO] [config.py:1004:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-04-01 04:25:55,555] [INFO] [config.py:1004:print]   amp_enabled .................. False
[2025-04-01 04:25:55,555] [INFO] [config.py:1004:print]   amp_params ................... False
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   bfloat16_enabled ............. False
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   bfloat16_immediate_grad_update  False
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   checkpoint_parallel_write_pipeline  False
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   checkpoint_tag_validation_enabled  True
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   checkpoint_tag_validation_fail  False
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fd10bf4e420>
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   communication_data_type ...... None
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   curriculum_enabled_legacy .... False
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   curriculum_params_legacy ..... False
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   data_efficiency_enabled ...... False
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   dataloader_drop_last ......... False
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   disable_allgather ............ False
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   dump_state ................... False
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   eigenvalue_enabled ........... False
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   eigenvalue_layer_num ......... 0
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   eigenvalue_max_iter .......... 100
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   eigenvalue_stability ......... 1e-06
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   eigenvalue_tol ............... 0.01
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   eigenvalue_verbose ........... False
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   elasticity_enabled ........... False
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   fp16_auto_cast ............... False
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   fp16_enabled ................. True
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   fp16_master_weights_and_gradients  False
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   global_rank .................. 0
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   grad_accum_dtype ............. None
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   gradient_accumulation_steps .. 1
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   gradient_clipping ............ 1.0
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   gradient_predivide_factor .... 1.0
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   graph_harvesting ............. False
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   initial_dynamic_scale ........ 65536
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   load_universal_checkpoint .... False
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   loss_scale ................... 0
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   memory_breakdown ............. False
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   mics_hierarchial_params_gather  False
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   mics_shard_size .............. -1
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-04-01 04:25:55,556] [INFO] [config.py:1004:print]   optimizer_legacy_fusion ...... False
[2025-04-01 04:25:55,557] [INFO] [config.py:1004:print]   optimizer_name ............... adamw
[2025-04-01 04:25:55,557] [INFO] [config.py:1004:print]   optimizer_params ............. {'lr': 0.0002, 'weight_decay': 0.01}
[2025-04-01 04:25:55,557] [INFO] [config.py:1004:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-04-01 04:25:55,557] [INFO] [config.py:1004:print]   pld_enabled .................. False
[2025-04-01 04:25:55,557] [INFO] [config.py:1004:print]   pld_params ................... False
[2025-04-01 04:25:55,557] [INFO] [config.py:1004:print]   prescale_gradients ........... False
[2025-04-01 04:25:55,557] [INFO] [config.py:1004:print]   scheduler_name ............... None
[2025-04-01 04:25:55,557] [INFO] [config.py:1004:print]   scheduler_params ............. None
[2025-04-01 04:25:55,557] [INFO] [config.py:1004:print]   seq_parallel_communication_data_type  torch.float32
[2025-04-01 04:25:55,557] [INFO] [config.py:1004:print]   sparse_attention ............. None
[2025-04-01 04:25:55,557] [INFO] [config.py:1004:print]   sparse_gradients_enabled ..... False
[2025-04-01 04:25:55,557] [INFO] [config.py:1004:print]   steps_per_print .............. 50
[2025-04-01 04:25:55,557] [INFO] [config.py:1004:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-04-01 04:25:55,557] [INFO] [config.py:1004:print]   timers_config ................ enabled=True synchronized=True
[2025-04-01 04:25:55,557] [INFO] [config.py:1004:print]   train_batch_size ............. 256
[2025-04-01 04:25:55,557] [INFO] [config.py:1004:print]   train_micro_batch_size_per_gpu  256
[2025-04-01 04:25:55,557] [INFO] [config.py:1004:print]   use_data_before_expert_parallel_  False
[2025-04-01 04:25:55,557] [INFO] [config.py:1004:print]   use_node_local_storage ....... False
[2025-04-01 04:25:55,557] [INFO] [config.py:1004:print]   wall_clock_breakdown ......... False
[2025-04-01 04:25:55,557] [INFO] [config.py:1004:print]   weight_quantization_config ... None
[2025-04-01 04:25:55,557] [INFO] [config.py:1004:print]   world_size ................... 1
[2025-04-01 04:25:55,557] [INFO] [config.py:1004:print]   zero_allow_untested_optimizer  False
[2025-04-01 04:25:55,557] [INFO] [config.py:1004:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-04-01 04:25:55,557] [INFO] [config.py:1004:print]   zero_enabled ................. True
[2025-04-01 04:25:55,557] [INFO] [config.py:1004:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-01 04:25:55,557] [INFO] [config.py:1004:print]   zero_optimization_stage ...... 2
[2025-04-01 04:25:55,557] [INFO] [config.py:990:print_user_config]   json = {
    "train_batch_size": 256, 
    "gradient_accumulation_steps": 1, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0002, 
            "weight_decay": 0.01
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 1000
    }, 
    "zero_optimization": {
        "stage": 2, 
        "contiguous_gradients": true, 
        "overlap_comm": true
    }, 
    "steps_per_print": 50, 
    "gradient_clipping": 1.0
}
****START TRAINING****
image_size: 8, batch_size: 256, timesteps: 1000, time_emb_dim: 256
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
[2025-04-01 04:25:56,459] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:03,  1.16it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
[2025-04-01 04:25:56,513] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:01<00:00,  3.34it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([101, 8, 8, 8])
输入final_proj前的形状: torch.Size([101, 8, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  5.18it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.87it/s]
/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py:190: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True) # 使用 pd.concat 追加数据
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.87it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00, 10.00it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([101, 8, 8, 8])
输入final_proj前的形状: torch.Size([101, 8, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.37it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.65it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  5.21it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00, 10.79it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([101, 8, 8, 8])
输入final_proj前的形状: torch.Size([101, 8, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.94it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.13it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.54it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.74it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([101, 8, 8, 8])
输入final_proj前的形状: torch.Size([101, 8, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.19it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.47it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.77it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.88it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([101, 8, 8, 8])
输入final_proj前的形状: torch.Size([101, 8, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.12it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.49it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.72it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.74it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([101, 8, 8, 8])
输入final_proj前的形状: torch.Size([101, 8, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.29it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.56it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.66it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.70it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([101, 8, 8, 8])
输入final_proj前的形状: torch.Size([101, 8, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.08it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.45it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.70it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.67it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([101, 8, 8, 8])
输入final_proj前的形状: torch.Size([101, 8, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.14it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.47it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.80it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.93it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([101, 8, 8, 8])
输入final_proj前的形状: torch.Size([101, 8, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.24it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.59it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.73it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.85it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([101, 8, 8, 8])
输入final_proj前的形状: torch.Size([101, 8, 8, 8])
[2025-04-01 04:26:01,765] [INFO] [logging.py:107:log_dist] [Rank 0] step=50, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:26:01,767] [INFO] [timer.py:264:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=3051.983271403972, CurrSamplesPerSec=4190.127060317073, MemAllocated=0.02GB, MaxMemAllocated=1.9GB
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.26it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.55it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.54it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.65it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([101, 8, 8, 8])
输入final_proj前的形状: torch.Size([101, 8, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.23it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.47it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.73it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.79it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([101, 8, 8, 8])
输入final_proj前的形状: torch.Size([101, 8, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.14it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.50it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.64it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.76it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([101, 8, 8, 8])
输入final_proj前的形状: torch.Size([101, 8, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.21it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.52it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.81it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00, 10.04it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([101, 8, 8, 8])
输入final_proj前的形状: torch.Size([101, 8, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.52it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.73it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.75it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.97it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([101, 8, 8, 8])
输入final_proj前的形状: torch.Size([101, 8, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.44it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.66it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.69it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.83it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([101, 8, 8, 8])
输入final_proj前的形状: torch.Size([101, 8, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.26it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.57it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.73it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.84it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([101, 8, 8, 8])
输入final_proj前的形状: torch.Size([101, 8, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.24it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.57it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.74it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.85it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([101, 8, 8, 8])
输入final_proj前的形状: torch.Size([101, 8, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.37it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.00it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  5.15it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00, 10.82it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([101, 8, 8, 8])
输入final_proj前的形状: torch.Size([101, 8, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 13.06it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.19it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.75it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.81it/s]输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([256, 8, 8, 8])
输入final_proj前的形状: torch.Size([101, 8, 8, 8])
输入final_proj前的形状: torch.Size([101, 8, 8, 8])
[2025-04-01 04:26:07,252] [INFO] [logging.py:107:log_dist] [Rank 0] step=100, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:26:07,254] [INFO] [timer.py:264:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=3095.1265728820463, CurrSamplesPerSec=4224.034033522259, MemAllocated=0.02GB, MaxMemAllocated=1.9GB
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.32it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.59it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/main.py", line 9, in <module>
[rank0]:     train_deepspeed(cfg)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py", line 210, in train_deepspeed
[rank0]:     generate_during_training(model_engine, sample_dir, config, epoch, num_images=config.num_images//config.image_size)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/generate.py", line 240, in generate_during_training_simulation_dif
[rank0]:     pred_noise = model_engine(x, t_batch)
[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/deepspeed/runtime/engine.py", line 2030, in forward
[rank0]:     loss = self.module(*inputs, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank0]:     return inner()
[rank0]:            ^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/unet.py", line 257, in forward
[rank0]:     assert x.dim() == 4, f"输入必须是4D张量，但得到 {x.shape}"
[rank0]: AssertionError: 输入必须是4D张量，但得到 torch.Size([1250, 1])
[rank0]:[W401 04:26:08.944202133 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-04-01 04:26:08,802] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 72302
[2025-04-01 04:26:08,803] [ERROR] [launch.py:325:sigkill_handler] ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1'] exits with return code = 1
[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ bash scripts/script_normal_dif_test.sh 
[?2004l[2025-04-01 04:26:54,742] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:26:56,953] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 04:26:56,954] [INFO] [runner.py:605:main] cmd = /hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None main.py --image_size 8 --batch_size 256 --num_epochs 200 --timesteps 1000 --lr 2e-4 --time_emb_dim 256 --mu1 4 --sigma1 1 --num1 9000 --mu2 10 --sigma2 4 --num2 1000 --samples_dir ./samples --checkpoints_dir ./checkpoints --fp16 --mode train --model_path  --num_images 10000 --simulation_distribution normal --use_different_noise --use_moe --num_experts 4 --moe_hidden_dim 64 --moe_tau 0.1
[2025-04-01 04:26:58,652] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:27:00,813] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 04:27:00,813] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 04:27:00,813] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 04:27:00,813] [INFO] [launch.py:164:main] dist_world_size=1
[2025-04-01 04:27:00,813] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 04:27:00,927] [INFO] [launch.py:256:main] process 73112 spawned with command: ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1']
[2025-04-01 04:27:03,591] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:27:06,444] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.16.5, git-hash=unknown, git-branch=unknown
[2025-04-01 04:27:06,444] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-01 04:27:06,444] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-04-01 04:27:06,449] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 1
[2025-04-01 04:27:07,616] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124/fused_adam/build.ninja...
/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.19590520858764648 seconds
[2025-04-01 04:27:07,817] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-04-01 04:27:07,818] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-04-01 04:27:07,822] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-04-01 04:27:07,822] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-04-01 04:27:07,823] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2025-04-01 04:27:07,823] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000
[2025-04-01 04:27:07,823] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000
[2025-04-01 04:27:07,823] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-04-01 04:27:07,823] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-04-01 04:27:08,163] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-04-01 04:27:08,164] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:27:08,164] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.11 GB, percent = 6.7%
[2025-04-01 04:27:08,271] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-04-01 04:27:08,271] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:27:08,272] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.11 GB, percent = 6.7%
[2025-04-01 04:27:08,272] [INFO] [stage_1_and_2.py:556:__init__] optimizer state initialized
[2025-04-01 04:27:08,371] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-04-01 04:27:08,372] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:27:08,372] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.11 GB, percent = 6.7%
[2025-04-01 04:27:08,374] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-04-01 04:27:08,374] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-04-01 04:27:08,374] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-04-01 04:27:08,374] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:27:08,374] [INFO] [config.py:1000:print] DeepSpeedEngine configuration:
[2025-04-01 04:27:08,374] [INFO] [config.py:1004:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-04-01 04:27:08,374] [INFO] [config.py:1004:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-04-01 04:27:08,374] [INFO] [config.py:1004:print]   amp_enabled .................. False
[2025-04-01 04:27:08,374] [INFO] [config.py:1004:print]   amp_params ................... False
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   bfloat16_enabled ............. False
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   bfloat16_immediate_grad_update  False
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   checkpoint_parallel_write_pipeline  False
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   checkpoint_tag_validation_enabled  True
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   checkpoint_tag_validation_fail  False
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f41c11bd6a0>
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   communication_data_type ...... None
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   curriculum_enabled_legacy .... False
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   curriculum_params_legacy ..... False
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   data_efficiency_enabled ...... False
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   dataloader_drop_last ......... False
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   disable_allgather ............ False
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   dump_state ................... False
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   eigenvalue_enabled ........... False
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   eigenvalue_layer_num ......... 0
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   eigenvalue_max_iter .......... 100
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   eigenvalue_stability ......... 1e-06
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   eigenvalue_tol ............... 0.01
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   eigenvalue_verbose ........... False
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   elasticity_enabled ........... False
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   fp16_auto_cast ............... False
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   fp16_enabled ................. True
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   fp16_master_weights_and_gradients  False
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   global_rank .................. 0
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   grad_accum_dtype ............. None
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   gradient_accumulation_steps .. 1
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   gradient_clipping ............ 1.0
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   gradient_predivide_factor .... 1.0
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   graph_harvesting ............. False
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   initial_dynamic_scale ........ 65536
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   load_universal_checkpoint .... False
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   loss_scale ................... 0
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   memory_breakdown ............. False
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   mics_hierarchial_params_gather  False
[2025-04-01 04:27:08,375] [INFO] [config.py:1004:print]   mics_shard_size .............. -1
[2025-04-01 04:27:08,376] [INFO] [config.py:1004:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-04-01 04:27:08,376] [INFO] [config.py:1004:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-04-01 04:27:08,376] [INFO] [config.py:1004:print]   optimizer_legacy_fusion ...... False
[2025-04-01 04:27:08,376] [INFO] [config.py:1004:print]   optimizer_name ............... adamw
[2025-04-01 04:27:08,376] [INFO] [config.py:1004:print]   optimizer_params ............. {'lr': 0.0002, 'weight_decay': 0.01}
[2025-04-01 04:27:08,376] [INFO] [config.py:1004:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-04-01 04:27:08,376] [INFO] [config.py:1004:print]   pld_enabled .................. False
[2025-04-01 04:27:08,376] [INFO] [config.py:1004:print]   pld_params ................... False
[2025-04-01 04:27:08,376] [INFO] [config.py:1004:print]   prescale_gradients ........... False
[2025-04-01 04:27:08,376] [INFO] [config.py:1004:print]   scheduler_name ............... None
[2025-04-01 04:27:08,376] [INFO] [config.py:1004:print]   scheduler_params ............. None
[2025-04-01 04:27:08,376] [INFO] [config.py:1004:print]   seq_parallel_communication_data_type  torch.float32
[2025-04-01 04:27:08,376] [INFO] [config.py:1004:print]   sparse_attention ............. None
[2025-04-01 04:27:08,376] [INFO] [config.py:1004:print]   sparse_gradients_enabled ..... False
[2025-04-01 04:27:08,376] [INFO] [config.py:1004:print]   steps_per_print .............. 50
[2025-04-01 04:27:08,376] [INFO] [config.py:1004:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-04-01 04:27:08,376] [INFO] [config.py:1004:print]   timers_config ................ enabled=True synchronized=True
[2025-04-01 04:27:08,376] [INFO] [config.py:1004:print]   train_batch_size ............. 256
[2025-04-01 04:27:08,376] [INFO] [config.py:1004:print]   train_micro_batch_size_per_gpu  256
[2025-04-01 04:27:08,376] [INFO] [config.py:1004:print]   use_data_before_expert_parallel_  False
[2025-04-01 04:27:08,376] [INFO] [config.py:1004:print]   use_node_local_storage ....... False
[2025-04-01 04:27:08,376] [INFO] [config.py:1004:print]   wall_clock_breakdown ......... False
[2025-04-01 04:27:08,376] [INFO] [config.py:1004:print]   weight_quantization_config ... None
[2025-04-01 04:27:08,376] [INFO] [config.py:1004:print]   world_size ................... 1
[2025-04-01 04:27:08,376] [INFO] [config.py:1004:print]   zero_allow_untested_optimizer  False
[2025-04-01 04:27:08,376] [INFO] [config.py:1004:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-04-01 04:27:08,376] [INFO] [config.py:1004:print]   zero_enabled ................. True
[2025-04-01 04:27:08,376] [INFO] [config.py:1004:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-01 04:27:08,376] [INFO] [config.py:1004:print]   zero_optimization_stage ...... 2
[2025-04-01 04:27:08,376] [INFO] [config.py:990:print_user_config]   json = {
    "train_batch_size": 256, 
    "gradient_accumulation_steps": 1, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0002, 
            "weight_decay": 0.01
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 1000
    }, 
    "zero_optimization": {
        "stage": 2, 
        "contiguous_gradients": true, 
        "overlap_comm": true
    }, 
    "steps_per_print": 50, 
    "gradient_clipping": 1.0
}
****START TRAINING****
image_size: 8, batch_size: 256, timesteps: 1000, time_emb_dim: 256
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
[2025-04-01 04:27:09,376] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:03,  1.18it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
[2025-04-01 04:27:09,430] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:01<00:00,  3.36it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([101, 1, 8, 8])
输入final_proj后的形状: torch.Size([101, 1, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  5.21it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.90it/s]
/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py:190: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True) # 使用 pd.concat 追加数据
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.72it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.76it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([101, 1, 8, 8])
输入final_proj后的形状: torch.Size([101, 1, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.32it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.55it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.65it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.80it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([101, 1, 8, 8])
输入final_proj后的形状: torch.Size([101, 1, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.45it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.63it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.63it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.61it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([101, 1, 8, 8])
输入final_proj后的形状: torch.Size([101, 1, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.20it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.49it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.79it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.92it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([101, 1, 8, 8])
输入final_proj后的形状: torch.Size([101, 1, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.26it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.58it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.71it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00, 10.02it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([101, 1, 8, 8])
输入final_proj后的形状: torch.Size([101, 1, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.33it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.63it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.73it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.86it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([101, 1, 8, 8])
输入final_proj后的形状: torch.Size([101, 1, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.19it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.54it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.61it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.74it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([101, 1, 8, 8])
输入final_proj后的形状: torch.Size([101, 1, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.31it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.53it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.72it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.92it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([101, 1, 8, 8])
输入final_proj后的形状: torch.Size([101, 1, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.46it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.68it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.81it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.91it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([101, 1, 8, 8])
输入final_proj后的形状: torch.Size([101, 1, 8, 8])
[2025-04-01 04:27:14,696] [INFO] [logging.py:107:log_dist] [Rank 0] step=50, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:27:14,697] [INFO] [timer.py:264:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=3042.217945586501, CurrSamplesPerSec=3897.370153338571, MemAllocated=0.02GB, MaxMemAllocated=1.9GB
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.23it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.60it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.72it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.80it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([101, 1, 8, 8])
输入final_proj后的形状: torch.Size([101, 1, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.15it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.51it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.78it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.86it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([101, 1, 8, 8])
输入final_proj后的形状: torch.Size([101, 1, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.30it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.61it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.53it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.62it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([101, 1, 8, 8])
输入final_proj后的形状: torch.Size([101, 1, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.22it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.56it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.17it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.14it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([101, 1, 8, 8])
输入final_proj后的形状: torch.Size([101, 1, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 11.67it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.14it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.86it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.95it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([101, 1, 8, 8])
输入final_proj后的形状: torch.Size([101, 1, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.37it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.75it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.75it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.79it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([101, 1, 8, 8])
输入final_proj后的形状: torch.Size([101, 1, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.15it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.51it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  5.11it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00, 10.36it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([101, 1, 8, 8])
输入final_proj后的形状: torch.Size([101, 1, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.78it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.99it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.76it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.81it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([101, 1, 8, 8])
输入final_proj后的形状: torch.Size([101, 1, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.09it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.48it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.76it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.84it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([101, 1, 8, 8])
输入final_proj后的形状: torch.Size([101, 1, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.30it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.59it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.82it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.82it/s]输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([256, 1, 8, 8])
输入final_proj后的形状: torch.Size([101, 1, 8, 8])
输入final_proj后的形状: torch.Size([101, 1, 8, 8])
[2025-04-01 04:27:20,185] [INFO] [logging.py:107:log_dist] [Rank 0] step=100, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:27:20,186] [INFO] [timer.py:264:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=3078.109373457405, CurrSamplesPerSec=4081.6412266153097, MemAllocated=0.02GB, MaxMemAllocated=1.9GB
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.17it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.51it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/main.py", line 9, in <module>
[rank0]:     train_deepspeed(cfg)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py", line 210, in train_deepspeed
[rank0]:     generate_during_training(model_engine, sample_dir, config, epoch, num_images=config.num_images//config.image_size)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/generate.py", line 240, in generate_during_training_simulation_dif
[rank0]:     pred_noise = model_engine(x, t_batch)
[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/deepspeed/runtime/engine.py", line 2030, in forward
[rank0]:     loss = self.module(*inputs, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank0]:     return inner()
[rank0]:            ^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/unet.py", line 257, in forward
[rank0]:     assert x.dim() == 4, f"输入必须是4D张量，但得到 {x.shape}"
[rank0]: AssertionError: 输入必须是4D张量，但得到 torch.Size([1250, 1])
[rank0]:[W401 04:27:20.867155505 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-04-01 04:27:21,930] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 73112
[2025-04-01 04:27:21,931] [ERROR] [launch.py:325:sigkill_handler] ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1'] exits with return code = 1
[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ bash scripts/script_normal_dif_test.sh [19Ppython test_unet.py[Cyaml .github/workflows/test.yml[11Ppython test_unet.py [19@bash scripts/script_normal_dif_test.sh[C[K[K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ bash scripts/script_normal_dif_test.sh 
[?2004l[2025-04-01 04:29:14,523] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:29:16,764] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 04:29:16,764] [INFO] [runner.py:605:main] cmd = /hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None main.py --image_size 8 --batch_size 256 --num_epochs 200 --timesteps 1000 --lr 2e-4 --time_emb_dim 256 --mu1 4 --sigma1 1 --num1 9000 --mu2 10 --sigma2 4 --num2 1000 --samples_dir ./samples --checkpoints_dir ./checkpoints --fp16 --mode train --model_path  --num_images 10000 --simulation_distribution normal --use_different_noise --use_moe --num_experts 4 --moe_hidden_dim 64 --moe_tau 0.1
[2025-04-01 04:29:18,605] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:29:20,846] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 04:29:20,846] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 04:29:20,846] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 04:29:20,846] [INFO] [launch.py:164:main] dist_world_size=1
[2025-04-01 04:29:20,846] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 04:29:20,961] [INFO] [launch.py:256:main] process 74043 spawned with command: ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1']
[2025-04-01 04:29:23,671] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:29:26,837] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.16.5, git-hash=unknown, git-branch=unknown
[2025-04-01 04:29:26,838] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-01 04:29:26,838] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-04-01 04:29:26,841] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 1
[2025-04-01 04:29:27,965] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124/fused_adam/build.ninja...
/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.2472066879272461 seconds
[2025-04-01 04:29:28,218] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-04-01 04:29:28,218] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-04-01 04:29:28,223] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-04-01 04:29:28,223] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-04-01 04:29:28,223] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2025-04-01 04:29:28,224] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000
[2025-04-01 04:29:28,224] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000
[2025-04-01 04:29:28,224] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-04-01 04:29:28,224] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-04-01 04:29:28,672] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-04-01 04:29:28,673] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:29:28,673] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.08 GB, percent = 6.7%
[2025-04-01 04:29:28,782] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-04-01 04:29:28,783] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:29:28,783] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.09 GB, percent = 6.7%
[2025-04-01 04:29:28,783] [INFO] [stage_1_and_2.py:556:__init__] optimizer state initialized
[2025-04-01 04:29:28,886] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-04-01 04:29:28,886] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:29:28,886] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.09 GB, percent = 6.7%
[2025-04-01 04:29:28,888] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-04-01 04:29:28,888] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-04-01 04:29:28,888] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-04-01 04:29:28,888] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:29:28,889] [INFO] [config.py:1000:print] DeepSpeedEngine configuration:
[2025-04-01 04:29:28,889] [INFO] [config.py:1004:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-04-01 04:29:28,889] [INFO] [config.py:1004:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-04-01 04:29:28,889] [INFO] [config.py:1004:print]   amp_enabled .................. False
[2025-04-01 04:29:28,889] [INFO] [config.py:1004:print]   amp_params ................... False
[2025-04-01 04:29:28,889] [INFO] [config.py:1004:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-01 04:29:28,889] [INFO] [config.py:1004:print]   bfloat16_enabled ............. False
[2025-04-01 04:29:28,889] [INFO] [config.py:1004:print]   bfloat16_immediate_grad_update  False
[2025-04-01 04:29:28,889] [INFO] [config.py:1004:print]   checkpoint_parallel_write_pipeline  False
[2025-04-01 04:29:28,889] [INFO] [config.py:1004:print]   checkpoint_tag_validation_enabled  True
[2025-04-01 04:29:28,889] [INFO] [config.py:1004:print]   checkpoint_tag_validation_fail  False
[2025-04-01 04:29:28,889] [INFO] [config.py:1004:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fd2c51d8980>
[2025-04-01 04:29:28,889] [INFO] [config.py:1004:print]   communication_data_type ...... None
[2025-04-01 04:29:28,889] [INFO] [config.py:1004:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-01 04:29:28,889] [INFO] [config.py:1004:print]   curriculum_enabled_legacy .... False
[2025-04-01 04:29:28,889] [INFO] [config.py:1004:print]   curriculum_params_legacy ..... False
[2025-04-01 04:29:28,889] [INFO] [config.py:1004:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-01 04:29:28,889] [INFO] [config.py:1004:print]   data_efficiency_enabled ...... False
[2025-04-01 04:29:28,889] [INFO] [config.py:1004:print]   dataloader_drop_last ......... False
[2025-04-01 04:29:28,889] [INFO] [config.py:1004:print]   disable_allgather ............ False
[2025-04-01 04:29:28,889] [INFO] [config.py:1004:print]   dump_state ................... False
[2025-04-01 04:29:28,889] [INFO] [config.py:1004:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-04-01 04:29:28,889] [INFO] [config.py:1004:print]   eigenvalue_enabled ........... False
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   eigenvalue_layer_num ......... 0
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   eigenvalue_max_iter .......... 100
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   eigenvalue_stability ......... 1e-06
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   eigenvalue_tol ............... 0.01
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   eigenvalue_verbose ........... False
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   elasticity_enabled ........... False
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   fp16_auto_cast ............... False
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   fp16_enabled ................. True
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   fp16_master_weights_and_gradients  False
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   global_rank .................. 0
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   grad_accum_dtype ............. None
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   gradient_accumulation_steps .. 1
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   gradient_clipping ............ 1.0
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   gradient_predivide_factor .... 1.0
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   graph_harvesting ............. False
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   initial_dynamic_scale ........ 65536
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   load_universal_checkpoint .... False
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   loss_scale ................... 0
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   memory_breakdown ............. False
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   mics_hierarchial_params_gather  False
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   mics_shard_size .............. -1
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   optimizer_legacy_fusion ...... False
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   optimizer_name ............... adamw
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   optimizer_params ............. {'lr': 0.0002, 'weight_decay': 0.01}
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   pld_enabled .................. False
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   pld_params ................... False
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   prescale_gradients ........... False
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   scheduler_name ............... None
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   scheduler_params ............. None
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   seq_parallel_communication_data_type  torch.float32
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   sparse_attention ............. None
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   sparse_gradients_enabled ..... False
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   steps_per_print .............. 50
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   timers_config ................ enabled=True synchronized=True
[2025-04-01 04:29:28,890] [INFO] [config.py:1004:print]   train_batch_size ............. 256
[2025-04-01 04:29:28,891] [INFO] [config.py:1004:print]   train_micro_batch_size_per_gpu  256
[2025-04-01 04:29:28,891] [INFO] [config.py:1004:print]   use_data_before_expert_parallel_  False
[2025-04-01 04:29:28,891] [INFO] [config.py:1004:print]   use_node_local_storage ....... False
[2025-04-01 04:29:28,891] [INFO] [config.py:1004:print]   wall_clock_breakdown ......... False
[2025-04-01 04:29:28,891] [INFO] [config.py:1004:print]   weight_quantization_config ... None
[2025-04-01 04:29:28,891] [INFO] [config.py:1004:print]   world_size ................... 1
[2025-04-01 04:29:28,891] [INFO] [config.py:1004:print]   zero_allow_untested_optimizer  False
[2025-04-01 04:29:28,891] [INFO] [config.py:1004:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-04-01 04:29:28,891] [INFO] [config.py:1004:print]   zero_enabled ................. True
[2025-04-01 04:29:28,891] [INFO] [config.py:1004:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-01 04:29:28,891] [INFO] [config.py:1004:print]   zero_optimization_stage ...... 2
[2025-04-01 04:29:28,891] [INFO] [config.py:990:print_user_config]   json = {
    "train_batch_size": 256, 
    "gradient_accumulation_steps": 1, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0002, 
            "weight_decay": 0.01
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 1000
    }, 
    "zero_optimization": {
        "stage": 2, 
        "contiguous_gradients": true, 
        "overlap_comm": true
    }, 
    "steps_per_print": 50, 
    "gradient_clipping": 1.0
}
****START TRAINING****
image_size: 8, batch_size: 256, timesteps: 1000, time_emb_dim: 256
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
[2025-04-01 04:29:29,834] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:03,  1.11it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
[2025-04-01 04:29:29,888] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:01<00:00,  3.21it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([101, 1])
x_flat的形状: torch.Size([101, 1])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  5.03it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.74it/s]
/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py:190: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True) # 使用 pd.concat 追加数据
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.73it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.83it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([101, 1])
x_flat的形状: torch.Size([101, 1])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.33it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.60it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.73it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.92it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([101, 1])
x_flat的形状: torch.Size([101, 1])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.41it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.66it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.77it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.81it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([101, 1])
x_flat的形状: torch.Size([101, 1])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.24it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.56it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.59it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.67it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([101, 1])
x_flat的形状: torch.Size([101, 1])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.14it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.45it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.66it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.64it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([101, 1])
x_flat的形状: torch.Size([101, 1])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.01it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.42it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.78it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.98it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([101, 1])
x_flat的形状: torch.Size([101, 1])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.36it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.68it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.72it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.67it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([101, 1])
x_flat的形状: torch.Size([101, 1])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.23it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.52it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.73it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.90it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([101, 1])
x_flat的形状: torch.Size([101, 1])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.30it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.59it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.63it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.70it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([101, 1])
x_flat的形状: torch.Size([101, 1])
[2025-04-01 04:29:35,164] [INFO] [logging.py:107:log_dist] [Rank 0] step=50, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:29:35,165] [INFO] [timer.py:264:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=3032.9888927852626, CurrSamplesPerSec=4017.4845506055062, MemAllocated=0.02GB, MaxMemAllocated=1.9GB
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.14it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.50it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.78it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.80it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([101, 1])
x_flat的形状: torch.Size([101, 1])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.13it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.53it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.74it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.78it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([101, 1])
x_flat的形状: torch.Size([101, 1])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.22it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.55it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.76it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.98it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([101, 1])
x_flat的形状: torch.Size([101, 1])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.46it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.68it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.69it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.63it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([101, 1])
x_flat的形状: torch.Size([101, 1])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.29it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.53it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.72it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.74it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([101, 1])
x_flat的形状: torch.Size([101, 1])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.27it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.55it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.55it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.40it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([101, 1])
x_flat的形状: torch.Size([101, 1])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 11.86it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.70it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.71it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.83it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([101, 1])
x_flat的形状: torch.Size([101, 1])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.25it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.52it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.78it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.91it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([101, 1])
x_flat的形状: torch.Size([101, 1])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.09it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.52it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.73it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.78it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([101, 1])
x_flat的形状: torch.Size([101, 1])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.12it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.48it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.76it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.48it/s]x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([256, 1])
x_flat的形状: torch.Size([101, 1])
x_flat的形状: torch.Size([101, 1])
[2025-04-01 04:29:40,731] [INFO] [logging.py:107:log_dist] [Rank 0] step=100, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:29:40,732] [INFO] [timer.py:264:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=3058.2664021065525, CurrSamplesPerSec=4017.1839387815044, MemAllocated=0.02GB, MaxMemAllocated=1.9GB
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 11.88it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.58it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/main.py", line 9, in <module>
[rank0]:     train_deepspeed(cfg)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py", line 210, in train_deepspeed
[rank0]:     generate_during_training(model_engine, sample_dir, config, epoch, num_images=config.num_images//config.image_size)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/generate.py", line 240, in generate_during_training_simulation_dif
[rank0]:     pred_noise = model_engine(x, t_batch)
[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/deepspeed/runtime/engine.py", line 2030, in forward
[rank0]:     loss = self.module(*inputs, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank0]:     return inner()
[rank0]:            ^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/unet.py", line 257, in forward
[rank0]:     assert x.dim() == 4, f"输入必须是4D张量，但得到 {x.shape}"
[rank0]: AssertionError: 输入必须是4D张量，但得到 torch.Size([1250, 1])
[rank0]:[W401 04:29:41.413997450 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-04-01 04:29:42,964] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 74043
[2025-04-01 04:29:42,965] [ERROR] [launch.py:325:sigkill_handler] ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1'] exits with return code = 1
[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ bash scripts/script_normal_dif_test.sh 
[?2004l[2025-04-01 04:30:07,941] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:30:10,253] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 04:30:10,253] [INFO] [runner.py:605:main] cmd = /hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None main.py --image_size 8 --batch_size 256 --num_epochs 200 --timesteps 1000 --lr 2e-4 --time_emb_dim 256 --mu1 4 --sigma1 1 --num1 9000 --mu2 10 --sigma2 4 --num2 1000 --samples_dir ./samples --checkpoints_dir ./checkpoints --fp16 --mode train --model_path  --num_images 10000 --simulation_distribution normal --use_different_noise --use_moe --num_experts 4 --moe_hidden_dim 64 --moe_tau 0.1
[2025-04-01 04:30:12,039] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:30:14,284] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 04:30:14,284] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 04:30:14,284] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 04:30:14,284] [INFO] [launch.py:164:main] dist_world_size=1
[2025-04-01 04:30:14,284] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 04:30:14,398] [INFO] [launch.py:256:main] process 74854 spawned with command: ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1']
[2025-04-01 04:30:17,200] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:30:20,239] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.16.5, git-hash=unknown, git-branch=unknown
[2025-04-01 04:30:20,239] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-01 04:30:20,239] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-04-01 04:30:20,242] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 1
[2025-04-01 04:30:21,476] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124/fused_adam/build.ninja...
/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.21052074432373047 seconds
[2025-04-01 04:30:21,691] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-04-01 04:30:21,692] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-04-01 04:30:21,696] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-04-01 04:30:21,696] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-04-01 04:30:21,697] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2025-04-01 04:30:21,697] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000
[2025-04-01 04:30:21,697] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000
[2025-04-01 04:30:21,697] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-04-01 04:30:21,697] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-04-01 04:30:22,088] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-04-01 04:30:22,088] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:30:22,089] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.09 GB, percent = 6.7%
[2025-04-01 04:30:22,208] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-04-01 04:30:22,209] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:30:22,209] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.1 GB, percent = 6.7%
[2025-04-01 04:30:22,209] [INFO] [stage_1_and_2.py:556:__init__] optimizer state initialized
[2025-04-01 04:30:22,323] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-04-01 04:30:22,324] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:30:22,324] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.1 GB, percent = 6.7%
[2025-04-01 04:30:22,326] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-04-01 04:30:22,326] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-04-01 04:30:22,326] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-04-01 04:30:22,326] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:30:22,326] [INFO] [config.py:1000:print] DeepSpeedEngine configuration:
[2025-04-01 04:30:22,326] [INFO] [config.py:1004:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-04-01 04:30:22,326] [INFO] [config.py:1004:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-04-01 04:30:22,326] [INFO] [config.py:1004:print]   amp_enabled .................. False
[2025-04-01 04:30:22,326] [INFO] [config.py:1004:print]   amp_params ................... False
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   bfloat16_enabled ............. False
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   bfloat16_immediate_grad_update  False
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   checkpoint_parallel_write_pipeline  False
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   checkpoint_tag_validation_enabled  True
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   checkpoint_tag_validation_fail  False
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f9148ec3e30>
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   communication_data_type ...... None
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   curriculum_enabled_legacy .... False
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   curriculum_params_legacy ..... False
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   data_efficiency_enabled ...... False
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   dataloader_drop_last ......... False
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   disable_allgather ............ False
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   dump_state ................... False
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   eigenvalue_enabled ........... False
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   eigenvalue_layer_num ......... 0
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   eigenvalue_max_iter .......... 100
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   eigenvalue_stability ......... 1e-06
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   eigenvalue_tol ............... 0.01
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   eigenvalue_verbose ........... False
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   elasticity_enabled ........... False
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   fp16_auto_cast ............... False
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   fp16_enabled ................. True
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   fp16_master_weights_and_gradients  False
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   global_rank .................. 0
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   grad_accum_dtype ............. None
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   gradient_accumulation_steps .. 1
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   gradient_clipping ............ 1.0
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   gradient_predivide_factor .... 1.0
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   graph_harvesting ............. False
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   initial_dynamic_scale ........ 65536
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   load_universal_checkpoint .... False
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   loss_scale ................... 0
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   memory_breakdown ............. False
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   mics_hierarchial_params_gather  False
[2025-04-01 04:30:22,327] [INFO] [config.py:1004:print]   mics_shard_size .............. -1
[2025-04-01 04:30:22,328] [INFO] [config.py:1004:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-04-01 04:30:22,328] [INFO] [config.py:1004:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-04-01 04:30:22,328] [INFO] [config.py:1004:print]   optimizer_legacy_fusion ...... False
[2025-04-01 04:30:22,328] [INFO] [config.py:1004:print]   optimizer_name ............... adamw
[2025-04-01 04:30:22,328] [INFO] [config.py:1004:print]   optimizer_params ............. {'lr': 0.0002, 'weight_decay': 0.01}
[2025-04-01 04:30:22,328] [INFO] [config.py:1004:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-04-01 04:30:22,328] [INFO] [config.py:1004:print]   pld_enabled .................. False
[2025-04-01 04:30:22,328] [INFO] [config.py:1004:print]   pld_params ................... False
[2025-04-01 04:30:22,328] [INFO] [config.py:1004:print]   prescale_gradients ........... False
[2025-04-01 04:30:22,328] [INFO] [config.py:1004:print]   scheduler_name ............... None
[2025-04-01 04:30:22,328] [INFO] [config.py:1004:print]   scheduler_params ............. None
[2025-04-01 04:30:22,328] [INFO] [config.py:1004:print]   seq_parallel_communication_data_type  torch.float32
[2025-04-01 04:30:22,328] [INFO] [config.py:1004:print]   sparse_attention ............. None
[2025-04-01 04:30:22,328] [INFO] [config.py:1004:print]   sparse_gradients_enabled ..... False
[2025-04-01 04:30:22,328] [INFO] [config.py:1004:print]   steps_per_print .............. 50
[2025-04-01 04:30:22,328] [INFO] [config.py:1004:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-04-01 04:30:22,328] [INFO] [config.py:1004:print]   timers_config ................ enabled=True synchronized=True
[2025-04-01 04:30:22,328] [INFO] [config.py:1004:print]   train_batch_size ............. 256
[2025-04-01 04:30:22,328] [INFO] [config.py:1004:print]   train_micro_batch_size_per_gpu  256
[2025-04-01 04:30:22,328] [INFO] [config.py:1004:print]   use_data_before_expert_parallel_  False
[2025-04-01 04:30:22,328] [INFO] [config.py:1004:print]   use_node_local_storage ....... False
[2025-04-01 04:30:22,328] [INFO] [config.py:1004:print]   wall_clock_breakdown ......... False
[2025-04-01 04:30:22,328] [INFO] [config.py:1004:print]   weight_quantization_config ... None
[2025-04-01 04:30:22,328] [INFO] [config.py:1004:print]   world_size ................... 1
[2025-04-01 04:30:22,328] [INFO] [config.py:1004:print]   zero_allow_untested_optimizer  False
[2025-04-01 04:30:22,328] [INFO] [config.py:1004:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-04-01 04:30:22,328] [INFO] [config.py:1004:print]   zero_enabled ................. True
[2025-04-01 04:30:22,328] [INFO] [config.py:1004:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-01 04:30:22,328] [INFO] [config.py:1004:print]   zero_optimization_stage ...... 2
[2025-04-01 04:30:22,328] [INFO] [config.py:990:print_user_config]   json = {
    "train_batch_size": 256, 
    "gradient_accumulation_steps": 1, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0002, 
            "weight_decay": 0.01
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 1000
    }, 
    "zero_optimization": {
        "stage": 2, 
        "contiguous_gradients": true, 
        "overlap_comm": true
    }, 
    "steps_per_print": 50, 
    "gradient_clipping": 1.0
}
****START TRAINING****
image_size: 8, batch_size: 256, timesteps: 1000, time_emb_dim: 256
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
[2025-04-01 04:30:23,468] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:03,  1.05it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
[2025-04-01 04:30:23,525] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:01<00:00,  3.19it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([101, 1])
moe_out的形状: torch.Size([101, 1])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  5.02it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.69it/s]
/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py:190: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True) # 使用 pd.concat 追加数据
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  5.05it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00, 10.14it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([101, 1])
moe_out的形状: torch.Size([101, 1])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.44it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.75it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.77it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.76it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([101, 1])
moe_out的形状: torch.Size([101, 1])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.25it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.52it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.35it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.23it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([101, 1])
moe_out的形状: torch.Size([101, 1])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 11.84it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.16it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.53it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.59it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([101, 1])
moe_out的形状: torch.Size([101, 1])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.12it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.36it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.60it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.57it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([101, 1])
moe_out的形状: torch.Size([101, 1])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 11.75it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.23it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.29it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.31it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([101, 1])
moe_out的形状: torch.Size([101, 1])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 11.73it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.05it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.50it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00, 10.23it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([101, 1])
moe_out的形状: torch.Size([101, 1])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.32it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.51it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  5.19it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00, 10.80it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([101, 1])
moe_out的形状: torch.Size([101, 1])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 13.01it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.11it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.51it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.65it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([101, 1])
moe_out的形状: torch.Size([101, 1])
[2025-04-01 04:30:28,841] [INFO] [logging.py:107:log_dist] [Rank 0] step=50, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:30:28,842] [INFO] [timer.py:264:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=3054.4024118702782, CurrSamplesPerSec=3840.825145629957, MemAllocated=0.02GB, MaxMemAllocated=1.9GB
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 11.86it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.18it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.30it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.20it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([101, 1])
moe_out的形状: torch.Size([101, 1])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 11.56it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.98it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.50it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.58it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([101, 1])
moe_out的形状: torch.Size([101, 1])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.06it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.29it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.56it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.54it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([101, 1])
moe_out的形状: torch.Size([101, 1])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.00it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.31it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.45it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.40it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([101, 1])
moe_out的形状: torch.Size([101, 1])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 11.88it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.19it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.59it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.64it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([101, 1])
moe_out的形状: torch.Size([101, 1])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 11.96it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.36it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.51it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.48it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([101, 1])
moe_out的形状: torch.Size([101, 1])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 11.94it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.24it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.44it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.41it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([101, 1])
moe_out的形状: torch.Size([101, 1])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 11.47it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.04it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.53it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.49it/s]moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([256, 1])
moe_out的形状: torch.Size([101, 1])
moe_out的形状: torch.Size([101, 1])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.04it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.33it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]^C[2025-04-01 04:30:33,573] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 74854
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/main.py", line 9, in <module>
[rank0]:     train_deepspeed(cfg)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py", line 142, in train_deepspeed
[rank0]:     for batch in tqdm(train_loader):
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/tqdm/std.py", line 1181, in __iter__
[rank0]:     for obj in iterable:
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:            ^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 708, in __next__
[rank0]:     data = self._next_data()
[rank0]:            ^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1458, in _next_data
[rank0]:     idx, data = self._get_data()
[rank0]:                 ^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1410, in _get_data
[rank0]:     success, data = self._try_get_data()
[rank0]:                     ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1251, in _try_get_data
[rank0]:     data = self._data_queue.get(timeout=timeout)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/queue.py", line 180, in get
[rank0]:     self.not_empty.wait(remaining)
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/threading.py", line 338, in wait
[rank0]:     gotit = waiter.acquire(True, timeout)
[rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: KeyboardInterrupt
[2025-04-01 04:30:33,751] [INFO] [launch.py:328:sigkill_handler] Main process received SIGINT, exiting
^CTraceback (most recent call last):
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/subprocess.py", line 1264, in wait
    return self._wait(timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/subprocess.py", line 2046, in _wait
    (pid, sts) = self._try_wait(0)
                 ^^^^^^^^^^^^^^^^^
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/subprocess.py", line 2004, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/deepspeed", line 6, in <module>
    main()
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/deepspeed/launcher/runner.py", line 621, in main
    result.wait()
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/subprocess.py", line 1277, in wait
    self._wait(timeout=sigint_timeout)
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/subprocess.py", line 2040, in _wait
    time.sleep(delay)
KeyboardInterrupt
[2025-04-01 04:30:33,767] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 74854
[2025-04-01 04:30:33,768] [INFO] [launch.py:328:sigkill_handler] Main process received SIGINT, exiting
Exception ignored in atexit callback: <function matmul_ext_update_autotune_table at 0x7faf56dfcd60>
Traceback (most recent call last):
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 480, in matmul_ext_update_autotune_table
    fp16_matmul._update_autotune_table()
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 457, in _update_autotune_table
    TritonMatmul._update_autotune_table(__class__.__name__ + "_2d_kernel", __class__._2d_kernel)
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 180, in _update_autotune_table
    cache_manager = AutotuneCacheManager(cache_key)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 90, in __init__
    TritonCacheDir.warn_if_nfs(self.cache_dir)
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 47, in warn_if_nfs
    if is_nfs_path(cache_dir) and not TritonCacheDir._warning_printed:
       ^^^^^^^^^^^^^^^^^^^^^^
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 30, in is_nfs_path
    output = subprocess.check_output(['df', '-T', path], encoding='utf-8')
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/subprocess.py", line 466, in check_output
    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/subprocess.py", line 548, in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/subprocess.py", line 1026, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/subprocess.py", line 1883, in _execute_child
    self.pid = _fork_exec(
               ^^^^^^^^^^^
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/deepspeed/launcher/launch.py", line 332, in sigkill_handler
    sys.exit(1)
SystemExit: 1
^C
[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ ^C[?2004l[?2004h[?2004l
[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ ^C[?2004l[?2004h[?2004l
[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ ^C[?2004l[?2004h[?2004l
[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ bash scripts/script_normal_dif_test.sh 
[?2004l[2025-04-01 04:38:03,969] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:38:06,146] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 04:38:06,146] [INFO] [runner.py:605:main] cmd = /hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None main.py --image_size 8 --batch_size 256 --num_epochs 200 --timesteps 1000 --lr 2e-4 --time_emb_dim 256 --mu1 4 --sigma1 1 --num1 9000 --mu2 10 --sigma2 4 --num2 1000 --samples_dir ./samples --checkpoints_dir ./checkpoints --fp16 --mode train --model_path  --num_images 10000 --simulation_distribution normal --use_different_noise --use_moe --num_experts 4 --moe_hidden_dim 64 --moe_tau 0.1
[2025-04-01 04:38:07,923] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:38:10,107] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 04:38:10,108] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 04:38:10,108] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 04:38:10,108] [INFO] [launch.py:164:main] dist_world_size=1
[2025-04-01 04:38:10,108] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 04:38:10,221] [INFO] [launch.py:256:main] process 76788 spawned with command: ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1']
[2025-04-01 04:38:13,051] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:38:16,031] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.16.5, git-hash=unknown, git-branch=unknown
[2025-04-01 04:38:16,031] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-01 04:38:16,031] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-04-01 04:38:16,034] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 1
[2025-04-01 04:38:17,180] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124/fused_adam/build.ninja...
/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.19602537155151367 seconds
[2025-04-01 04:38:17,381] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-04-01 04:38:17,382] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-04-01 04:38:17,386] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-04-01 04:38:17,387] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-04-01 04:38:17,387] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2025-04-01 04:38:17,387] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000
[2025-04-01 04:38:17,387] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000
[2025-04-01 04:38:17,387] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-04-01 04:38:17,387] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-04-01 04:38:17,747] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-04-01 04:38:17,747] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:38:17,747] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 66.99 GB, percent = 6.6%
[2025-04-01 04:38:17,851] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-04-01 04:38:17,852] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:38:17,852] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.0 GB, percent = 6.7%
[2025-04-01 04:38:17,852] [INFO] [stage_1_and_2.py:556:__init__] optimizer state initialized
[2025-04-01 04:38:17,949] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-04-01 04:38:17,949] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:38:17,949] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.02 GB, percent = 6.7%
[2025-04-01 04:38:17,951] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-04-01 04:38:17,951] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-04-01 04:38:17,951] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-04-01 04:38:17,951] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:38:17,952] [INFO] [config.py:1000:print] DeepSpeedEngine configuration:
[2025-04-01 04:38:17,952] [INFO] [config.py:1004:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-04-01 04:38:17,952] [INFO] [config.py:1004:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-04-01 04:38:17,952] [INFO] [config.py:1004:print]   amp_enabled .................. False
[2025-04-01 04:38:17,952] [INFO] [config.py:1004:print]   amp_params ................... False
[2025-04-01 04:38:17,952] [INFO] [config.py:1004:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-01 04:38:17,952] [INFO] [config.py:1004:print]   bfloat16_enabled ............. False
[2025-04-01 04:38:17,952] [INFO] [config.py:1004:print]   bfloat16_immediate_grad_update  False
[2025-04-01 04:38:17,952] [INFO] [config.py:1004:print]   checkpoint_parallel_write_pipeline  False
[2025-04-01 04:38:17,952] [INFO] [config.py:1004:print]   checkpoint_tag_validation_enabled  True
[2025-04-01 04:38:17,952] [INFO] [config.py:1004:print]   checkpoint_tag_validation_fail  False
[2025-04-01 04:38:17,952] [INFO] [config.py:1004:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fdb3d62bb30>
[2025-04-01 04:38:17,952] [INFO] [config.py:1004:print]   communication_data_type ...... None
[2025-04-01 04:38:17,952] [INFO] [config.py:1004:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-01 04:38:17,952] [INFO] [config.py:1004:print]   curriculum_enabled_legacy .... False
[2025-04-01 04:38:17,952] [INFO] [config.py:1004:print]   curriculum_params_legacy ..... False
[2025-04-01 04:38:17,952] [INFO] [config.py:1004:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-01 04:38:17,952] [INFO] [config.py:1004:print]   data_efficiency_enabled ...... False
[2025-04-01 04:38:17,952] [INFO] [config.py:1004:print]   dataloader_drop_last ......... False
[2025-04-01 04:38:17,952] [INFO] [config.py:1004:print]   disable_allgather ............ False
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   dump_state ................... False
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   eigenvalue_enabled ........... False
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   eigenvalue_layer_num ......... 0
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   eigenvalue_max_iter .......... 100
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   eigenvalue_stability ......... 1e-06
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   eigenvalue_tol ............... 0.01
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   eigenvalue_verbose ........... False
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   elasticity_enabled ........... False
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   fp16_auto_cast ............... False
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   fp16_enabled ................. True
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   fp16_master_weights_and_gradients  False
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   global_rank .................. 0
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   grad_accum_dtype ............. None
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   gradient_accumulation_steps .. 1
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   gradient_clipping ............ 1.0
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   gradient_predivide_factor .... 1.0
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   graph_harvesting ............. False
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   initial_dynamic_scale ........ 65536
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   load_universal_checkpoint .... False
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   loss_scale ................... 0
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   memory_breakdown ............. False
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   mics_hierarchial_params_gather  False
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   mics_shard_size .............. -1
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   optimizer_legacy_fusion ...... False
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   optimizer_name ............... adamw
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   optimizer_params ............. {'lr': 0.0002, 'weight_decay': 0.01}
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   pld_enabled .................. False
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   pld_params ................... False
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   prescale_gradients ........... False
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   scheduler_name ............... None
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   scheduler_params ............. None
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   seq_parallel_communication_data_type  torch.float32
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   sparse_attention ............. None
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   sparse_gradients_enabled ..... False
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   steps_per_print .............. 50
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   timers_config ................ enabled=True synchronized=True
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   train_batch_size ............. 256
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   train_micro_batch_size_per_gpu  256
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   use_data_before_expert_parallel_  False
[2025-04-01 04:38:17,953] [INFO] [config.py:1004:print]   use_node_local_storage ....... False
[2025-04-01 04:38:17,954] [INFO] [config.py:1004:print]   wall_clock_breakdown ......... False
[2025-04-01 04:38:17,954] [INFO] [config.py:1004:print]   weight_quantization_config ... None
[2025-04-01 04:38:17,954] [INFO] [config.py:1004:print]   world_size ................... 1
[2025-04-01 04:38:17,954] [INFO] [config.py:1004:print]   zero_allow_untested_optimizer  False
[2025-04-01 04:38:17,954] [INFO] [config.py:1004:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-04-01 04:38:17,954] [INFO] [config.py:1004:print]   zero_enabled ................. True
[2025-04-01 04:38:17,954] [INFO] [config.py:1004:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-01 04:38:17,954] [INFO] [config.py:1004:print]   zero_optimization_stage ...... 2
[2025-04-01 04:38:17,954] [INFO] [config.py:990:print_user_config]   json = {
    "train_batch_size": 256, 
    "gradient_accumulation_steps": 1, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0002, 
            "weight_decay": 0.01
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 1000
    }, 
    "zero_optimization": {
        "stage": 2, 
        "contiguous_gradients": true, 
        "overlap_comm": true
    }, 
    "steps_per_print": 50, 
    "gradient_clipping": 1.0
}
****START TRAINING****
image_size: 8, batch_size: 256, timesteps: 1000, time_emb_dim: 256
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]moe_out的形状: torch.Size([256, 1])
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/main.py", line 9, in <module>
[rank0]:     train_deepspeed(cfg)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py", line 155, in train_deepspeed
[rank0]:     noisy_images, noise = forward_diffusion_with_moe(
[rank0]:                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/diffusion.py", line 101, in forward_diffusion_with_moe
[rank0]:     assert pred_params.shape == x0.shape, f"模型输出形状{pred_params.shape}与输入{x0.shape}不匹配"
[rank0]: AssertionError: 模型输出形状torch.Size([256, 1])与输入torch.Size([256, 1, 8, 8])不匹配
[rank0]:[W401 04:38:19.161327291 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-04-01 04:38:20,223] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 76788
[2025-04-01 04:38:20,224] [ERROR] [launch.py:325:sigkill_handler] ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1'] exits with return code = 1
[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ bash scripts/script_normal_dif_test.sh 
[?2004l[2025-04-01 04:39:06,014] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:39:08,256] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 04:39:08,256] [INFO] [runner.py:605:main] cmd = /hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None main.py --image_size 8 --batch_size 256 --num_epochs 200 --timesteps 1000 --lr 2e-4 --time_emb_dim 256 --mu1 4 --sigma1 1 --num1 9000 --mu2 10 --sigma2 4 --num2 1000 --samples_dir ./samples --checkpoints_dir ./checkpoints --fp16 --mode train --model_path  --num_images 10000 --simulation_distribution normal --use_different_noise --use_moe --num_experts 4 --moe_hidden_dim 64 --moe_tau 0.1
[2025-04-01 04:39:09,995] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:39:12,266] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 04:39:12,266] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 04:39:12,266] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 04:39:12,266] [INFO] [launch.py:164:main] dist_world_size=1
[2025-04-01 04:39:12,266] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 04:39:12,380] [INFO] [launch.py:256:main] process 77431 spawned with command: ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1']
[2025-04-01 04:39:15,228] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:39:18,136] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.16.5, git-hash=unknown, git-branch=unknown
[2025-04-01 04:39:18,136] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-01 04:39:18,136] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-04-01 04:39:18,139] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 1
[2025-04-01 04:39:19,349] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124/fused_adam/build.ninja...
/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.18204760551452637 seconds
[2025-04-01 04:39:19,536] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-04-01 04:39:19,536] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-04-01 04:39:19,541] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-04-01 04:39:19,541] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-04-01 04:39:19,541] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2025-04-01 04:39:19,541] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000
[2025-04-01 04:39:19,542] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000
[2025-04-01 04:39:19,542] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-04-01 04:39:19,542] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-04-01 04:39:19,958] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-04-01 04:39:19,958] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:39:19,958] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.04 GB, percent = 6.7%
[2025-04-01 04:39:20,087] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-04-01 04:39:20,087] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:39:20,087] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.05 GB, percent = 6.7%
[2025-04-01 04:39:20,087] [INFO] [stage_1_and_2.py:556:__init__] optimizer state initialized
[2025-04-01 04:39:20,195] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-04-01 04:39:20,196] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:39:20,196] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.05 GB, percent = 6.7%
[2025-04-01 04:39:20,197] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-04-01 04:39:20,197] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-04-01 04:39:20,198] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-04-01 04:39:20,198] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:39:20,198] [INFO] [config.py:1000:print] DeepSpeedEngine configuration:
[2025-04-01 04:39:20,198] [INFO] [config.py:1004:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-04-01 04:39:20,198] [INFO] [config.py:1004:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-04-01 04:39:20,198] [INFO] [config.py:1004:print]   amp_enabled .................. False
[2025-04-01 04:39:20,198] [INFO] [config.py:1004:print]   amp_params ................... False
[2025-04-01 04:39:20,198] [INFO] [config.py:1004:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-01 04:39:20,198] [INFO] [config.py:1004:print]   bfloat16_enabled ............. False
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   bfloat16_immediate_grad_update  False
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   checkpoint_parallel_write_pipeline  False
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   checkpoint_tag_validation_enabled  True
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   checkpoint_tag_validation_fail  False
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fbf410e5b20>
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   communication_data_type ...... None
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   curriculum_enabled_legacy .... False
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   curriculum_params_legacy ..... False
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   data_efficiency_enabled ...... False
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   dataloader_drop_last ......... False
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   disable_allgather ............ False
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   dump_state ................... False
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   eigenvalue_enabled ........... False
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   eigenvalue_layer_num ......... 0
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   eigenvalue_max_iter .......... 100
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   eigenvalue_stability ......... 1e-06
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   eigenvalue_tol ............... 0.01
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   eigenvalue_verbose ........... False
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   elasticity_enabled ........... False
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   fp16_auto_cast ............... False
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   fp16_enabled ................. True
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   fp16_master_weights_and_gradients  False
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   global_rank .................. 0
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   grad_accum_dtype ............. None
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   gradient_accumulation_steps .. 1
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   gradient_clipping ............ 1.0
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   gradient_predivide_factor .... 1.0
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   graph_harvesting ............. False
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   initial_dynamic_scale ........ 65536
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   load_universal_checkpoint .... False
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   loss_scale ................... 0
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   memory_breakdown ............. False
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   mics_hierarchial_params_gather  False
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   mics_shard_size .............. -1
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   optimizer_legacy_fusion ...... False
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   optimizer_name ............... adamw
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   optimizer_params ............. {'lr': 0.0002, 'weight_decay': 0.01}
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-04-01 04:39:20,199] [INFO] [config.py:1004:print]   pld_enabled .................. False
[2025-04-01 04:39:20,200] [INFO] [config.py:1004:print]   pld_params ................... False
[2025-04-01 04:39:20,200] [INFO] [config.py:1004:print]   prescale_gradients ........... False
[2025-04-01 04:39:20,200] [INFO] [config.py:1004:print]   scheduler_name ............... None
[2025-04-01 04:39:20,200] [INFO] [config.py:1004:print]   scheduler_params ............. None
[2025-04-01 04:39:20,200] [INFO] [config.py:1004:print]   seq_parallel_communication_data_type  torch.float32
[2025-04-01 04:39:20,200] [INFO] [config.py:1004:print]   sparse_attention ............. None
[2025-04-01 04:39:20,200] [INFO] [config.py:1004:print]   sparse_gradients_enabled ..... False
[2025-04-01 04:39:20,200] [INFO] [config.py:1004:print]   steps_per_print .............. 50
[2025-04-01 04:39:20,200] [INFO] [config.py:1004:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-04-01 04:39:20,200] [INFO] [config.py:1004:print]   timers_config ................ enabled=True synchronized=True
[2025-04-01 04:39:20,200] [INFO] [config.py:1004:print]   train_batch_size ............. 256
[2025-04-01 04:39:20,200] [INFO] [config.py:1004:print]   train_micro_batch_size_per_gpu  256
[2025-04-01 04:39:20,200] [INFO] [config.py:1004:print]   use_data_before_expert_parallel_  False
[2025-04-01 04:39:20,200] [INFO] [config.py:1004:print]   use_node_local_storage ....... False
[2025-04-01 04:39:20,200] [INFO] [config.py:1004:print]   wall_clock_breakdown ......... False
[2025-04-01 04:39:20,200] [INFO] [config.py:1004:print]   weight_quantization_config ... None
[2025-04-01 04:39:20,200] [INFO] [config.py:1004:print]   world_size ................... 1
[2025-04-01 04:39:20,200] [INFO] [config.py:1004:print]   zero_allow_untested_optimizer  False
[2025-04-01 04:39:20,200] [INFO] [config.py:1004:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-04-01 04:39:20,200] [INFO] [config.py:1004:print]   zero_enabled ................. True
[2025-04-01 04:39:20,200] [INFO] [config.py:1004:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-01 04:39:20,200] [INFO] [config.py:1004:print]   zero_optimization_stage ...... 2
[2025-04-01 04:39:20,200] [INFO] [config.py:990:print_user_config]   json = {
    "train_batch_size": 256, 
    "gradient_accumulation_steps": 1, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0002, 
            "weight_decay": 0.01
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 1000
    }, 
    "zero_optimization": {
        "stage": 2, 
        "contiguous_gradients": true, 
        "overlap_comm": true
    }, 
    "steps_per_print": 50, 
    "gradient_clipping": 1.0
}
****START TRAINING****
image_size: 8, batch_size: 256, timesteps: 1000, time_emb_dim: 256
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]moe_out的形状: torch.Size([256, 1])
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/main.py", line 9, in <module>
[rank0]:     train_deepspeed(cfg)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py", line 155, in train_deepspeed
[rank0]:     noisy_images, noise = forward_diffusion_with_moe(
[rank0]:                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/diffusion.py", line 101, in forward_diffusion_with_moe
[rank0]:     assert pred_params.shape == x0.shape, f"模型输出形状{pred_params.shape}与输入{x0.shape}不匹配"
[rank0]: AssertionError: 模型输出形状torch.Size([256, 1])与输入torch.Size([256, 1, 8, 8])不匹配
[rank0]:[W401 04:39:21.534051524 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-04-01 04:39:22,382] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 77431
[2025-04-01 04:39:22,383] [ERROR] [launch.py:325:sigkill_handler] ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1'] exits with return code = 1
[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ bash scripts/script_normal_dif_test.sh 
[?2004l[2025-04-01 04:39:31,349] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:39:33,630] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 04:39:33,630] [INFO] [runner.py:605:main] cmd = /hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None main.py --image_size 8 --batch_size 256 --num_epochs 200 --timesteps 1000 --lr 2e-4 --time_emb_dim 256 --mu1 4 --sigma1 1 --num1 9000 --mu2 10 --sigma2 4 --num2 1000 --samples_dir ./samples --checkpoints_dir ./checkpoints --fp16 --mode train --model_path  --num_images 10000 --simulation_distribution normal --use_different_noise --use_moe --num_experts 4 --moe_hidden_dim 64 --moe_tau 0.1
[2025-04-01 04:39:35,351] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:39:37,541] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 04:39:37,542] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 04:39:37,542] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 04:39:37,542] [INFO] [launch.py:164:main] dist_world_size=1
[2025-04-01 04:39:37,542] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 04:39:37,656] [INFO] [launch.py:256:main] process 77947 spawned with command: ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1']
[2025-04-01 04:39:40,383] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:39:43,239] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.16.5, git-hash=unknown, git-branch=unknown
[2025-04-01 04:39:43,239] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-01 04:39:43,239] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-04-01 04:39:43,243] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 1
[2025-04-01 04:39:44,407] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124/fused_adam/build.ninja...
/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.18419909477233887 seconds
[2025-04-01 04:39:44,596] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-04-01 04:39:44,597] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-04-01 04:39:44,601] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-04-01 04:39:44,602] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-04-01 04:39:44,602] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2025-04-01 04:39:44,602] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000
[2025-04-01 04:39:44,602] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000
[2025-04-01 04:39:44,602] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-04-01 04:39:44,602] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-04-01 04:39:44,947] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-04-01 04:39:44,948] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:39:44,948] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.05 GB, percent = 6.7%
[2025-04-01 04:39:45,058] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-04-01 04:39:45,058] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:39:45,058] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.06 GB, percent = 6.7%
[2025-04-01 04:39:45,058] [INFO] [stage_1_and_2.py:556:__init__] optimizer state initialized
[2025-04-01 04:39:45,175] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-04-01 04:39:45,176] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:39:45,176] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.07 GB, percent = 6.7%
[2025-04-01 04:39:45,178] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-04-01 04:39:45,178] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-04-01 04:39:45,178] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-04-01 04:39:45,178] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:39:45,178] [INFO] [config.py:1000:print] DeepSpeedEngine configuration:
[2025-04-01 04:39:45,178] [INFO] [config.py:1004:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-04-01 04:39:45,178] [INFO] [config.py:1004:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-04-01 04:39:45,178] [INFO] [config.py:1004:print]   amp_enabled .................. False
[2025-04-01 04:39:45,178] [INFO] [config.py:1004:print]   amp_params ................... False
[2025-04-01 04:39:45,179] [INFO] [config.py:1004:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-01 04:39:45,179] [INFO] [config.py:1004:print]   bfloat16_enabled ............. False
[2025-04-01 04:39:45,179] [INFO] [config.py:1004:print]   bfloat16_immediate_grad_update  False
[2025-04-01 04:39:45,179] [INFO] [config.py:1004:print]   checkpoint_parallel_write_pipeline  False
[2025-04-01 04:39:45,179] [INFO] [config.py:1004:print]   checkpoint_tag_validation_enabled  True
[2025-04-01 04:39:45,179] [INFO] [config.py:1004:print]   checkpoint_tag_validation_fail  False
[2025-04-01 04:39:45,179] [INFO] [config.py:1004:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7faa0a77e450>
[2025-04-01 04:39:45,179] [INFO] [config.py:1004:print]   communication_data_type ...... None
[2025-04-01 04:39:45,179] [INFO] [config.py:1004:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-01 04:39:45,179] [INFO] [config.py:1004:print]   curriculum_enabled_legacy .... False
[2025-04-01 04:39:45,179] [INFO] [config.py:1004:print]   curriculum_params_legacy ..... False
[2025-04-01 04:39:45,179] [INFO] [config.py:1004:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-01 04:39:45,179] [INFO] [config.py:1004:print]   data_efficiency_enabled ...... False
[2025-04-01 04:39:45,179] [INFO] [config.py:1004:print]   dataloader_drop_last ......... False
[2025-04-01 04:39:45,179] [INFO] [config.py:1004:print]   disable_allgather ............ False
[2025-04-01 04:39:45,179] [INFO] [config.py:1004:print]   dump_state ................... False
[2025-04-01 04:39:45,179] [INFO] [config.py:1004:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-04-01 04:39:45,179] [INFO] [config.py:1004:print]   eigenvalue_enabled ........... False
[2025-04-01 04:39:45,179] [INFO] [config.py:1004:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-01 04:39:45,179] [INFO] [config.py:1004:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-01 04:39:45,179] [INFO] [config.py:1004:print]   eigenvalue_layer_num ......... 0
[2025-04-01 04:39:45,179] [INFO] [config.py:1004:print]   eigenvalue_max_iter .......... 100
[2025-04-01 04:39:45,179] [INFO] [config.py:1004:print]   eigenvalue_stability ......... 1e-06
[2025-04-01 04:39:45,179] [INFO] [config.py:1004:print]   eigenvalue_tol ............... 0.01
[2025-04-01 04:39:45,179] [INFO] [config.py:1004:print]   eigenvalue_verbose ........... False
[2025-04-01 04:39:45,179] [INFO] [config.py:1004:print]   elasticity_enabled ........... False
[2025-04-01 04:39:45,179] [INFO] [config.py:1004:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-04-01 04:39:45,179] [INFO] [config.py:1004:print]   fp16_auto_cast ............... False
[2025-04-01 04:39:45,179] [INFO] [config.py:1004:print]   fp16_enabled ................. True
[2025-04-01 04:39:45,179] [INFO] [config.py:1004:print]   fp16_master_weights_and_gradients  False
[2025-04-01 04:39:45,179] [INFO] [config.py:1004:print]   global_rank .................. 0
[2025-04-01 04:39:45,179] [INFO] [config.py:1004:print]   grad_accum_dtype ............. None
[2025-04-01 04:39:45,179] [INFO] [config.py:1004:print]   gradient_accumulation_steps .. 1
[2025-04-01 04:39:45,179] [INFO] [config.py:1004:print]   gradient_clipping ............ 1.0
[2025-04-01 04:39:45,179] [INFO] [config.py:1004:print]   gradient_predivide_factor .... 1.0
[2025-04-01 04:39:45,179] [INFO] [config.py:1004:print]   graph_harvesting ............. False
[2025-04-01 04:39:45,179] [INFO] [config.py:1004:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-01 04:39:45,179] [INFO] [config.py:1004:print]   initial_dynamic_scale ........ 65536
[2025-04-01 04:39:45,180] [INFO] [config.py:1004:print]   load_universal_checkpoint .... False
[2025-04-01 04:39:45,180] [INFO] [config.py:1004:print]   loss_scale ................... 0
[2025-04-01 04:39:45,180] [INFO] [config.py:1004:print]   memory_breakdown ............. False
[2025-04-01 04:39:45,180] [INFO] [config.py:1004:print]   mics_hierarchial_params_gather  False
[2025-04-01 04:39:45,180] [INFO] [config.py:1004:print]   mics_shard_size .............. -1
[2025-04-01 04:39:45,180] [INFO] [config.py:1004:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-04-01 04:39:45,180] [INFO] [config.py:1004:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-04-01 04:39:45,180] [INFO] [config.py:1004:print]   optimizer_legacy_fusion ...... False
[2025-04-01 04:39:45,180] [INFO] [config.py:1004:print]   optimizer_name ............... adamw
[2025-04-01 04:39:45,180] [INFO] [config.py:1004:print]   optimizer_params ............. {'lr': 0.0002, 'weight_decay': 0.01}
[2025-04-01 04:39:45,180] [INFO] [config.py:1004:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-04-01 04:39:45,180] [INFO] [config.py:1004:print]   pld_enabled .................. False
[2025-04-01 04:39:45,180] [INFO] [config.py:1004:print]   pld_params ................... False
[2025-04-01 04:39:45,180] [INFO] [config.py:1004:print]   prescale_gradients ........... False
[2025-04-01 04:39:45,180] [INFO] [config.py:1004:print]   scheduler_name ............... None
[2025-04-01 04:39:45,180] [INFO] [config.py:1004:print]   scheduler_params ............. None
[2025-04-01 04:39:45,180] [INFO] [config.py:1004:print]   seq_parallel_communication_data_type  torch.float32
[2025-04-01 04:39:45,180] [INFO] [config.py:1004:print]   sparse_attention ............. None
[2025-04-01 04:39:45,180] [INFO] [config.py:1004:print]   sparse_gradients_enabled ..... False
[2025-04-01 04:39:45,180] [INFO] [config.py:1004:print]   steps_per_print .............. 50
[2025-04-01 04:39:45,180] [INFO] [config.py:1004:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-04-01 04:39:45,180] [INFO] [config.py:1004:print]   timers_config ................ enabled=True synchronized=True
[2025-04-01 04:39:45,180] [INFO] [config.py:1004:print]   train_batch_size ............. 256
[2025-04-01 04:39:45,180] [INFO] [config.py:1004:print]   train_micro_batch_size_per_gpu  256
[2025-04-01 04:39:45,180] [INFO] [config.py:1004:print]   use_data_before_expert_parallel_  False
[2025-04-01 04:39:45,180] [INFO] [config.py:1004:print]   use_node_local_storage ....... False
[2025-04-01 04:39:45,180] [INFO] [config.py:1004:print]   wall_clock_breakdown ......... False
[2025-04-01 04:39:45,180] [INFO] [config.py:1004:print]   weight_quantization_config ... None
[2025-04-01 04:39:45,180] [INFO] [config.py:1004:print]   world_size ................... 1
[2025-04-01 04:39:45,180] [INFO] [config.py:1004:print]   zero_allow_untested_optimizer  False
[2025-04-01 04:39:45,180] [INFO] [config.py:1004:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-04-01 04:39:45,180] [INFO] [config.py:1004:print]   zero_enabled ................. True
[2025-04-01 04:39:45,180] [INFO] [config.py:1004:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-01 04:39:45,180] [INFO] [config.py:1004:print]   zero_optimization_stage ...... 2
[2025-04-01 04:39:45,180] [INFO] [config.py:990:print_user_config]   json = {
    "train_batch_size": 256, 
    "gradient_accumulation_steps": 1, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0002, 
            "weight_decay": 0.01
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 1000
    }, 
    "zero_optimization": {
        "stage": 2, 
        "contiguous_gradients": true, 
        "overlap_comm": true
    }, 
    "steps_per_print": 50, 
    "gradient_clipping": 1.0
}
****START TRAINING****
image_size: 8, batch_size: 256, timesteps: 1000, time_emb_dim: 256
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]moe_out的形状: torch.Size([256, 1])
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/main.py", line 9, in <module>
[rank0]:     train_deepspeed(cfg)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py", line 155, in train_deepspeed
[rank0]:     noisy_images, noise = forward_diffusion_with_moe(
[rank0]:                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/diffusion.py", line 101, in forward_diffusion_with_moe
[rank0]:     assert pred_params.shape == x0.shape, f"模型输出形状{pred_params.shape}与输入{x0.shape}不匹配"
[rank0]: AssertionError: 模型输出形状torch.Size([256, 1])与输入torch.Size([256, 1, 8, 8])不匹配
[rank0]:[W401 04:39:46.443707520 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-04-01 04:39:47,658] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 77947
[2025-04-01 04:39:47,659] [ERROR] [launch.py:325:sigkill_handler] ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1'] exits with return code = 1
[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ bash scripts/script_normal_dif_test.sh 
[?2004l[2025-04-01 04:40:18,531] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:40:20,868] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 04:40:20,869] [INFO] [runner.py:605:main] cmd = /hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None main.py --image_size 8 --batch_size 256 --num_epochs 200 --timesteps 1000 --lr 2e-4 --time_emb_dim 256 --mu1 4 --sigma1 1 --num1 9000 --mu2 10 --sigma2 4 --num2 1000 --samples_dir ./samples --checkpoints_dir ./checkpoints --fp16 --mode train --model_path  --num_images 10000 --simulation_distribution normal --use_different_noise --use_moe --num_experts 4 --moe_hidden_dim 64 --moe_tau 0.1
[2025-04-01 04:40:22,619] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:40:24,868] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 04:40:24,869] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 04:40:24,869] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 04:40:24,869] [INFO] [launch.py:164:main] dist_world_size=1
[2025-04-01 04:40:24,869] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 04:40:24,987] [INFO] [launch.py:256:main] process 78485 spawned with command: ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1']
[2025-04-01 04:40:27,838] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:40:30,844] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.16.5, git-hash=unknown, git-branch=unknown
[2025-04-01 04:40:30,844] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-01 04:40:30,844] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-04-01 04:40:30,849] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 1
[2025-04-01 04:40:32,051] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124/fused_adam/build.ninja...
/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.17917180061340332 seconds
[2025-04-01 04:40:32,235] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-04-01 04:40:32,236] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-04-01 04:40:32,240] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-04-01 04:40:32,241] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-04-01 04:40:32,241] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2025-04-01 04:40:32,241] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000
[2025-04-01 04:40:32,241] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000
[2025-04-01 04:40:32,241] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-04-01 04:40:32,241] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-04-01 04:40:32,651] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-04-01 04:40:32,652] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:40:32,652] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.08 GB, percent = 6.7%
[2025-04-01 04:40:32,765] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-04-01 04:40:32,765] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:40:32,766] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.08 GB, percent = 6.7%
[2025-04-01 04:40:32,766] [INFO] [stage_1_and_2.py:556:__init__] optimizer state initialized
[2025-04-01 04:40:32,881] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-04-01 04:40:32,882] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:40:32,882] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.08 GB, percent = 6.7%
[2025-04-01 04:40:32,884] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-04-01 04:40:32,884] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-04-01 04:40:32,884] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-04-01 04:40:32,884] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:40:32,884] [INFO] [config.py:1000:print] DeepSpeedEngine configuration:
[2025-04-01 04:40:32,884] [INFO] [config.py:1004:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-04-01 04:40:32,885] [INFO] [config.py:1004:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-04-01 04:40:32,885] [INFO] [config.py:1004:print]   amp_enabled .................. False
[2025-04-01 04:40:32,885] [INFO] [config.py:1004:print]   amp_params ................... False
[2025-04-01 04:40:32,885] [INFO] [config.py:1004:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-01 04:40:32,885] [INFO] [config.py:1004:print]   bfloat16_enabled ............. False
[2025-04-01 04:40:32,885] [INFO] [config.py:1004:print]   bfloat16_immediate_grad_update  False
[2025-04-01 04:40:32,885] [INFO] [config.py:1004:print]   checkpoint_parallel_write_pipeline  False
[2025-04-01 04:40:32,885] [INFO] [config.py:1004:print]   checkpoint_tag_validation_enabled  True
[2025-04-01 04:40:32,885] [INFO] [config.py:1004:print]   checkpoint_tag_validation_fail  False
[2025-04-01 04:40:32,885] [INFO] [config.py:1004:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f7a5896a0f0>
[2025-04-01 04:40:32,885] [INFO] [config.py:1004:print]   communication_data_type ...... None
[2025-04-01 04:40:32,885] [INFO] [config.py:1004:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-01 04:40:32,885] [INFO] [config.py:1004:print]   curriculum_enabled_legacy .... False
[2025-04-01 04:40:32,885] [INFO] [config.py:1004:print]   curriculum_params_legacy ..... False
[2025-04-01 04:40:32,885] [INFO] [config.py:1004:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-01 04:40:32,885] [INFO] [config.py:1004:print]   data_efficiency_enabled ...... False
[2025-04-01 04:40:32,885] [INFO] [config.py:1004:print]   dataloader_drop_last ......... False
[2025-04-01 04:40:32,885] [INFO] [config.py:1004:print]   disable_allgather ............ False
[2025-04-01 04:40:32,885] [INFO] [config.py:1004:print]   dump_state ................... False
[2025-04-01 04:40:32,885] [INFO] [config.py:1004:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-04-01 04:40:32,885] [INFO] [config.py:1004:print]   eigenvalue_enabled ........... False
[2025-04-01 04:40:32,885] [INFO] [config.py:1004:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-01 04:40:32,885] [INFO] [config.py:1004:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-01 04:40:32,885] [INFO] [config.py:1004:print]   eigenvalue_layer_num ......... 0
[2025-04-01 04:40:32,885] [INFO] [config.py:1004:print]   eigenvalue_max_iter .......... 100
[2025-04-01 04:40:32,885] [INFO] [config.py:1004:print]   eigenvalue_stability ......... 1e-06
[2025-04-01 04:40:32,885] [INFO] [config.py:1004:print]   eigenvalue_tol ............... 0.01
[2025-04-01 04:40:32,885] [INFO] [config.py:1004:print]   eigenvalue_verbose ........... False
[2025-04-01 04:40:32,885] [INFO] [config.py:1004:print]   elasticity_enabled ........... False
[2025-04-01 04:40:32,885] [INFO] [config.py:1004:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-04-01 04:40:32,885] [INFO] [config.py:1004:print]   fp16_auto_cast ............... False
[2025-04-01 04:40:32,885] [INFO] [config.py:1004:print]   fp16_enabled ................. True
[2025-04-01 04:40:32,885] [INFO] [config.py:1004:print]   fp16_master_weights_and_gradients  False
[2025-04-01 04:40:32,885] [INFO] [config.py:1004:print]   global_rank .................. 0
[2025-04-01 04:40:32,885] [INFO] [config.py:1004:print]   grad_accum_dtype ............. None
[2025-04-01 04:40:32,885] [INFO] [config.py:1004:print]   gradient_accumulation_steps .. 1
[2025-04-01 04:40:32,885] [INFO] [config.py:1004:print]   gradient_clipping ............ 1.0
[2025-04-01 04:40:32,886] [INFO] [config.py:1004:print]   gradient_predivide_factor .... 1.0
[2025-04-01 04:40:32,886] [INFO] [config.py:1004:print]   graph_harvesting ............. False
[2025-04-01 04:40:32,886] [INFO] [config.py:1004:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-01 04:40:32,886] [INFO] [config.py:1004:print]   initial_dynamic_scale ........ 65536
[2025-04-01 04:40:32,886] [INFO] [config.py:1004:print]   load_universal_checkpoint .... False
[2025-04-01 04:40:32,886] [INFO] [config.py:1004:print]   loss_scale ................... 0
[2025-04-01 04:40:32,886] [INFO] [config.py:1004:print]   memory_breakdown ............. False
[2025-04-01 04:40:32,886] [INFO] [config.py:1004:print]   mics_hierarchial_params_gather  False
[2025-04-01 04:40:32,886] [INFO] [config.py:1004:print]   mics_shard_size .............. -1
[2025-04-01 04:40:32,886] [INFO] [config.py:1004:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-04-01 04:40:32,886] [INFO] [config.py:1004:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-04-01 04:40:32,886] [INFO] [config.py:1004:print]   optimizer_legacy_fusion ...... False
[2025-04-01 04:40:32,886] [INFO] [config.py:1004:print]   optimizer_name ............... adamw
[2025-04-01 04:40:32,886] [INFO] [config.py:1004:print]   optimizer_params ............. {'lr': 0.0002, 'weight_decay': 0.01}
[2025-04-01 04:40:32,886] [INFO] [config.py:1004:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-04-01 04:40:32,886] [INFO] [config.py:1004:print]   pld_enabled .................. False
[2025-04-01 04:40:32,886] [INFO] [config.py:1004:print]   pld_params ................... False
[2025-04-01 04:40:32,886] [INFO] [config.py:1004:print]   prescale_gradients ........... False
[2025-04-01 04:40:32,886] [INFO] [config.py:1004:print]   scheduler_name ............... None
[2025-04-01 04:40:32,886] [INFO] [config.py:1004:print]   scheduler_params ............. None
[2025-04-01 04:40:32,886] [INFO] [config.py:1004:print]   seq_parallel_communication_data_type  torch.float32
[2025-04-01 04:40:32,886] [INFO] [config.py:1004:print]   sparse_attention ............. None
[2025-04-01 04:40:32,886] [INFO] [config.py:1004:print]   sparse_gradients_enabled ..... False
[2025-04-01 04:40:32,886] [INFO] [config.py:1004:print]   steps_per_print .............. 50
[2025-04-01 04:40:32,886] [INFO] [config.py:1004:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-04-01 04:40:32,886] [INFO] [config.py:1004:print]   timers_config ................ enabled=True synchronized=True
[2025-04-01 04:40:32,886] [INFO] [config.py:1004:print]   train_batch_size ............. 256
[2025-04-01 04:40:32,886] [INFO] [config.py:1004:print]   train_micro_batch_size_per_gpu  256
[2025-04-01 04:40:32,886] [INFO] [config.py:1004:print]   use_data_before_expert_parallel_  False
[2025-04-01 04:40:32,886] [INFO] [config.py:1004:print]   use_node_local_storage ....... False
[2025-04-01 04:40:32,886] [INFO] [config.py:1004:print]   wall_clock_breakdown ......... False
[2025-04-01 04:40:32,886] [INFO] [config.py:1004:print]   weight_quantization_config ... None
[2025-04-01 04:40:32,886] [INFO] [config.py:1004:print]   world_size ................... 1
[2025-04-01 04:40:32,886] [INFO] [config.py:1004:print]   zero_allow_untested_optimizer  False
[2025-04-01 04:40:32,886] [INFO] [config.py:1004:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-04-01 04:40:32,886] [INFO] [config.py:1004:print]   zero_enabled ................. True
[2025-04-01 04:40:32,886] [INFO] [config.py:1004:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-01 04:40:32,886] [INFO] [config.py:1004:print]   zero_optimization_stage ...... 2
[2025-04-01 04:40:32,886] [INFO] [config.py:990:print_user_config]   json = {
    "train_batch_size": 256, 
    "gradient_accumulation_steps": 1, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0002, 
            "weight_decay": 0.01
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 1000
    }, 
    "zero_optimization": {
        "stage": 2, 
        "contiguous_gradients": true, 
        "overlap_comm": true
    }, 
    "steps_per_print": 50, 
    "gradient_clipping": 1.0
}
****START TRAINING****
image_size: 8, batch_size: 256, timesteps: 1000, time_emb_dim: 256
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s][2025-04-01 04:40:34,071] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:03,  1.02it/s][2025-04-01 04:40:34,138] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:01<00:00,  3.05it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.76it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.53it/s]
/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py:190: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True) # 使用 pd.concat 追加数据
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.26it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.51it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.53it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.36it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.94it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.49it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.92it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.86it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.96it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.18it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.87it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.93it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.94it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.11it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.82it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.98it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.93it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.16it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.85it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.00it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.95it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.26it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.99it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.94it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.96it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.34it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.98it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.89it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.75it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.21it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 11.03it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.97it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  5.03it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.36it/s][2025-04-01 04:40:39,757] [INFO] [logging.py:107:log_dist] [Rank 0] step=50, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:40:39,757] [INFO] [timer.py:264:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=2786.3485587483415, CurrSamplesPerSec=3519.9664967117087, MemAllocated=0.02GB, MaxMemAllocated=1.9GB
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 11.11it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.06it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.84it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.23it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 11.06it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.01it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.87it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.41it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.95it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.04it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.85it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.56it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 11.05it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.97it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.96it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.64it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 11.10it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.23it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.93it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.18it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.80it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.75it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.78it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.02it/s][2025-04-01 04:40:43,245] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 11.14it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.86it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.31it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.50it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.39it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.32it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.34it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.59it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.59it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.42it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.16it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.41it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.40it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.28it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:01,  3.58it/s] 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  7.64it/s][2025-04-01 04:40:45,810] [INFO] [logging.py:107:log_dist] [Rank 0] step=100, skipped=3, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:40:45,812] [INFO] [timer.py:264:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=2774.73782221576, CurrSamplesPerSec=3335.3042752414735, MemAllocated=0.02GB, MaxMemAllocated=1.9GB
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.61it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.61it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/main.py", line 9, in <module>
[rank0]:     train_deepspeed(cfg)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py", line 210, in train_deepspeed
[rank0]:     generate_during_training(model_engine, sample_dir, config, epoch, num_images=config.num_images//config.image_size)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/generate.py", line 240, in generate_during_training_simulation_dif
[rank0]:     pred_noise = model_engine(x, t_batch)
[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/deepspeed/runtime/engine.py", line 2030, in forward
[rank0]:     loss = self.module(*inputs, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank0]:     return inner()
[rank0]:            ^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/unet.py", line 257, in forward
[rank0]:     assert x.dim() == 4, f"输入必须是4D张量，但得到 {x.shape}"
[rank0]: AssertionError: 输入必须是4D张量，但得到 torch.Size([1250, 1])
[rank0]:[W401 04:40:46.538335141 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-04-01 04:40:47,990] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 78485
[2025-04-01 04:40:47,991] [ERROR] [launch.py:325:sigkill_handler] ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1'] exits with return code = 1
[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ bash scripts/script_normal_dif_test.sh 
[?2004l[2025-04-01 04:41:11,713] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:41:14,036] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 04:41:14,036] [INFO] [runner.py:605:main] cmd = /hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None main.py --image_size 8 --batch_size 256 --num_epochs 200 --timesteps 1000 --lr 2e-4 --time_emb_dim 256 --mu1 4 --sigma1 1 --num1 9000 --mu2 10 --sigma2 4 --num2 1000 --samples_dir ./samples --checkpoints_dir ./checkpoints --fp16 --mode train --model_path  --num_images 10000 --simulation_distribution normal --use_different_noise --use_moe --num_experts 4 --moe_hidden_dim 64 --moe_tau 0.1
[2025-04-01 04:41:15,734] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:41:17,904] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 04:41:17,904] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 04:41:17,904] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 04:41:17,904] [INFO] [launch.py:164:main] dist_world_size=1
[2025-04-01 04:41:17,904] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 04:41:18,019] [INFO] [launch.py:256:main] process 79304 spawned with command: ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1']
[2025-04-01 04:41:20,780] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:41:23,835] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.16.5, git-hash=unknown, git-branch=unknown
[2025-04-01 04:41:23,835] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-01 04:41:23,835] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-04-01 04:41:23,838] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 1
[2025-04-01 04:41:24,956] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124/fused_adam/build.ninja...
/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.1873948574066162 seconds
[2025-04-01 04:41:25,148] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-04-01 04:41:25,149] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-04-01 04:41:25,153] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-04-01 04:41:25,153] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-04-01 04:41:25,154] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2025-04-01 04:41:25,154] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000
[2025-04-01 04:41:25,154] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000
[2025-04-01 04:41:25,154] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-04-01 04:41:25,154] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-04-01 04:41:25,555] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-04-01 04:41:25,556] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:41:25,556] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.09 GB, percent = 6.7%
[2025-04-01 04:41:25,683] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-04-01 04:41:25,684] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:41:25,684] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.1 GB, percent = 6.7%
[2025-04-01 04:41:25,684] [INFO] [stage_1_and_2.py:556:__init__] optimizer state initialized
[2025-04-01 04:41:25,792] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-04-01 04:41:25,792] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:41:25,793] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.1 GB, percent = 6.7%
[2025-04-01 04:41:25,794] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-04-01 04:41:25,794] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-04-01 04:41:25,794] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-04-01 04:41:25,794] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:41:25,795] [INFO] [config.py:1000:print] DeepSpeedEngine configuration:
[2025-04-01 04:41:25,795] [INFO] [config.py:1004:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-04-01 04:41:25,795] [INFO] [config.py:1004:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-04-01 04:41:25,795] [INFO] [config.py:1004:print]   amp_enabled .................. False
[2025-04-01 04:41:25,795] [INFO] [config.py:1004:print]   amp_params ................... False
[2025-04-01 04:41:25,795] [INFO] [config.py:1004:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-01 04:41:25,795] [INFO] [config.py:1004:print]   bfloat16_enabled ............. False
[2025-04-01 04:41:25,795] [INFO] [config.py:1004:print]   bfloat16_immediate_grad_update  False
[2025-04-01 04:41:25,795] [INFO] [config.py:1004:print]   checkpoint_parallel_write_pipeline  False
[2025-04-01 04:41:25,795] [INFO] [config.py:1004:print]   checkpoint_tag_validation_enabled  True
[2025-04-01 04:41:25,795] [INFO] [config.py:1004:print]   checkpoint_tag_validation_fail  False
[2025-04-01 04:41:25,795] [INFO] [config.py:1004:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f7d4e739130>
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   communication_data_type ...... None
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   curriculum_enabled_legacy .... False
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   curriculum_params_legacy ..... False
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   data_efficiency_enabled ...... False
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   dataloader_drop_last ......... False
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   disable_allgather ............ False
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   dump_state ................... False
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   eigenvalue_enabled ........... False
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   eigenvalue_layer_num ......... 0
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   eigenvalue_max_iter .......... 100
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   eigenvalue_stability ......... 1e-06
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   eigenvalue_tol ............... 0.01
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   eigenvalue_verbose ........... False
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   elasticity_enabled ........... False
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   fp16_auto_cast ............... False
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   fp16_enabled ................. True
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   fp16_master_weights_and_gradients  False
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   global_rank .................. 0
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   grad_accum_dtype ............. None
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   gradient_accumulation_steps .. 1
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   gradient_clipping ............ 1.0
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   gradient_predivide_factor .... 1.0
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   graph_harvesting ............. False
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   initial_dynamic_scale ........ 65536
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   load_universal_checkpoint .... False
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   loss_scale ................... 0
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   memory_breakdown ............. False
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   mics_hierarchial_params_gather  False
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   mics_shard_size .............. -1
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   optimizer_legacy_fusion ...... False
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   optimizer_name ............... adamw
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   optimizer_params ............. {'lr': 0.0002, 'weight_decay': 0.01}
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   pld_enabled .................. False
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   pld_params ................... False
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   prescale_gradients ........... False
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   scheduler_name ............... None
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   scheduler_params ............. None
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   seq_parallel_communication_data_type  torch.float32
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   sparse_attention ............. None
[2025-04-01 04:41:25,796] [INFO] [config.py:1004:print]   sparse_gradients_enabled ..... False
[2025-04-01 04:41:25,797] [INFO] [config.py:1004:print]   steps_per_print .............. 50
[2025-04-01 04:41:25,797] [INFO] [config.py:1004:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-04-01 04:41:25,797] [INFO] [config.py:1004:print]   timers_config ................ enabled=True synchronized=True
[2025-04-01 04:41:25,797] [INFO] [config.py:1004:print]   train_batch_size ............. 256
[2025-04-01 04:41:25,797] [INFO] [config.py:1004:print]   train_micro_batch_size_per_gpu  256
[2025-04-01 04:41:25,797] [INFO] [config.py:1004:print]   use_data_before_expert_parallel_  False
[2025-04-01 04:41:25,797] [INFO] [config.py:1004:print]   use_node_local_storage ....... False
[2025-04-01 04:41:25,797] [INFO] [config.py:1004:print]   wall_clock_breakdown ......... False
[2025-04-01 04:41:25,797] [INFO] [config.py:1004:print]   weight_quantization_config ... None
[2025-04-01 04:41:25,797] [INFO] [config.py:1004:print]   world_size ................... 1
[2025-04-01 04:41:25,797] [INFO] [config.py:1004:print]   zero_allow_untested_optimizer  False
[2025-04-01 04:41:25,797] [INFO] [config.py:1004:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-04-01 04:41:25,797] [INFO] [config.py:1004:print]   zero_enabled ................. True
[2025-04-01 04:41:25,797] [INFO] [config.py:1004:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-01 04:41:25,797] [INFO] [config.py:1004:print]   zero_optimization_stage ...... 2
[2025-04-01 04:41:25,797] [INFO] [config.py:990:print_user_config]   json = {
    "train_batch_size": 256, 
    "gradient_accumulation_steps": 1, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0002, 
            "weight_decay": 0.01
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 1000
    }, 
    "zero_optimization": {
        "stage": 2, 
        "contiguous_gradients": true, 
        "overlap_comm": true
    }, 
    "steps_per_print": 50, 
    "gradient_clipping": 1.0
}
****START TRAINING****
image_size: 8, batch_size: 256, timesteps: 1000, time_emb_dim: 256
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
[2025-04-01 04:41:26,828] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:03,  1.02it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
[2025-04-01 04:41:26,900] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:01<00:00,  3.00it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([101, 1, 8, 8])
output的形状: torch.Size([101, 1, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.67it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.49it/s]
/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py:190: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True) # 使用 pd.concat 追加数据
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.97it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.20it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([101, 1, 8, 8])
output的形状: torch.Size([101, 1, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 11.03it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.94it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.57it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.76it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([101, 1, 8, 8])
output的形状: torch.Size([101, 1, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.51it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.49it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.36it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.55it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([101, 1, 8, 8])
output的形状: torch.Size([101, 1, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.51it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.36it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.31it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.56it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([101, 1, 8, 8])
output的形状: torch.Size([101, 1, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.52it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.38it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.30it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.62it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([101, 1, 8, 8])
output的形状: torch.Size([101, 1, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.42it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.33it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.29it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.49it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([101, 1, 8, 8])
output的形状: torch.Size([101, 1, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.52it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.36it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.32it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.65it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([101, 1, 8, 8])
output的形状: torch.Size([101, 1, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.51it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.38it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.36it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.63it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([101, 1, 8, 8])
output的形状: torch.Size([101, 1, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.62it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.42it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.34it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.54it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([101, 1, 8, 8])
output的形状: torch.Size([101, 1, 8, 8])
[2025-04-01 04:41:32,773] [INFO] [logging.py:107:log_dist] [Rank 0] step=50, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:41:32,775] [INFO] [timer.py:264:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=2654.207736750026, CurrSamplesPerSec=3434.0596172002793, MemAllocated=0.02GB, MaxMemAllocated=1.9GB
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.48it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.34it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.32it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.66it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([101, 1, 8, 8])
output的形状: torch.Size([101, 1, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.44it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.37it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.30it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.63it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([101, 1, 8, 8])
output的形状: torch.Size([101, 1, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.42it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.57it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.96it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  9.25it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([101, 1, 8, 8])
output的形状: torch.Size([101, 1, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.78it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.74it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.38it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.58it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([101, 1, 8, 8])
output的形状: torch.Size([101, 1, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.46it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.37it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.37it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.57it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([101, 1, 8, 8])
output的形状: torch.Size([101, 1, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.58it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.39it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.35it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.66it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([101, 1, 8, 8])
output的形状: torch.Size([101, 1, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.44it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.36it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.45it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.65it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([101, 1, 8, 8])
output的形状: torch.Size([101, 1, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.64it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.50it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.34it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.68it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([101, 1, 8, 8])
output的形状: torch.Size([101, 1, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.66it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.47it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.33it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.68it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([101, 1, 8, 8])
output的形状: torch.Size([101, 1, 8, 8])
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.68it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.46it/s]
  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:00,  4.33it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:00<00:00,  8.66it/s]output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([256, 1, 8, 8])
output的形状: torch.Size([101, 1, 8, 8])
output的形状: torch.Size([101, 1, 8, 8])
[2025-04-01 04:41:38,923] [INFO] [logging.py:107:log_dist] [Rank 0] step=100, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:41:38,924] [INFO] [timer.py:264:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=2669.8198428311503, CurrSamplesPerSec=3431.0748733916644, MemAllocated=0.02GB, MaxMemAllocated=1.9GB
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.57it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.40it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/main.py", line 9, in <module>
[rank0]:     train_deepspeed(cfg)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py", line 210, in train_deepspeed
[rank0]:     generate_during_training(model_engine, sample_dir, config, epoch, num_images=config.num_images//config.image_size)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/generate.py", line 240, in generate_during_training_simulation_dif
[rank0]:     pred_noise = model_engine(x, t_batch)
[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/deepspeed/runtime/engine.py", line 2030, in forward
[rank0]:     loss = self.module(*inputs, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank0]:     return inner()
[rank0]:            ^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/unet.py", line 257, in forward
[rank0]:     assert x.dim() == 4, f"输入必须是4D张量，但得到 {x.shape}"
[rank0]: AssertionError: 输入必须是4D张量，但得到 torch.Size([1250, 1])
[rank0]:[W401 04:41:39.555966832 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-04-01 04:41:41,022] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 79304
[2025-04-01 04:41:41,023] [ERROR] [launch.py:325:sigkill_handler] ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1'] exits with return code = 1
[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ bash scripts/script_normal_dif_test.sh 
[?2004l[2025-04-01 04:49:38,669] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:49:40,875] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 04:49:40,875] [INFO] [runner.py:605:main] cmd = /hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None main.py --image_size 8 --batch_size 256 --num_epochs 200 --timesteps 1000 --lr 2e-4 --time_emb_dim 256 --mu1 4 --sigma1 1 --num1 9000 --mu2 10 --sigma2 4 --num2 1000 --samples_dir ./samples --checkpoints_dir ./checkpoints --fp16 --mode train --model_path  --num_images 10000 --simulation_distribution normal --use_different_noise --use_moe --num_experts 4 --moe_hidden_dim 64 --moe_tau 0.1
[2025-04-01 04:49:42,620] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:49:44,783] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 04:49:44,783] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 04:49:44,783] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 04:49:44,783] [INFO] [launch.py:164:main] dist_world_size=1
[2025-04-01 04:49:44,783] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 04:49:44,898] [INFO] [launch.py:256:main] process 80825 spawned with command: ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1']
[2025-04-01 04:49:47,649] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:49:50,541] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.16.5, git-hash=unknown, git-branch=unknown
[2025-04-01 04:49:50,541] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-01 04:49:50,541] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-04-01 04:49:50,546] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 1
[2025-04-01 04:49:51,725] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124/fused_adam/build.ninja...
/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.1946732997894287 seconds
[2025-04-01 04:49:51,926] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-04-01 04:49:51,926] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-04-01 04:49:51,931] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-04-01 04:49:51,931] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-04-01 04:49:51,931] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2025-04-01 04:49:51,931] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000
[2025-04-01 04:49:51,932] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000
[2025-04-01 04:49:51,932] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-04-01 04:49:51,932] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-04-01 04:49:52,358] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-04-01 04:49:52,358] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:49:52,359] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.01 GB, percent = 6.7%
[2025-04-01 04:49:52,471] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-04-01 04:49:52,471] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:49:52,472] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.02 GB, percent = 6.7%
[2025-04-01 04:49:52,472] [INFO] [stage_1_and_2.py:556:__init__] optimizer state initialized
[2025-04-01 04:49:52,577] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-04-01 04:49:52,578] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:49:52,578] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.02 GB, percent = 6.7%
[2025-04-01 04:49:52,579] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-04-01 04:49:52,579] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-04-01 04:49:52,579] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-04-01 04:49:52,580] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:49:52,580] [INFO] [config.py:1000:print] DeepSpeedEngine configuration:
[2025-04-01 04:49:52,580] [INFO] [config.py:1004:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-04-01 04:49:52,580] [INFO] [config.py:1004:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-04-01 04:49:52,580] [INFO] [config.py:1004:print]   amp_enabled .................. False
[2025-04-01 04:49:52,580] [INFO] [config.py:1004:print]   amp_params ................... False
[2025-04-01 04:49:52,580] [INFO] [config.py:1004:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-01 04:49:52,580] [INFO] [config.py:1004:print]   bfloat16_enabled ............. False
[2025-04-01 04:49:52,580] [INFO] [config.py:1004:print]   bfloat16_immediate_grad_update  False
[2025-04-01 04:49:52,580] [INFO] [config.py:1004:print]   checkpoint_parallel_write_pipeline  False
[2025-04-01 04:49:52,580] [INFO] [config.py:1004:print]   checkpoint_tag_validation_enabled  True
[2025-04-01 04:49:52,580] [INFO] [config.py:1004:print]   checkpoint_tag_validation_fail  False
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f971643abd0>
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   communication_data_type ...... None
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   curriculum_enabled_legacy .... False
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   curriculum_params_legacy ..... False
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   data_efficiency_enabled ...... False
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   dataloader_drop_last ......... False
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   disable_allgather ............ False
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   dump_state ................... False
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   eigenvalue_enabled ........... False
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   eigenvalue_layer_num ......... 0
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   eigenvalue_max_iter .......... 100
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   eigenvalue_stability ......... 1e-06
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   eigenvalue_tol ............... 0.01
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   eigenvalue_verbose ........... False
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   elasticity_enabled ........... False
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   fp16_auto_cast ............... False
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   fp16_enabled ................. True
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   fp16_master_weights_and_gradients  False
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   global_rank .................. 0
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   grad_accum_dtype ............. None
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   gradient_accumulation_steps .. 1
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   gradient_clipping ............ 1.0
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   gradient_predivide_factor .... 1.0
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   graph_harvesting ............. False
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   initial_dynamic_scale ........ 65536
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   load_universal_checkpoint .... False
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   loss_scale ................... 0
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   memory_breakdown ............. False
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   mics_hierarchial_params_gather  False
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   mics_shard_size .............. -1
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   optimizer_legacy_fusion ...... False
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   optimizer_name ............... adamw
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   optimizer_params ............. {'lr': 0.0002, 'weight_decay': 0.01}
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   pld_enabled .................. False
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   pld_params ................... False
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   prescale_gradients ........... False
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   scheduler_name ............... None
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   scheduler_params ............. None
[2025-04-01 04:49:52,581] [INFO] [config.py:1004:print]   seq_parallel_communication_data_type  torch.float32
[2025-04-01 04:49:52,582] [INFO] [config.py:1004:print]   sparse_attention ............. None
[2025-04-01 04:49:52,582] [INFO] [config.py:1004:print]   sparse_gradients_enabled ..... False
[2025-04-01 04:49:52,582] [INFO] [config.py:1004:print]   steps_per_print .............. 50
[2025-04-01 04:49:52,582] [INFO] [config.py:1004:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-04-01 04:49:52,582] [INFO] [config.py:1004:print]   timers_config ................ enabled=True synchronized=True
[2025-04-01 04:49:52,582] [INFO] [config.py:1004:print]   train_batch_size ............. 256
[2025-04-01 04:49:52,582] [INFO] [config.py:1004:print]   train_micro_batch_size_per_gpu  256
[2025-04-01 04:49:52,582] [INFO] [config.py:1004:print]   use_data_before_expert_parallel_  False
[2025-04-01 04:49:52,582] [INFO] [config.py:1004:print]   use_node_local_storage ....... False
[2025-04-01 04:49:52,582] [INFO] [config.py:1004:print]   wall_clock_breakdown ......... False
[2025-04-01 04:49:52,582] [INFO] [config.py:1004:print]   weight_quantization_config ... None
[2025-04-01 04:49:52,582] [INFO] [config.py:1004:print]   world_size ................... 1
[2025-04-01 04:49:52,582] [INFO] [config.py:1004:print]   zero_allow_untested_optimizer  False
[2025-04-01 04:49:52,582] [INFO] [config.py:1004:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-04-01 04:49:52,582] [INFO] [config.py:1004:print]   zero_enabled ................. True
[2025-04-01 04:49:52,582] [INFO] [config.py:1004:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-01 04:49:52,582] [INFO] [config.py:1004:print]   zero_optimization_stage ...... 2
[2025-04-01 04:49:52,582] [INFO] [config.py:990:print_user_config]   json = {
    "train_batch_size": 256, 
    "gradient_accumulation_steps": 1, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0002, 
            "weight_decay": 0.01
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 1000
    }, 
    "zero_optimization": {
        "stage": 2, 
        "contiguous_gradients": true, 
        "overlap_comm": true
    }, 
    "steps_per_print": 50, 
    "gradient_clipping": 1.0
}
****START TRAINING****
image_size: 8, batch_size: 256, timesteps: 1000, time_emb_dim: 256
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s][2025-04-01 04:49:53,601] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:03,  1.02it/s][2025-04-01 04:49:53,671] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:01<00:00,  3.01it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.68it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.48it/s]
/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py:190: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True) # 使用 pd.concat 追加数据
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.83it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  9.17it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 11.09it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.30it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.33it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.52it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.34it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.26it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.87it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  9.49it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 11.23it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.20it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.32it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.55it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.59it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.40it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.96it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  9.26it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.92it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.95it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.42it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.60it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.67it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.49it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.90it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  9.34it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 11.05it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.01it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.33it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.65it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.43it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.36it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.35it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.66it/s][2025-04-01 04:49:59,477] [INFO] [logging.py:107:log_dist] [Rank 0] step=50, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:49:59,479] [INFO] [timer.py:264:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=2711.8336266352285, CurrSamplesPerSec=3288.5198247765593, MemAllocated=0.02GB, MaxMemAllocated=1.9GB
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.47it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.38it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.38it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.59it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.39it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.35it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.27it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.45it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.43it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.28it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.44it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.63it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.63it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.47it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.39it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.57it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.58it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.45it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.42it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.60it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.45it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.36it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.97it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.62it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.28it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.57it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.06it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  7.79it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.98it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.84it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.18it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.37it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.24it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.15it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.24it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.49it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.32it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.22it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.38it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.63it/s][2025-04-01 04:50:05,767] [INFO] [logging.py:107:log_dist] [Rank 0] step=100, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:50:05,768] [INFO] [timer.py:264:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=2680.214995288909, CurrSamplesPerSec=3346.1555864618963, MemAllocated=0.02GB, MaxMemAllocated=1.9GB
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.61it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.44it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/main.py", line 9, in <module>
[rank0]:     train_deepspeed(cfg)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py", line 210, in train_deepspeed
[rank0]:     generate_during_training(model_engine, sample_dir, config, epoch, num_images=config.num_images//config.image_size)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/generate.py", line 227, in generate_during_training_simulation_dif
[rank0]:     x = model_engine.module.moe.sample_initial_noise(num_images, config)
[rank0]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/reparam_moe.py", line 126, in sample_initial_noise
[rank0]:     noise_4d = combined_samples.view(num_samples, 1, image_size, image_size)
[rank0]:                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: RuntimeError: shape '[1250, 1, 8, 8]' is invalid for input of size 1250
[rank0]:[W401 04:50:06.410053684 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-04-01 04:50:07,901] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 80825
[2025-04-01 04:50:07,902] [ERROR] [launch.py:325:sigkill_handler] ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1'] exits with return code = 1
[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ bash scripts/script_normal_dif_test.sh 
[?2004l[2025-04-01 04:52:37,850] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:52:40,144] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 04:52:40,144] [INFO] [runner.py:605:main] cmd = /hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None main.py --image_size 8 --batch_size 256 --num_epochs 200 --timesteps 1000 --lr 2e-4 --time_emb_dim 256 --mu1 4 --sigma1 1 --num1 9000 --mu2 10 --sigma2 4 --num2 1000 --samples_dir ./samples --checkpoints_dir ./checkpoints --fp16 --mode train --model_path  --num_images 10000 --simulation_distribution normal --use_different_noise --use_moe --num_experts 4 --moe_hidden_dim 64 --moe_tau 0.1
[2025-04-01 04:52:41,892] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:52:44,190] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 04:52:44,190] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 04:52:44,191] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 04:52:44,191] [INFO] [launch.py:164:main] dist_world_size=1
[2025-04-01 04:52:44,191] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 04:52:44,305] [INFO] [launch.py:256:main] process 81826 spawned with command: ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1']
[2025-04-01 04:52:47,119] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:52:50,242] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.16.5, git-hash=unknown, git-branch=unknown
[2025-04-01 04:52:50,242] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-01 04:52:50,242] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-04-01 04:52:50,246] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 1
[2025-04-01 04:52:51,438] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124/fused_adam/build.ninja...
/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.18080973625183105 seconds
[2025-04-01 04:52:51,624] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-04-01 04:52:51,625] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-04-01 04:52:51,629] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-04-01 04:52:51,630] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-04-01 04:52:51,630] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2025-04-01 04:52:51,630] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000
[2025-04-01 04:52:51,630] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000
[2025-04-01 04:52:51,631] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-04-01 04:52:51,631] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-04-01 04:52:52,071] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-04-01 04:52:52,071] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:52:52,072] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.06 GB, percent = 6.7%
[2025-04-01 04:52:52,186] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-04-01 04:52:52,187] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:52:52,187] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.06 GB, percent = 6.7%
[2025-04-01 04:52:52,187] [INFO] [stage_1_and_2.py:556:__init__] optimizer state initialized
[2025-04-01 04:52:52,295] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-04-01 04:52:52,296] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:52:52,296] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.06 GB, percent = 6.7%
[2025-04-01 04:52:52,297] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-04-01 04:52:52,298] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-04-01 04:52:52,298] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-04-01 04:52:52,298] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:52:52,298] [INFO] [config.py:1000:print] DeepSpeedEngine configuration:
[2025-04-01 04:52:52,298] [INFO] [config.py:1004:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-04-01 04:52:52,298] [INFO] [config.py:1004:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-04-01 04:52:52,298] [INFO] [config.py:1004:print]   amp_enabled .................. False
[2025-04-01 04:52:52,298] [INFO] [config.py:1004:print]   amp_params ................... False
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   bfloat16_enabled ............. False
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   bfloat16_immediate_grad_update  False
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   checkpoint_parallel_write_pipeline  False
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   checkpoint_tag_validation_enabled  True
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   checkpoint_tag_validation_fail  False
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fdc6f20b080>
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   communication_data_type ...... None
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   curriculum_enabled_legacy .... False
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   curriculum_params_legacy ..... False
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   data_efficiency_enabled ...... False
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   dataloader_drop_last ......... False
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   disable_allgather ............ False
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   dump_state ................... False
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   eigenvalue_enabled ........... False
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   eigenvalue_layer_num ......... 0
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   eigenvalue_max_iter .......... 100
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   eigenvalue_stability ......... 1e-06
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   eigenvalue_tol ............... 0.01
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   eigenvalue_verbose ........... False
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   elasticity_enabled ........... False
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   fp16_auto_cast ............... False
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   fp16_enabled ................. True
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   fp16_master_weights_and_gradients  False
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   global_rank .................. 0
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   grad_accum_dtype ............. None
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   gradient_accumulation_steps .. 1
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   gradient_clipping ............ 1.0
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   gradient_predivide_factor .... 1.0
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   graph_harvesting ............. False
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   initial_dynamic_scale ........ 65536
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   load_universal_checkpoint .... False
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   loss_scale ................... 0
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   memory_breakdown ............. False
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   mics_hierarchial_params_gather  False
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   mics_shard_size .............. -1
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-04-01 04:52:52,299] [INFO] [config.py:1004:print]   optimizer_legacy_fusion ...... False
[2025-04-01 04:52:52,300] [INFO] [config.py:1004:print]   optimizer_name ............... adamw
[2025-04-01 04:52:52,300] [INFO] [config.py:1004:print]   optimizer_params ............. {'lr': 0.0002, 'weight_decay': 0.01}
[2025-04-01 04:52:52,300] [INFO] [config.py:1004:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-04-01 04:52:52,300] [INFO] [config.py:1004:print]   pld_enabled .................. False
[2025-04-01 04:52:52,300] [INFO] [config.py:1004:print]   pld_params ................... False
[2025-04-01 04:52:52,300] [INFO] [config.py:1004:print]   prescale_gradients ........... False
[2025-04-01 04:52:52,300] [INFO] [config.py:1004:print]   scheduler_name ............... None
[2025-04-01 04:52:52,300] [INFO] [config.py:1004:print]   scheduler_params ............. None
[2025-04-01 04:52:52,300] [INFO] [config.py:1004:print]   seq_parallel_communication_data_type  torch.float32
[2025-04-01 04:52:52,300] [INFO] [config.py:1004:print]   sparse_attention ............. None
[2025-04-01 04:52:52,300] [INFO] [config.py:1004:print]   sparse_gradients_enabled ..... False
[2025-04-01 04:52:52,300] [INFO] [config.py:1004:print]   steps_per_print .............. 50
[2025-04-01 04:52:52,300] [INFO] [config.py:1004:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-04-01 04:52:52,300] [INFO] [config.py:1004:print]   timers_config ................ enabled=True synchronized=True
[2025-04-01 04:52:52,300] [INFO] [config.py:1004:print]   train_batch_size ............. 256
[2025-04-01 04:52:52,300] [INFO] [config.py:1004:print]   train_micro_batch_size_per_gpu  256
[2025-04-01 04:52:52,300] [INFO] [config.py:1004:print]   use_data_before_expert_parallel_  False
[2025-04-01 04:52:52,300] [INFO] [config.py:1004:print]   use_node_local_storage ....... False
[2025-04-01 04:52:52,300] [INFO] [config.py:1004:print]   wall_clock_breakdown ......... False
[2025-04-01 04:52:52,300] [INFO] [config.py:1004:print]   weight_quantization_config ... None
[2025-04-01 04:52:52,300] [INFO] [config.py:1004:print]   world_size ................... 1
[2025-04-01 04:52:52,300] [INFO] [config.py:1004:print]   zero_allow_untested_optimizer  False
[2025-04-01 04:52:52,300] [INFO] [config.py:1004:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-04-01 04:52:52,300] [INFO] [config.py:1004:print]   zero_enabled ................. True
[2025-04-01 04:52:52,300] [INFO] [config.py:1004:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-01 04:52:52,300] [INFO] [config.py:1004:print]   zero_optimization_stage ...... 2
[2025-04-01 04:52:52,300] [INFO] [config.py:990:print_user_config]   json = {
    "train_batch_size": 256, 
    "gradient_accumulation_steps": 1, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0002, 
            "weight_decay": 0.01
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 1000
    }, 
    "zero_optimization": {
        "stage": 2, 
        "contiguous_gradients": true, 
        "overlap_comm": true
    }, 
    "steps_per_print": 50, 
    "gradient_clipping": 1.0
}
****START TRAINING****
image_size: 8, batch_size: 256, timesteps: 1000, time_emb_dim: 256
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s]  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/main.py", line 9, in <module>
[rank0]:     train_deepspeed(cfg)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py", line 155, in train_deepspeed
[rank0]:     noisy_images, noise = forward_diffusion_with_moe(
[rank0]:                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/diffusion.py", line 101, in forward_diffusion_with_moe
[rank0]:     assert pred_params.shape == x0.shape, f"模型输出形状{pred_params.shape}与输入{x0.shape}不匹配"
[rank0]: AssertionError: 模型输出形状torch.Size([256, 8, 8, 8])与输入torch.Size([256, 1, 8, 8])不匹配
[rank0]:[W401 04:52:53.490041178 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-04-01 04:52:54,378] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 81826
[2025-04-01 04:52:54,379] [ERROR] [launch.py:325:sigkill_handler] ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1'] exits with return code = 1
[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ bash scripts/script_normal_dif_test.sh 
[?2004l[2025-04-01 04:54:48,796] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:54:51,037] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 04:54:51,038] [INFO] [runner.py:605:main] cmd = /hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None main.py --image_size 8 --batch_size 256 --num_epochs 200 --timesteps 1000 --lr 2e-4 --time_emb_dim 256 --mu1 4 --sigma1 1 --num1 9000 --mu2 10 --sigma2 4 --num2 1000 --samples_dir ./samples --checkpoints_dir ./checkpoints --fp16 --mode train --model_path  --num_images 10000 --simulation_distribution normal --use_different_noise --use_moe --num_experts 4 --moe_hidden_dim 64 --moe_tau 0.1
[2025-04-01 04:54:52,788] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:54:55,049] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 04:54:55,049] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 04:54:55,049] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 04:54:55,049] [INFO] [launch.py:164:main] dist_world_size=1
[2025-04-01 04:54:55,049] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 04:54:55,167] [INFO] [launch.py:256:main] process 82525 spawned with command: ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1']
[2025-04-01 04:54:57,936] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:55:00,837] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.16.5, git-hash=unknown, git-branch=unknown
[2025-04-01 04:55:00,837] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-01 04:55:00,837] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-04-01 04:55:00,841] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 1
[2025-04-01 04:55:01,942] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124/fused_adam/build.ninja...
/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.19207262992858887 seconds
[2025-04-01 04:55:02,139] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-04-01 04:55:02,139] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-04-01 04:55:02,144] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-04-01 04:55:02,144] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-04-01 04:55:02,144] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2025-04-01 04:55:02,144] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000
[2025-04-01 04:55:02,144] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000
[2025-04-01 04:55:02,144] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-04-01 04:55:02,144] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-04-01 04:55:02,667] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-04-01 04:55:02,667] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:55:02,667] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.05 GB, percent = 6.7%
[2025-04-01 04:55:02,785] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-04-01 04:55:02,786] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:55:02,786] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.08 GB, percent = 6.7%
[2025-04-01 04:55:02,786] [INFO] [stage_1_and_2.py:556:__init__] optimizer state initialized
[2025-04-01 04:55:02,895] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-04-01 04:55:02,896] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:55:02,896] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.08 GB, percent = 6.7%
[2025-04-01 04:55:02,897] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-04-01 04:55:02,898] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-04-01 04:55:02,898] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-04-01 04:55:02,898] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:55:02,898] [INFO] [config.py:1000:print] DeepSpeedEngine configuration:
[2025-04-01 04:55:02,898] [INFO] [config.py:1004:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-04-01 04:55:02,898] [INFO] [config.py:1004:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-04-01 04:55:02,898] [INFO] [config.py:1004:print]   amp_enabled .................. False
[2025-04-01 04:55:02,898] [INFO] [config.py:1004:print]   amp_params ................... False
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   bfloat16_enabled ............. False
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   bfloat16_immediate_grad_update  False
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   checkpoint_parallel_write_pipeline  False
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   checkpoint_tag_validation_enabled  True
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   checkpoint_tag_validation_fail  False
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f68540c1490>
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   communication_data_type ...... None
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   curriculum_enabled_legacy .... False
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   curriculum_params_legacy ..... False
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   data_efficiency_enabled ...... False
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   dataloader_drop_last ......... False
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   disable_allgather ............ False
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   dump_state ................... False
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   eigenvalue_enabled ........... False
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   eigenvalue_layer_num ......... 0
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   eigenvalue_max_iter .......... 100
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   eigenvalue_stability ......... 1e-06
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   eigenvalue_tol ............... 0.01
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   eigenvalue_verbose ........... False
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   elasticity_enabled ........... False
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   fp16_auto_cast ............... False
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   fp16_enabled ................. True
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   fp16_master_weights_and_gradients  False
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   global_rank .................. 0
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   grad_accum_dtype ............. None
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   gradient_accumulation_steps .. 1
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   gradient_clipping ............ 1.0
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   gradient_predivide_factor .... 1.0
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   graph_harvesting ............. False
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   initial_dynamic_scale ........ 65536
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   load_universal_checkpoint .... False
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   loss_scale ................... 0
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   memory_breakdown ............. False
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   mics_hierarchial_params_gather  False
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   mics_shard_size .............. -1
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-04-01 04:55:02,899] [INFO] [config.py:1004:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-04-01 04:55:02,900] [INFO] [config.py:1004:print]   optimizer_legacy_fusion ...... False
[2025-04-01 04:55:02,900] [INFO] [config.py:1004:print]   optimizer_name ............... adamw
[2025-04-01 04:55:02,900] [INFO] [config.py:1004:print]   optimizer_params ............. {'lr': 0.0002, 'weight_decay': 0.01}
[2025-04-01 04:55:02,900] [INFO] [config.py:1004:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-04-01 04:55:02,900] [INFO] [config.py:1004:print]   pld_enabled .................. False
[2025-04-01 04:55:02,900] [INFO] [config.py:1004:print]   pld_params ................... False
[2025-04-01 04:55:02,900] [INFO] [config.py:1004:print]   prescale_gradients ........... False
[2025-04-01 04:55:02,900] [INFO] [config.py:1004:print]   scheduler_name ............... None
[2025-04-01 04:55:02,900] [INFO] [config.py:1004:print]   scheduler_params ............. None
[2025-04-01 04:55:02,900] [INFO] [config.py:1004:print]   seq_parallel_communication_data_type  torch.float32
[2025-04-01 04:55:02,900] [INFO] [config.py:1004:print]   sparse_attention ............. None
[2025-04-01 04:55:02,900] [INFO] [config.py:1004:print]   sparse_gradients_enabled ..... False
[2025-04-01 04:55:02,900] [INFO] [config.py:1004:print]   steps_per_print .............. 50
[2025-04-01 04:55:02,900] [INFO] [config.py:1004:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-04-01 04:55:02,900] [INFO] [config.py:1004:print]   timers_config ................ enabled=True synchronized=True
[2025-04-01 04:55:02,900] [INFO] [config.py:1004:print]   train_batch_size ............. 256
[2025-04-01 04:55:02,900] [INFO] [config.py:1004:print]   train_micro_batch_size_per_gpu  256
[2025-04-01 04:55:02,900] [INFO] [config.py:1004:print]   use_data_before_expert_parallel_  False
[2025-04-01 04:55:02,900] [INFO] [config.py:1004:print]   use_node_local_storage ....... False
[2025-04-01 04:55:02,900] [INFO] [config.py:1004:print]   wall_clock_breakdown ......... False
[2025-04-01 04:55:02,900] [INFO] [config.py:1004:print]   weight_quantization_config ... None
[2025-04-01 04:55:02,900] [INFO] [config.py:1004:print]   world_size ................... 1
[2025-04-01 04:55:02,900] [INFO] [config.py:1004:print]   zero_allow_untested_optimizer  False
[2025-04-01 04:55:02,900] [INFO] [config.py:1004:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-04-01 04:55:02,900] [INFO] [config.py:1004:print]   zero_enabled ................. True
[2025-04-01 04:55:02,900] [INFO] [config.py:1004:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-01 04:55:02,900] [INFO] [config.py:1004:print]   zero_optimization_stage ...... 2
[2025-04-01 04:55:02,900] [INFO] [config.py:990:print_user_config]   json = {
    "train_batch_size": 256, 
    "gradient_accumulation_steps": 1, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0002, 
            "weight_decay": 0.01
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 1000
    }, 
    "zero_optimization": {
        "stage": 2, 
        "contiguous_gradients": true, 
        "overlap_comm": true
    }, 
    "steps_per_print": 50, 
    "gradient_clipping": 1.0
}
****START TRAINING****
image_size: 8, batch_size: 256, timesteps: 1000, time_emb_dim: 256
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s]  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/main.py", line 9, in <module>
[rank0]:     train_deepspeed(cfg)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py", line 155, in train_deepspeed
[rank0]:     noisy_images, noise = forward_diffusion_with_moe(
[rank0]:                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/diffusion.py", line 99, in forward_diffusion_with_moe
[rank0]:     pred_params = model(x0, t)  # 输出已包含混合高斯的采样结果
[rank0]:                   ^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank0]:     return inner()
[rank0]:            ^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/unet.py", line 264, in forward
[rank0]:     x1 = F.gelu(self.conv1(x))  # [B,64,64,64]
[rank0]:                 ^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/conv.py", line 554, in forward
[rank0]:     return self._conv_forward(input, self.weight, self.bias)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/conv.py", line 549, in _conv_forward
[rank0]:     return F.conv2d(
[rank0]:            ^^^^^^^^^
[rank0]: RuntimeError: Given groups=1, weight of size [8, 8, 3, 3], expected input[256, 1, 8, 8] to have 8 channels, but got 1 channels instead
[rank0]:[W401 04:55:04.960767188 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-04-01 04:55:05,169] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 82525
[2025-04-01 04:55:05,170] [ERROR] [launch.py:325:sigkill_handler] ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1'] exits with return code = 1
[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ bash scripts/script_normal_dif_test.sh 
[?2004l[2025-04-01 04:55:45,414] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:55:47,743] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 04:55:47,744] [INFO] [runner.py:605:main] cmd = /hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None main.py --image_size 8 --batch_size 256 --num_epochs 200 --timesteps 1000 --lr 2e-4 --time_emb_dim 256 --mu1 4 --sigma1 1 --num1 9000 --mu2 10 --sigma2 4 --num2 1000 --samples_dir ./samples --checkpoints_dir ./checkpoints --fp16 --mode train --model_path  --num_images 10000 --simulation_distribution normal --use_different_noise --use_moe --num_experts 4 --moe_hidden_dim 64 --moe_tau 0.1
[2025-04-01 04:55:49,429] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:55:51,619] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 04:55:51,620] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 04:55:51,620] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 04:55:51,620] [INFO] [launch.py:164:main] dist_world_size=1
[2025-04-01 04:55:51,620] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 04:55:51,733] [INFO] [launch.py:256:main] process 83075 spawned with command: ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1']
[2025-04-01 04:55:54,404] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:55:57,151] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.16.5, git-hash=unknown, git-branch=unknown
[2025-04-01 04:55:57,151] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-01 04:55:57,151] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-04-01 04:55:57,154] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 1
[2025-04-01 04:55:58,363] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124/fused_adam/build.ninja...
/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.22734999656677246 seconds
[2025-04-01 04:55:58,595] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-04-01 04:55:58,596] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-04-01 04:55:58,600] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-04-01 04:55:58,600] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-04-01 04:55:58,601] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2025-04-01 04:55:58,601] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000
[2025-04-01 04:55:58,601] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000
[2025-04-01 04:55:58,601] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-04-01 04:55:58,601] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-04-01 04:55:58,964] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-04-01 04:55:58,965] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:55:58,965] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.14 GB, percent = 6.7%
[2025-04-01 04:55:59,077] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-04-01 04:55:59,078] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:55:59,078] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.14 GB, percent = 6.7%
[2025-04-01 04:55:59,078] [INFO] [stage_1_and_2.py:556:__init__] optimizer state initialized
[2025-04-01 04:55:59,183] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-04-01 04:55:59,184] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:55:59,184] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.14 GB, percent = 6.7%
[2025-04-01 04:55:59,186] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-04-01 04:55:59,186] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-04-01 04:55:59,186] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-04-01 04:55:59,186] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:55:59,186] [INFO] [config.py:1000:print] DeepSpeedEngine configuration:
[2025-04-01 04:55:59,187] [INFO] [config.py:1004:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-04-01 04:55:59,187] [INFO] [config.py:1004:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-04-01 04:55:59,187] [INFO] [config.py:1004:print]   amp_enabled .................. False
[2025-04-01 04:55:59,187] [INFO] [config.py:1004:print]   amp_params ................... False
[2025-04-01 04:55:59,187] [INFO] [config.py:1004:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-01 04:55:59,187] [INFO] [config.py:1004:print]   bfloat16_enabled ............. False
[2025-04-01 04:55:59,187] [INFO] [config.py:1004:print]   bfloat16_immediate_grad_update  False
[2025-04-01 04:55:59,187] [INFO] [config.py:1004:print]   checkpoint_parallel_write_pipeline  False
[2025-04-01 04:55:59,187] [INFO] [config.py:1004:print]   checkpoint_tag_validation_enabled  True
[2025-04-01 04:55:59,187] [INFO] [config.py:1004:print]   checkpoint_tag_validation_fail  False
[2025-04-01 04:55:59,187] [INFO] [config.py:1004:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f169e15e8a0>
[2025-04-01 04:55:59,187] [INFO] [config.py:1004:print]   communication_data_type ...... None
[2025-04-01 04:55:59,187] [INFO] [config.py:1004:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-01 04:55:59,187] [INFO] [config.py:1004:print]   curriculum_enabled_legacy .... False
[2025-04-01 04:55:59,187] [INFO] [config.py:1004:print]   curriculum_params_legacy ..... False
[2025-04-01 04:55:59,187] [INFO] [config.py:1004:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-01 04:55:59,187] [INFO] [config.py:1004:print]   data_efficiency_enabled ...... False
[2025-04-01 04:55:59,187] [INFO] [config.py:1004:print]   dataloader_drop_last ......... False
[2025-04-01 04:55:59,187] [INFO] [config.py:1004:print]   disable_allgather ............ False
[2025-04-01 04:55:59,187] [INFO] [config.py:1004:print]   dump_state ................... False
[2025-04-01 04:55:59,187] [INFO] [config.py:1004:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-04-01 04:55:59,187] [INFO] [config.py:1004:print]   eigenvalue_enabled ........... False
[2025-04-01 04:55:59,187] [INFO] [config.py:1004:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-01 04:55:59,187] [INFO] [config.py:1004:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-01 04:55:59,187] [INFO] [config.py:1004:print]   eigenvalue_layer_num ......... 0
[2025-04-01 04:55:59,187] [INFO] [config.py:1004:print]   eigenvalue_max_iter .......... 100
[2025-04-01 04:55:59,187] [INFO] [config.py:1004:print]   eigenvalue_stability ......... 1e-06
[2025-04-01 04:55:59,187] [INFO] [config.py:1004:print]   eigenvalue_tol ............... 0.01
[2025-04-01 04:55:59,187] [INFO] [config.py:1004:print]   eigenvalue_verbose ........... False
[2025-04-01 04:55:59,187] [INFO] [config.py:1004:print]   elasticity_enabled ........... False
[2025-04-01 04:55:59,187] [INFO] [config.py:1004:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-04-01 04:55:59,187] [INFO] [config.py:1004:print]   fp16_auto_cast ............... False
[2025-04-01 04:55:59,187] [INFO] [config.py:1004:print]   fp16_enabled ................. True
[2025-04-01 04:55:59,187] [INFO] [config.py:1004:print]   fp16_master_weights_and_gradients  False
[2025-04-01 04:55:59,187] [INFO] [config.py:1004:print]   global_rank .................. 0
[2025-04-01 04:55:59,188] [INFO] [config.py:1004:print]   grad_accum_dtype ............. None
[2025-04-01 04:55:59,188] [INFO] [config.py:1004:print]   gradient_accumulation_steps .. 1
[2025-04-01 04:55:59,188] [INFO] [config.py:1004:print]   gradient_clipping ............ 1.0
[2025-04-01 04:55:59,188] [INFO] [config.py:1004:print]   gradient_predivide_factor .... 1.0
[2025-04-01 04:55:59,188] [INFO] [config.py:1004:print]   graph_harvesting ............. False
[2025-04-01 04:55:59,188] [INFO] [config.py:1004:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-01 04:55:59,188] [INFO] [config.py:1004:print]   initial_dynamic_scale ........ 65536
[2025-04-01 04:55:59,188] [INFO] [config.py:1004:print]   load_universal_checkpoint .... False
[2025-04-01 04:55:59,188] [INFO] [config.py:1004:print]   loss_scale ................... 0
[2025-04-01 04:55:59,188] [INFO] [config.py:1004:print]   memory_breakdown ............. False
[2025-04-01 04:55:59,188] [INFO] [config.py:1004:print]   mics_hierarchial_params_gather  False
[2025-04-01 04:55:59,188] [INFO] [config.py:1004:print]   mics_shard_size .............. -1
[2025-04-01 04:55:59,188] [INFO] [config.py:1004:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-04-01 04:55:59,188] [INFO] [config.py:1004:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-04-01 04:55:59,188] [INFO] [config.py:1004:print]   optimizer_legacy_fusion ...... False
[2025-04-01 04:55:59,188] [INFO] [config.py:1004:print]   optimizer_name ............... adamw
[2025-04-01 04:55:59,188] [INFO] [config.py:1004:print]   optimizer_params ............. {'lr': 0.0002, 'weight_decay': 0.01}
[2025-04-01 04:55:59,188] [INFO] [config.py:1004:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-04-01 04:55:59,188] [INFO] [config.py:1004:print]   pld_enabled .................. False
[2025-04-01 04:55:59,188] [INFO] [config.py:1004:print]   pld_params ................... False
[2025-04-01 04:55:59,188] [INFO] [config.py:1004:print]   prescale_gradients ........... False
[2025-04-01 04:55:59,188] [INFO] [config.py:1004:print]   scheduler_name ............... None
[2025-04-01 04:55:59,188] [INFO] [config.py:1004:print]   scheduler_params ............. None
[2025-04-01 04:55:59,188] [INFO] [config.py:1004:print]   seq_parallel_communication_data_type  torch.float32
[2025-04-01 04:55:59,188] [INFO] [config.py:1004:print]   sparse_attention ............. None
[2025-04-01 04:55:59,188] [INFO] [config.py:1004:print]   sparse_gradients_enabled ..... False
[2025-04-01 04:55:59,188] [INFO] [config.py:1004:print]   steps_per_print .............. 50
[2025-04-01 04:55:59,188] [INFO] [config.py:1004:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-04-01 04:55:59,188] [INFO] [config.py:1004:print]   timers_config ................ enabled=True synchronized=True
[2025-04-01 04:55:59,188] [INFO] [config.py:1004:print]   train_batch_size ............. 256
[2025-04-01 04:55:59,188] [INFO] [config.py:1004:print]   train_micro_batch_size_per_gpu  256
[2025-04-01 04:55:59,188] [INFO] [config.py:1004:print]   use_data_before_expert_parallel_  False
[2025-04-01 04:55:59,188] [INFO] [config.py:1004:print]   use_node_local_storage ....... False
[2025-04-01 04:55:59,188] [INFO] [config.py:1004:print]   wall_clock_breakdown ......... False
[2025-04-01 04:55:59,188] [INFO] [config.py:1004:print]   weight_quantization_config ... None
[2025-04-01 04:55:59,188] [INFO] [config.py:1004:print]   world_size ................... 1
[2025-04-01 04:55:59,188] [INFO] [config.py:1004:print]   zero_allow_untested_optimizer  False
[2025-04-01 04:55:59,188] [INFO] [config.py:1004:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-04-01 04:55:59,188] [INFO] [config.py:1004:print]   zero_enabled ................. True
[2025-04-01 04:55:59,188] [INFO] [config.py:1004:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-01 04:55:59,188] [INFO] [config.py:1004:print]   zero_optimization_stage ...... 2
[2025-04-01 04:55:59,188] [INFO] [config.py:990:print_user_config]   json = {
    "train_batch_size": 256, 
    "gradient_accumulation_steps": 1, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0002, 
            "weight_decay": 0.01
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 1000
    }, 
    "zero_optimization": {
        "stage": 2, 
        "contiguous_gradients": true, 
        "overlap_comm": true
    }, 
    "steps_per_print": 50, 
    "gradient_clipping": 1.0
}
****START TRAINING****
image_size: 8, batch_size: 256, timesteps: 1000, time_emb_dim: 256
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s]x0形状: torch.Size([256, 1, 8, 8]), t形状: torch.Size([256])
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/main.py", line 9, in <module>
[rank0]:     train_deepspeed(cfg)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py", line 155, in train_deepspeed
[rank0]:     noisy_images, noise = forward_diffusion_with_moe(
[rank0]:                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/diffusion.py", line 99, in forward_diffusion_with_moe
[rank0]:     pred_params = model(x0, t)  # 输出已包含混合高斯的采样结果
[rank0]:                   ^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank0]:     return inner()
[rank0]:            ^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/unet.py", line 264, in forward
[rank0]:     x1 = F.gelu(self.conv1(x))  # [B,64,64,64]
[rank0]:                 ^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/conv.py", line 554, in forward
[rank0]:     return self._conv_forward(input, self.weight, self.bias)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/conv.py", line 549, in _conv_forward
[rank0]:     return F.conv2d(
[rank0]:            ^^^^^^^^^
[rank0]: RuntimeError: Given groups=1, weight of size [8, 8, 3, 3], expected input[256, 1, 8, 8] to have 8 channels, but got 1 channels instead
[rank0]:[W401 04:56:00.087549105 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-04-01 04:56:00,735] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 83075
[2025-04-01 04:56:00,736] [ERROR] [launch.py:325:sigkill_handler] ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1'] exits with return code = 1
[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ bash scripts/script_normal_dif_test.sh 
[?2004l[2025-04-01 04:58:03,202] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:58:05,510] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 04:58:05,510] [INFO] [runner.py:605:main] cmd = /hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None main.py --image_size 8 --batch_size 256 --num_epochs 200 --timesteps 1000 --lr 2e-4 --time_emb_dim 256 --mu1 4 --sigma1 1 --num1 9000 --mu2 10 --sigma2 4 --num2 1000 --samples_dir ./samples --checkpoints_dir ./checkpoints --fp16 --mode train --model_path  --num_images 10000 --simulation_distribution normal --use_different_noise --use_moe --num_experts 4 --moe_hidden_dim 64 --moe_tau 0.1
[2025-04-01 04:58:07,233] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:58:09,461] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 04:58:09,461] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 04:58:09,461] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 04:58:09,461] [INFO] [launch.py:164:main] dist_world_size=1
[2025-04-01 04:58:09,461] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 04:58:09,575] [INFO] [launch.py:256:main] process 83694 spawned with command: ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1']
[2025-04-01 04:58:12,278] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:58:15,243] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.16.5, git-hash=unknown, git-branch=unknown
[2025-04-01 04:58:15,243] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-01 04:58:15,244] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-04-01 04:58:15,247] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 1
[2025-04-01 04:58:16,559] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124/fused_adam/build.ninja...
/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.1804943084716797 seconds
[2025-04-01 04:58:16,744] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-04-01 04:58:16,744] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-04-01 04:58:16,749] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-04-01 04:58:16,749] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-04-01 04:58:16,750] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2025-04-01 04:58:16,750] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000
[2025-04-01 04:58:16,750] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000
[2025-04-01 04:58:16,750] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-04-01 04:58:16,750] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-04-01 04:58:17,161] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-04-01 04:58:17,162] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:58:17,162] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.09 GB, percent = 6.7%
[2025-04-01 04:58:17,273] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-04-01 04:58:17,273] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:58:17,273] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.1 GB, percent = 6.7%
[2025-04-01 04:58:17,273] [INFO] [stage_1_and_2.py:556:__init__] optimizer state initialized
[2025-04-01 04:58:17,377] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-04-01 04:58:17,377] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:58:17,378] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.1 GB, percent = 6.7%
[2025-04-01 04:58:17,379] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-04-01 04:58:17,379] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-04-01 04:58:17,379] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-04-01 04:58:17,379] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:58:17,380] [INFO] [config.py:1000:print] DeepSpeedEngine configuration:
[2025-04-01 04:58:17,380] [INFO] [config.py:1004:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-04-01 04:58:17,380] [INFO] [config.py:1004:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-04-01 04:58:17,380] [INFO] [config.py:1004:print]   amp_enabled .................. False
[2025-04-01 04:58:17,380] [INFO] [config.py:1004:print]   amp_params ................... False
[2025-04-01 04:58:17,380] [INFO] [config.py:1004:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-01 04:58:17,380] [INFO] [config.py:1004:print]   bfloat16_enabled ............. False
[2025-04-01 04:58:17,380] [INFO] [config.py:1004:print]   bfloat16_immediate_grad_update  False
[2025-04-01 04:58:17,380] [INFO] [config.py:1004:print]   checkpoint_parallel_write_pipeline  False
[2025-04-01 04:58:17,380] [INFO] [config.py:1004:print]   checkpoint_tag_validation_enabled  True
[2025-04-01 04:58:17,380] [INFO] [config.py:1004:print]   checkpoint_tag_validation_fail  False
[2025-04-01 04:58:17,380] [INFO] [config.py:1004:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f0e826e4cb0>
[2025-04-01 04:58:17,380] [INFO] [config.py:1004:print]   communication_data_type ...... None
[2025-04-01 04:58:17,380] [INFO] [config.py:1004:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-01 04:58:17,380] [INFO] [config.py:1004:print]   curriculum_enabled_legacy .... False
[2025-04-01 04:58:17,380] [INFO] [config.py:1004:print]   curriculum_params_legacy ..... False
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   data_efficiency_enabled ...... False
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   dataloader_drop_last ......... False
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   disable_allgather ............ False
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   dump_state ................... False
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   eigenvalue_enabled ........... False
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   eigenvalue_layer_num ......... 0
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   eigenvalue_max_iter .......... 100
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   eigenvalue_stability ......... 1e-06
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   eigenvalue_tol ............... 0.01
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   eigenvalue_verbose ........... False
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   elasticity_enabled ........... False
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   fp16_auto_cast ............... False
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   fp16_enabled ................. True
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   fp16_master_weights_and_gradients  False
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   global_rank .................. 0
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   grad_accum_dtype ............. None
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   gradient_accumulation_steps .. 1
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   gradient_clipping ............ 1.0
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   gradient_predivide_factor .... 1.0
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   graph_harvesting ............. False
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   initial_dynamic_scale ........ 65536
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   load_universal_checkpoint .... False
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   loss_scale ................... 0
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   memory_breakdown ............. False
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   mics_hierarchial_params_gather  False
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   mics_shard_size .............. -1
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   optimizer_legacy_fusion ...... False
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   optimizer_name ............... adamw
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   optimizer_params ............. {'lr': 0.0002, 'weight_decay': 0.01}
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   pld_enabled .................. False
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   pld_params ................... False
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   prescale_gradients ........... False
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   scheduler_name ............... None
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   scheduler_params ............. None
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   seq_parallel_communication_data_type  torch.float32
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   sparse_attention ............. None
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   sparse_gradients_enabled ..... False
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   steps_per_print .............. 50
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-04-01 04:58:17,381] [INFO] [config.py:1004:print]   timers_config ................ enabled=True synchronized=True
[2025-04-01 04:58:17,382] [INFO] [config.py:1004:print]   train_batch_size ............. 256
[2025-04-01 04:58:17,382] [INFO] [config.py:1004:print]   train_micro_batch_size_per_gpu  256
[2025-04-01 04:58:17,382] [INFO] [config.py:1004:print]   use_data_before_expert_parallel_  False
[2025-04-01 04:58:17,382] [INFO] [config.py:1004:print]   use_node_local_storage ....... False
[2025-04-01 04:58:17,382] [INFO] [config.py:1004:print]   wall_clock_breakdown ......... False
[2025-04-01 04:58:17,382] [INFO] [config.py:1004:print]   weight_quantization_config ... None
[2025-04-01 04:58:17,382] [INFO] [config.py:1004:print]   world_size ................... 1
[2025-04-01 04:58:17,382] [INFO] [config.py:1004:print]   zero_allow_untested_optimizer  False
[2025-04-01 04:58:17,382] [INFO] [config.py:1004:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-04-01 04:58:17,382] [INFO] [config.py:1004:print]   zero_enabled ................. True
[2025-04-01 04:58:17,382] [INFO] [config.py:1004:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-01 04:58:17,382] [INFO] [config.py:1004:print]   zero_optimization_stage ...... 2
[2025-04-01 04:58:17,382] [INFO] [config.py:990:print_user_config]   json = {
    "train_batch_size": 256, 
    "gradient_accumulation_steps": 1, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0002, 
            "weight_decay": 0.01
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 1000
    }, 
    "zero_optimization": {
        "stage": 2, 
        "contiguous_gradients": true, 
        "overlap_comm": true
    }, 
    "steps_per_print": 50, 
    "gradient_clipping": 1.0
}
****START TRAINING****
image_size: 8, batch_size: 256, timesteps: 1000, time_emb_dim: 256
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s][2025-04-01 04:58:18,499] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:03,  1.03it/s][2025-04-01 04:58:18,568] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:01<00:00,  3.03it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.71it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.51it/s]
/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py:190: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True) # 使用 pd.concat 追加数据
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.94it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  9.18it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 11.03it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.95it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.40it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.41it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.33it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.51it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.37it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.65it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.50it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.46it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.30it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.56it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.44it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.32it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:01,  3.84it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.06it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.27it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.03it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.31it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.51it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.54it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.39it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.35it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.55it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.50it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.35it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.26it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.57it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.38it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.28it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.39it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.52it/s][2025-04-01 04:58:24,484] [INFO] [logging.py:107:log_dist] [Rank 0] step=50, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:58:24,485] [INFO] [timer.py:264:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=2646.2461348030843, CurrSamplesPerSec=3225.329436289164, MemAllocated=0.02GB, MaxMemAllocated=1.9GB
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.34it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.26it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.32it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.45it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.36it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.23it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.30it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.50it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.40it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.28it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.30it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.54it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.36it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.28it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.32it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.71it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.56it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.42it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.37it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.78it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.68it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.52it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.27it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.46it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.57it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.34it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.31it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.51it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.53it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.36it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.25it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.45it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.45it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.31it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.29it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.53it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.49it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.31it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.82it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.99it/s][2025-04-01 04:58:30,709] [INFO] [logging.py:107:log_dist] [Rank 0] step=100, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:58:30,710] [INFO] [timer.py:264:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=2661.5094394103025, CurrSamplesPerSec=3574.656861499831, MemAllocated=0.02GB, MaxMemAllocated=1.9GB
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.87it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.70it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/main.py", line 9, in <module>
[rank0]:     train_deepspeed(cfg)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py", line 210, in train_deepspeed
[rank0]:     generate_during_training(model_engine, sample_dir, config, epoch, num_images=config.num_images//config.image_size)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/generate.py", line 227, in generate_during_training_simulation_dif
[rank0]:     x = model_engine.module.moe.sample_initial_noise(num_images, config)
[rank0]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/reparam_moe.py", line 126, in sample_initial_noise
[rank0]:     noise_4d = combined_samples.view(num_samples, 1, image_size, image_size)
[rank0]:                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: RuntimeError: shape '[1250, 1, 8, 8]' is invalid for input of size 1250
[rank0]:[W401 04:58:31.357411764 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-04-01 04:58:32,578] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 83694
[2025-04-01 04:58:32,579] [ERROR] [launch.py:325:sigkill_handler] ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1'] exits with return code = 1
[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ bash scripts/script_normal_dif_test.sh 
[?2004l[2025-04-01 04:58:51,437] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:58:53,765] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 04:58:53,765] [INFO] [runner.py:605:main] cmd = /hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None main.py --image_size 8 --batch_size 256 --num_epochs 200 --timesteps 1000 --lr 2e-4 --time_emb_dim 256 --mu1 4 --sigma1 1 --num1 9000 --mu2 10 --sigma2 4 --num2 1000 --samples_dir ./samples --checkpoints_dir ./checkpoints --fp16 --mode train --model_path  --num_images 10000 --simulation_distribution normal --use_different_noise --use_moe --num_experts 4 --moe_hidden_dim 64 --moe_tau 0.1
[2025-04-01 04:58:55,525] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:58:57,706] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 04:58:57,706] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 04:58:57,706] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 04:58:57,706] [INFO] [launch.py:164:main] dist_world_size=1
[2025-04-01 04:58:57,706] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 04:58:57,823] [INFO] [launch.py:256:main] process 84512 spawned with command: ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1']
[2025-04-01 04:59:00,494] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:59:03,536] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.16.5, git-hash=unknown, git-branch=unknown
[2025-04-01 04:59:03,536] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-01 04:59:03,537] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-04-01 04:59:03,542] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 1
[2025-04-01 04:59:04,647] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124/fused_adam/build.ninja...
/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.18419289588928223 seconds
[2025-04-01 04:59:04,838] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-04-01 04:59:04,838] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-04-01 04:59:04,843] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-04-01 04:59:04,843] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-04-01 04:59:04,843] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2025-04-01 04:59:04,844] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000
[2025-04-01 04:59:04,844] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000
[2025-04-01 04:59:04,844] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-04-01 04:59:04,844] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-04-01 04:59:05,277] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-04-01 04:59:05,277] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:59:05,277] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.17 GB, percent = 6.7%
[2025-04-01 04:59:05,388] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-04-01 04:59:05,388] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:59:05,388] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.17 GB, percent = 6.7%
[2025-04-01 04:59:05,388] [INFO] [stage_1_and_2.py:556:__init__] optimizer state initialized
[2025-04-01 04:59:05,492] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-04-01 04:59:05,492] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:59:05,492] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.17 GB, percent = 6.7%
[2025-04-01 04:59:05,494] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-04-01 04:59:05,494] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-04-01 04:59:05,494] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-04-01 04:59:05,494] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:59:05,494] [INFO] [config.py:1000:print] DeepSpeedEngine configuration:
[2025-04-01 04:59:05,495] [INFO] [config.py:1004:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-04-01 04:59:05,495] [INFO] [config.py:1004:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-04-01 04:59:05,495] [INFO] [config.py:1004:print]   amp_enabled .................. False
[2025-04-01 04:59:05,495] [INFO] [config.py:1004:print]   amp_params ................... False
[2025-04-01 04:59:05,495] [INFO] [config.py:1004:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-01 04:59:05,495] [INFO] [config.py:1004:print]   bfloat16_enabled ............. False
[2025-04-01 04:59:05,495] [INFO] [config.py:1004:print]   bfloat16_immediate_grad_update  False
[2025-04-01 04:59:05,495] [INFO] [config.py:1004:print]   checkpoint_parallel_write_pipeline  False
[2025-04-01 04:59:05,495] [INFO] [config.py:1004:print]   checkpoint_tag_validation_enabled  True
[2025-04-01 04:59:05,495] [INFO] [config.py:1004:print]   checkpoint_tag_validation_fail  False
[2025-04-01 04:59:05,495] [INFO] [config.py:1004:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f82d74f19a0>
[2025-04-01 04:59:05,495] [INFO] [config.py:1004:print]   communication_data_type ...... None
[2025-04-01 04:59:05,495] [INFO] [config.py:1004:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-01 04:59:05,495] [INFO] [config.py:1004:print]   curriculum_enabled_legacy .... False
[2025-04-01 04:59:05,495] [INFO] [config.py:1004:print]   curriculum_params_legacy ..... False
[2025-04-01 04:59:05,495] [INFO] [config.py:1004:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-01 04:59:05,495] [INFO] [config.py:1004:print]   data_efficiency_enabled ...... False
[2025-04-01 04:59:05,495] [INFO] [config.py:1004:print]   dataloader_drop_last ......... False
[2025-04-01 04:59:05,495] [INFO] [config.py:1004:print]   disable_allgather ............ False
[2025-04-01 04:59:05,495] [INFO] [config.py:1004:print]   dump_state ................... False
[2025-04-01 04:59:05,495] [INFO] [config.py:1004:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-04-01 04:59:05,495] [INFO] [config.py:1004:print]   eigenvalue_enabled ........... False
[2025-04-01 04:59:05,495] [INFO] [config.py:1004:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-01 04:59:05,495] [INFO] [config.py:1004:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-01 04:59:05,495] [INFO] [config.py:1004:print]   eigenvalue_layer_num ......... 0
[2025-04-01 04:59:05,495] [INFO] [config.py:1004:print]   eigenvalue_max_iter .......... 100
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   eigenvalue_stability ......... 1e-06
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   eigenvalue_tol ............... 0.01
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   eigenvalue_verbose ........... False
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   elasticity_enabled ........... False
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   fp16_auto_cast ............... False
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   fp16_enabled ................. True
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   fp16_master_weights_and_gradients  False
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   global_rank .................. 0
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   grad_accum_dtype ............. None
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   gradient_accumulation_steps .. 1
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   gradient_clipping ............ 1.0
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   gradient_predivide_factor .... 1.0
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   graph_harvesting ............. False
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   initial_dynamic_scale ........ 65536
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   load_universal_checkpoint .... False
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   loss_scale ................... 0
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   memory_breakdown ............. False
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   mics_hierarchial_params_gather  False
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   mics_shard_size .............. -1
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   optimizer_legacy_fusion ...... False
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   optimizer_name ............... adamw
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   optimizer_params ............. {'lr': 0.0002, 'weight_decay': 0.01}
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   pld_enabled .................. False
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   pld_params ................... False
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   prescale_gradients ........... False
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   scheduler_name ............... None
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   scheduler_params ............. None
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   seq_parallel_communication_data_type  torch.float32
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   sparse_attention ............. None
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   sparse_gradients_enabled ..... False
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   steps_per_print .............. 50
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   timers_config ................ enabled=True synchronized=True
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   train_batch_size ............. 256
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   train_micro_batch_size_per_gpu  256
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   use_data_before_expert_parallel_  False
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   use_node_local_storage ....... False
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   wall_clock_breakdown ......... False
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   weight_quantization_config ... None
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   world_size ................... 1
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   zero_allow_untested_optimizer  False
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-04-01 04:59:05,496] [INFO] [config.py:1004:print]   zero_enabled ................. True
[2025-04-01 04:59:05,497] [INFO] [config.py:1004:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-01 04:59:05,497] [INFO] [config.py:1004:print]   zero_optimization_stage ...... 2
[2025-04-01 04:59:05,497] [INFO] [config.py:990:print_user_config]   json = {
    "train_batch_size": 256, 
    "gradient_accumulation_steps": 1, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0002, 
            "weight_decay": 0.01
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 1000
    }, 
    "zero_optimization": {
        "stage": 2, 
        "contiguous_gradients": true, 
        "overlap_comm": true
    }, 
    "steps_per_print": 50, 
    "gradient_clipping": 1.0
}
****START TRAINING****
image_size: 8, batch_size: 256, timesteps: 1000, time_emb_dim: 256
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s][2025-04-01 04:59:06,558] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
 20%|█████████████████████▏                                                                                    | 1/5 [00:01<00:04,  1.00s/it][2025-04-01 04:59:06,628] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:01<00:00,  2.96it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.64it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.44it/s]
/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py:190: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True) # 使用 pd.concat 追加数据
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.29it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.48it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.31it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.26it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.29it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.60it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.39it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.28it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.35it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.59it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.38it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.28it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.36it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.55it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.37it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.31it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.32it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.64it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.43it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.32it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.25it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.57it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.44it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.30it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.34it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.46it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.30it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.22it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.36it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.58it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.62it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.43it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.31it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.50it/s][2025-04-01 04:59:12,603] [INFO] [logging.py:107:log_dist] [Rank 0] step=50, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:59:12,605] [INFO] [timer.py:264:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=2613.207932469812, CurrSamplesPerSec=3425.5907965110714, MemAllocated=0.02GB, MaxMemAllocated=1.9GB
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.37it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.24it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.28it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.60it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.54it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.25it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.37it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.56it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.43it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.31it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.33it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.46it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.22it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.19it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.30it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.55it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.49it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.34it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.16it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.46it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.30it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.18it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.27it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.46it/s][2025-04-01 04:59:16,394] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.57it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.34it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.31it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.51it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.53it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.33it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.31it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.70it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.47it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.34it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.26it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.40it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.32it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.20it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.03it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.15it/s][2025-04-01 04:59:18,933] [INFO] [logging.py:107:log_dist] [Rank 0] step=100, skipped=3, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:59:18,934] [INFO] [timer.py:264:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=2629.705128208332, CurrSamplesPerSec=3229.879666771331, MemAllocated=0.02GB, MaxMemAllocated=1.9GB
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.04it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.99it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/main.py", line 9, in <module>
[rank0]:     train_deepspeed(cfg)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py", line 210, in train_deepspeed
[rank0]:     generate_during_training(model_engine, sample_dir, config, epoch, num_images=config.num_images)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/generate.py", line 227, in generate_during_training_simulation_dif
[rank0]:     x = model_engine.module.moe.sample_initial_noise(num_images, config)
[rank0]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/reparam_moe.py", line 126, in sample_initial_noise
[rank0]:     noise_4d = combined_samples.view(num_samples, 1, image_size, image_size)
[rank0]:                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: RuntimeError: shape '[10000, 1, 8, 8]' is invalid for input of size 10000
[rank0]:[W401 04:59:19.811301493 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-04-01 04:59:20,826] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 84512
[2025-04-01 04:59:20,827] [ERROR] [launch.py:325:sigkill_handler] ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1'] exits with return code = 1
[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ bash scripts/script_normal_dif_test.sh 
[?2004l[2025-04-01 04:59:28,491] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:59:30,814] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 04:59:30,815] [INFO] [runner.py:605:main] cmd = /hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None main.py --image_size 8 --batch_size 256 --num_epochs 200 --timesteps 1000 --lr 2e-4 --time_emb_dim 256 --mu1 4 --sigma1 1 --num1 9000 --mu2 10 --sigma2 4 --num2 1000 --samples_dir ./samples --checkpoints_dir ./checkpoints --fp16 --mode train --model_path  --num_images 80000 --simulation_distribution normal --use_different_noise --use_moe --num_experts 4 --moe_hidden_dim 64 --moe_tau 0.1
[2025-04-01 04:59:32,517] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:59:34,791] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 04:59:34,791] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 04:59:34,791] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 04:59:34,791] [INFO] [launch.py:164:main] dist_world_size=1
[2025-04-01 04:59:34,791] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 04:59:34,908] [INFO] [launch.py:256:main] process 85336 spawned with command: ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '80000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1']
[2025-04-01 04:59:37,683] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 04:59:40,936] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.16.5, git-hash=unknown, git-branch=unknown
[2025-04-01 04:59:40,936] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-01 04:59:40,936] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-04-01 04:59:40,939] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 1
[2025-04-01 04:59:42,122] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124/fused_adam/build.ninja...
/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.19172930717468262 seconds
[2025-04-01 04:59:42,319] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-04-01 04:59:42,320] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-04-01 04:59:42,325] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-04-01 04:59:42,325] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-04-01 04:59:42,325] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2025-04-01 04:59:42,325] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000
[2025-04-01 04:59:42,326] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000
[2025-04-01 04:59:42,326] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-04-01 04:59:42,326] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-04-01 04:59:42,767] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-04-01 04:59:42,768] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:59:42,768] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.07 GB, percent = 6.7%
[2025-04-01 04:59:42,880] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-04-01 04:59:42,881] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:59:42,881] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.07 GB, percent = 6.7%
[2025-04-01 04:59:42,881] [INFO] [stage_1_and_2.py:556:__init__] optimizer state initialized
[2025-04-01 04:59:42,986] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-04-01 04:59:42,987] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 04:59:42,987] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.07 GB, percent = 6.7%
[2025-04-01 04:59:42,989] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-04-01 04:59:42,989] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-04-01 04:59:42,989] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-04-01 04:59:42,989] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:59:42,989] [INFO] [config.py:1000:print] DeepSpeedEngine configuration:
[2025-04-01 04:59:42,989] [INFO] [config.py:1004:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-04-01 04:59:42,989] [INFO] [config.py:1004:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-04-01 04:59:42,989] [INFO] [config.py:1004:print]   amp_enabled .................. False
[2025-04-01 04:59:42,989] [INFO] [config.py:1004:print]   amp_params ................... False
[2025-04-01 04:59:42,990] [INFO] [config.py:1004:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-01 04:59:42,990] [INFO] [config.py:1004:print]   bfloat16_enabled ............. False
[2025-04-01 04:59:42,990] [INFO] [config.py:1004:print]   bfloat16_immediate_grad_update  False
[2025-04-01 04:59:42,990] [INFO] [config.py:1004:print]   checkpoint_parallel_write_pipeline  False
[2025-04-01 04:59:42,990] [INFO] [config.py:1004:print]   checkpoint_tag_validation_enabled  True
[2025-04-01 04:59:42,990] [INFO] [config.py:1004:print]   checkpoint_tag_validation_fail  False
[2025-04-01 04:59:42,990] [INFO] [config.py:1004:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fcd2e1acef0>
[2025-04-01 04:59:42,990] [INFO] [config.py:1004:print]   communication_data_type ...... None
[2025-04-01 04:59:42,990] [INFO] [config.py:1004:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-01 04:59:42,990] [INFO] [config.py:1004:print]   curriculum_enabled_legacy .... False
[2025-04-01 04:59:42,990] [INFO] [config.py:1004:print]   curriculum_params_legacy ..... False
[2025-04-01 04:59:42,990] [INFO] [config.py:1004:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-01 04:59:42,990] [INFO] [config.py:1004:print]   data_efficiency_enabled ...... False
[2025-04-01 04:59:42,990] [INFO] [config.py:1004:print]   dataloader_drop_last ......... False
[2025-04-01 04:59:42,990] [INFO] [config.py:1004:print]   disable_allgather ............ False
[2025-04-01 04:59:42,990] [INFO] [config.py:1004:print]   dump_state ................... False
[2025-04-01 04:59:42,990] [INFO] [config.py:1004:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-04-01 04:59:42,990] [INFO] [config.py:1004:print]   eigenvalue_enabled ........... False
[2025-04-01 04:59:42,990] [INFO] [config.py:1004:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-01 04:59:42,990] [INFO] [config.py:1004:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-01 04:59:42,990] [INFO] [config.py:1004:print]   eigenvalue_layer_num ......... 0
[2025-04-01 04:59:42,990] [INFO] [config.py:1004:print]   eigenvalue_max_iter .......... 100
[2025-04-01 04:59:42,990] [INFO] [config.py:1004:print]   eigenvalue_stability ......... 1e-06
[2025-04-01 04:59:42,990] [INFO] [config.py:1004:print]   eigenvalue_tol ............... 0.01
[2025-04-01 04:59:42,990] [INFO] [config.py:1004:print]   eigenvalue_verbose ........... False
[2025-04-01 04:59:42,990] [INFO] [config.py:1004:print]   elasticity_enabled ........... False
[2025-04-01 04:59:42,990] [INFO] [config.py:1004:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-04-01 04:59:42,990] [INFO] [config.py:1004:print]   fp16_auto_cast ............... False
[2025-04-01 04:59:42,990] [INFO] [config.py:1004:print]   fp16_enabled ................. True
[2025-04-01 04:59:42,990] [INFO] [config.py:1004:print]   fp16_master_weights_and_gradients  False
[2025-04-01 04:59:42,990] [INFO] [config.py:1004:print]   global_rank .................. 0
[2025-04-01 04:59:42,990] [INFO] [config.py:1004:print]   grad_accum_dtype ............. None
[2025-04-01 04:59:42,990] [INFO] [config.py:1004:print]   gradient_accumulation_steps .. 1
[2025-04-01 04:59:42,990] [INFO] [config.py:1004:print]   gradient_clipping ............ 1.0
[2025-04-01 04:59:42,990] [INFO] [config.py:1004:print]   gradient_predivide_factor .... 1.0
[2025-04-01 04:59:42,990] [INFO] [config.py:1004:print]   graph_harvesting ............. False
[2025-04-01 04:59:42,991] [INFO] [config.py:1004:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-01 04:59:42,991] [INFO] [config.py:1004:print]   initial_dynamic_scale ........ 65536
[2025-04-01 04:59:42,991] [INFO] [config.py:1004:print]   load_universal_checkpoint .... False
[2025-04-01 04:59:42,991] [INFO] [config.py:1004:print]   loss_scale ................... 0
[2025-04-01 04:59:42,991] [INFO] [config.py:1004:print]   memory_breakdown ............. False
[2025-04-01 04:59:42,991] [INFO] [config.py:1004:print]   mics_hierarchial_params_gather  False
[2025-04-01 04:59:42,991] [INFO] [config.py:1004:print]   mics_shard_size .............. -1
[2025-04-01 04:59:42,991] [INFO] [config.py:1004:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-04-01 04:59:42,991] [INFO] [config.py:1004:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-04-01 04:59:42,991] [INFO] [config.py:1004:print]   optimizer_legacy_fusion ...... False
[2025-04-01 04:59:42,991] [INFO] [config.py:1004:print]   optimizer_name ............... adamw
[2025-04-01 04:59:42,991] [INFO] [config.py:1004:print]   optimizer_params ............. {'lr': 0.0002, 'weight_decay': 0.01}
[2025-04-01 04:59:42,991] [INFO] [config.py:1004:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-04-01 04:59:42,991] [INFO] [config.py:1004:print]   pld_enabled .................. False
[2025-04-01 04:59:42,991] [INFO] [config.py:1004:print]   pld_params ................... False
[2025-04-01 04:59:42,991] [INFO] [config.py:1004:print]   prescale_gradients ........... False
[2025-04-01 04:59:42,991] [INFO] [config.py:1004:print]   scheduler_name ............... None
[2025-04-01 04:59:42,991] [INFO] [config.py:1004:print]   scheduler_params ............. None
[2025-04-01 04:59:42,991] [INFO] [config.py:1004:print]   seq_parallel_communication_data_type  torch.float32
[2025-04-01 04:59:42,991] [INFO] [config.py:1004:print]   sparse_attention ............. None
[2025-04-01 04:59:42,991] [INFO] [config.py:1004:print]   sparse_gradients_enabled ..... False
[2025-04-01 04:59:42,991] [INFO] [config.py:1004:print]   steps_per_print .............. 50
[2025-04-01 04:59:42,991] [INFO] [config.py:1004:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-04-01 04:59:42,991] [INFO] [config.py:1004:print]   timers_config ................ enabled=True synchronized=True
[2025-04-01 04:59:42,991] [INFO] [config.py:1004:print]   train_batch_size ............. 256
[2025-04-01 04:59:42,991] [INFO] [config.py:1004:print]   train_micro_batch_size_per_gpu  256
[2025-04-01 04:59:42,991] [INFO] [config.py:1004:print]   use_data_before_expert_parallel_  False
[2025-04-01 04:59:42,991] [INFO] [config.py:1004:print]   use_node_local_storage ....... False
[2025-04-01 04:59:42,991] [INFO] [config.py:1004:print]   wall_clock_breakdown ......... False
[2025-04-01 04:59:42,991] [INFO] [config.py:1004:print]   weight_quantization_config ... None
[2025-04-01 04:59:42,991] [INFO] [config.py:1004:print]   world_size ................... 1
[2025-04-01 04:59:42,991] [INFO] [config.py:1004:print]   zero_allow_untested_optimizer  False
[2025-04-01 04:59:42,991] [INFO] [config.py:1004:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-04-01 04:59:42,991] [INFO] [config.py:1004:print]   zero_enabled ................. True
[2025-04-01 04:59:42,991] [INFO] [config.py:1004:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-01 04:59:42,991] [INFO] [config.py:1004:print]   zero_optimization_stage ...... 2
[2025-04-01 04:59:42,991] [INFO] [config.py:990:print_user_config]   json = {
    "train_batch_size": 256, 
    "gradient_accumulation_steps": 1, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0002, 
            "weight_decay": 0.01
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 1000
    }, 
    "zero_optimization": {
        "stage": 2, 
        "contiguous_gradients": true, 
        "overlap_comm": true
    }, 
    "steps_per_print": 50, 
    "gradient_clipping": 1.0
}
****START TRAINING****
image_size: 8, batch_size: 256, timesteps: 1000, time_emb_dim: 256
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s][2025-04-01 04:59:44,019] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:03,  1.03it/s][2025-04-01 04:59:44,091] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:01<00:00,  3.01it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.69it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.49it/s]
/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py:190: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True) # 使用 pd.concat 追加数据
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.22it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.42it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.48it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.33it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.33it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.53it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.36it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.28it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.28it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.54it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.36it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.25it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.37it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.51it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.54it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.37it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.29it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.49it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.48it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.32it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.28it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.45it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.33it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.23it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.26it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.46it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.37it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.26it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.26it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.59it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.61it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.07it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.18it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.37it/s][2025-04-01 04:59:50,100] [INFO] [logging.py:107:log_dist] [Rank 0] step=50, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:59:50,101] [INFO] [timer.py:264:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=2612.6873220587463, CurrSamplesPerSec=3226.8512195566086, MemAllocated=0.02GB, MaxMemAllocated=1.9GB
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.29it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.18it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.26it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.59it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.54it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.37it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.33it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.47it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.38it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.32it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.25it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.45it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.50it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.32it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.30it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.47it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.31it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.23it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.25it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.45it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.36it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.24it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.28it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.48it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.32it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.25it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:01,  3.76it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  7.92it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.97it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.89it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.23it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.37it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.30it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.20it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.33it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.48it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.45it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.30it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.22it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.43it/s][2025-04-01 04:59:56,437] [INFO] [logging.py:107:log_dist] [Rank 0] step=100, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 04:59:56,438] [INFO] [timer.py:264:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=2626.5137686006465, CurrSamplesPerSec=3462.1957395102067, MemAllocated=0.02GB, MaxMemAllocated=1.9GB
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.47it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.29it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/main.py", line 9, in <module>
[rank0]:     train_deepspeed(cfg)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py", line 210, in train_deepspeed
[rank0]:     generate_during_training(model_engine, sample_dir, config, epoch, num_images=config.num_images)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/generate.py", line 227, in generate_during_training_simulation_dif
[rank0]:     x = model_engine.module.moe.sample_initial_noise(num_images, config)
[rank0]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/reparam_moe.py", line 126, in sample_initial_noise
[rank0]:     noise_4d = combined_samples.view(num_samples, 1, image_size, image_size)
[rank0]:                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: RuntimeError: shape '[80000, 1, 8, 8]' is invalid for input of size 80000
[rank0]:[W401 04:59:57.214615178 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-04-01 04:59:57,911] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 85336
[2025-04-01 04:59:57,912] [ERROR] [launch.py:325:sigkill_handler] ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '80000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1'] exits with return code = 1
[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ bash scripts/script_normal_dif_test.sh 
[?2004l[2025-04-01 05:01:27,906] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 05:01:30,295] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 05:01:30,295] [INFO] [runner.py:605:main] cmd = /hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None main.py --image_size 8 --batch_size 256 --num_epochs 200 --timesteps 1000 --lr 2e-4 --time_emb_dim 256 --mu1 4 --sigma1 1 --num1 9000 --mu2 10 --sigma2 4 --num2 1000 --samples_dir ./samples --checkpoints_dir ./checkpoints --fp16 --mode train --model_path  --num_images 80000 --simulation_distribution normal --use_different_noise --use_moe --num_experts 4 --moe_hidden_dim 64 --moe_tau 0.1
[2025-04-01 05:01:32,015] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 05:01:34,228] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 05:01:34,228] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 05:01:34,228] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 05:01:34,228] [INFO] [launch.py:164:main] dist_world_size=1
[2025-04-01 05:01:34,228] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 05:01:34,344] [INFO] [launch.py:256:main] process 86186 spawned with command: ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '80000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1']
[2025-04-01 05:01:37,029] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 05:01:39,933] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.16.5, git-hash=unknown, git-branch=unknown
[2025-04-01 05:01:39,933] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-01 05:01:39,934] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-04-01 05:01:39,939] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 1
[2025-04-01 05:01:41,071] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124/fused_adam/build.ninja...
/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.18711590766906738 seconds
[2025-04-01 05:01:41,264] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-04-01 05:01:41,264] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-04-01 05:01:41,269] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-04-01 05:01:41,269] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-04-01 05:01:41,270] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2025-04-01 05:01:41,270] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000
[2025-04-01 05:01:41,270] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000
[2025-04-01 05:01:41,270] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-04-01 05:01:41,270] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-04-01 05:01:41,762] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-04-01 05:01:41,762] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 05:01:41,762] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.18 GB, percent = 6.7%
[2025-04-01 05:01:41,874] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-04-01 05:01:41,875] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 05:01:41,875] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.19 GB, percent = 6.7%
[2025-04-01 05:01:41,875] [INFO] [stage_1_and_2.py:556:__init__] optimizer state initialized
[2025-04-01 05:01:41,980] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-04-01 05:01:41,980] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 05:01:41,980] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.19 GB, percent = 6.7%
[2025-04-01 05:01:41,982] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-04-01 05:01:41,982] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-04-01 05:01:41,982] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-04-01 05:01:41,982] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 05:01:41,983] [INFO] [config.py:1000:print] DeepSpeedEngine configuration:
[2025-04-01 05:01:41,983] [INFO] [config.py:1004:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-04-01 05:01:41,983] [INFO] [config.py:1004:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-04-01 05:01:41,983] [INFO] [config.py:1004:print]   amp_enabled .................. False
[2025-04-01 05:01:41,983] [INFO] [config.py:1004:print]   amp_params ................... False
[2025-04-01 05:01:41,983] [INFO] [config.py:1004:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-01 05:01:41,983] [INFO] [config.py:1004:print]   bfloat16_enabled ............. False
[2025-04-01 05:01:41,983] [INFO] [config.py:1004:print]   bfloat16_immediate_grad_update  False
[2025-04-01 05:01:41,983] [INFO] [config.py:1004:print]   checkpoint_parallel_write_pipeline  False
[2025-04-01 05:01:41,983] [INFO] [config.py:1004:print]   checkpoint_tag_validation_enabled  True
[2025-04-01 05:01:41,983] [INFO] [config.py:1004:print]   checkpoint_tag_validation_fail  False
[2025-04-01 05:01:41,983] [INFO] [config.py:1004:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ff027e4c800>
[2025-04-01 05:01:41,983] [INFO] [config.py:1004:print]   communication_data_type ...... None
[2025-04-01 05:01:41,983] [INFO] [config.py:1004:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-01 05:01:41,983] [INFO] [config.py:1004:print]   curriculum_enabled_legacy .... False
[2025-04-01 05:01:41,983] [INFO] [config.py:1004:print]   curriculum_params_legacy ..... False
[2025-04-01 05:01:41,983] [INFO] [config.py:1004:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-01 05:01:41,983] [INFO] [config.py:1004:print]   data_efficiency_enabled ...... False
[2025-04-01 05:01:41,983] [INFO] [config.py:1004:print]   dataloader_drop_last ......... False
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   disable_allgather ............ False
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   dump_state ................... False
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   eigenvalue_enabled ........... False
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   eigenvalue_layer_num ......... 0
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   eigenvalue_max_iter .......... 100
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   eigenvalue_stability ......... 1e-06
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   eigenvalue_tol ............... 0.01
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   eigenvalue_verbose ........... False
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   elasticity_enabled ........... False
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   fp16_auto_cast ............... False
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   fp16_enabled ................. True
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   fp16_master_weights_and_gradients  False
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   global_rank .................. 0
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   grad_accum_dtype ............. None
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   gradient_accumulation_steps .. 1
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   gradient_clipping ............ 1.0
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   gradient_predivide_factor .... 1.0
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   graph_harvesting ............. False
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   initial_dynamic_scale ........ 65536
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   load_universal_checkpoint .... False
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   loss_scale ................... 0
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   memory_breakdown ............. False
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   mics_hierarchial_params_gather  False
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   mics_shard_size .............. -1
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   optimizer_legacy_fusion ...... False
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   optimizer_name ............... adamw
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   optimizer_params ............. {'lr': 0.0002, 'weight_decay': 0.01}
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   pld_enabled .................. False
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   pld_params ................... False
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   prescale_gradients ........... False
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   scheduler_name ............... None
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   scheduler_params ............. None
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   seq_parallel_communication_data_type  torch.float32
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   sparse_attention ............. None
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   sparse_gradients_enabled ..... False
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   steps_per_print .............. 50
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   timers_config ................ enabled=True synchronized=True
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   train_batch_size ............. 256
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   train_micro_batch_size_per_gpu  256
[2025-04-01 05:01:41,984] [INFO] [config.py:1004:print]   use_data_before_expert_parallel_  False
[2025-04-01 05:01:41,985] [INFO] [config.py:1004:print]   use_node_local_storage ....... False
[2025-04-01 05:01:41,985] [INFO] [config.py:1004:print]   wall_clock_breakdown ......... False
[2025-04-01 05:01:41,985] [INFO] [config.py:1004:print]   weight_quantization_config ... None
[2025-04-01 05:01:41,985] [INFO] [config.py:1004:print]   world_size ................... 1
[2025-04-01 05:01:41,985] [INFO] [config.py:1004:print]   zero_allow_untested_optimizer  False
[2025-04-01 05:01:41,985] [INFO] [config.py:1004:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-04-01 05:01:41,985] [INFO] [config.py:1004:print]   zero_enabled ................. True
[2025-04-01 05:01:41,985] [INFO] [config.py:1004:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-01 05:01:41,985] [INFO] [config.py:1004:print]   zero_optimization_stage ...... 2
[2025-04-01 05:01:41,985] [INFO] [config.py:990:print_user_config]   json = {
    "train_batch_size": 256, 
    "gradient_accumulation_steps": 1, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0002, 
            "weight_decay": 0.01
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 1000
    }, 
    "zero_optimization": {
        "stage": 2, 
        "contiguous_gradients": true, 
        "overlap_comm": true
    }, 
    "steps_per_print": 50, 
    "gradient_clipping": 1.0
}
****START TRAINING****
image_size: 8, batch_size: 256, timesteps: 1000, time_emb_dim: 256
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s][2025-04-01 05:01:43,042] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:03,  1.01it/s][2025-04-01 05:01:43,109] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:01<00:00,  2.98it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.62it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.45it/s]
/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py:190: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True) # 使用 pd.concat 追加数据
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.24it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.41it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.17it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.21it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.16it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.58it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.57it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.32it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.28it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.56it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.56it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.36it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.29it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.46it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.35it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.25it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.30it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.42it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.25it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.23it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.22it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.40it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.23it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.17it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.31it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.45it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.40it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.28it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.37it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.61it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.53it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.35it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.27it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.45it/s][2025-04-01 05:01:49,122] [INFO] [logging.py:107:log_dist] [Rank 0] step=50, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 05:01:49,123] [INFO] [timer.py:264:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=2608.278678429685, CurrSamplesPerSec=3436.686530760191, MemAllocated=0.02GB, MaxMemAllocated=1.9GB
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.46it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.28it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.35it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.62it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.46it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.32it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.37it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.63it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.61it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.42it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.26it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.43it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.32it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.21it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.19it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.42it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.32it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.20it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.23it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.40it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.37it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.23it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.30it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.48it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.49it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.31it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.14it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.31it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.24it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.14it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.26it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.43it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.32it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.23it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.28it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.60it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.58it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.37it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.26it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.64it/s][2025-04-01 05:01:55,451] [INFO] [logging.py:107:log_dist] [Rank 0] step=100, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 05:01:55,453] [INFO] [timer.py:264:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=2630.843735330267, CurrSamplesPerSec=3410.791886042856, MemAllocated=0.02GB, MaxMemAllocated=1.9GB
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.39it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.28it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/main.py", line 9, in <module>
[rank0]:     train_deepspeed(cfg)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py", line 210, in train_deepspeed
[rank0]:     generate_during_training(model_engine, sample_dir, config, epoch, num_images=config.num_images)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/generate.py", line 240, in generate_during_training_simulation_dif
[rank0]:     pred_noise = model_engine(x, t_batch)
[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/deepspeed/runtime/engine.py", line 2030, in forward
[rank0]:     loss = self.module(*inputs, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank0]:     return inner()
[rank0]:            ^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/unet.py", line 265, in forward
[rank0]:     x2 = self.down1(x1, t_embed)
[rank0]:          ^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/unet.py", line 51, in forward
[rank0]:     h = h * (scale[:, :, None, None] + 1) + shift[:, :, None, None]
[rank0]:         ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[rank0]: RuntimeError: The size of tensor a (1250) must match the size of tensor b (80000) at non-singleton dimension 0
[rank0]:[W401 05:01:56.260905348 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-04-01 05:01:57,347] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 86186
[2025-04-01 05:01:57,348] [ERROR] [launch.py:325:sigkill_handler] ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '80000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1'] exits with return code = 1
[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ bash scripts/script_normal_dif_test.sh 
[?2004l[2025-04-01 05:03:45,917] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 05:03:48,115] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 05:03:48,116] [INFO] [runner.py:605:main] cmd = /hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None main.py --image_size 8 --batch_size 256 --num_epochs 200 --timesteps 1000 --lr 2e-4 --time_emb_dim 256 --mu1 4 --sigma1 1 --num1 9000 --mu2 10 --sigma2 4 --num2 1000 --samples_dir ./samples --checkpoints_dir ./checkpoints --fp16 --mode train --model_path  --num_images 80000 --simulation_distribution normal --use_different_noise --use_moe --num_experts 4 --moe_hidden_dim 64 --moe_tau 0.1
[2025-04-01 05:03:49,848] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 05:03:52,103] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 05:03:52,103] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 05:03:52,103] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 05:03:52,103] [INFO] [launch.py:164:main] dist_world_size=1
[2025-04-01 05:03:52,103] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 05:03:52,218] [INFO] [launch.py:256:main] process 87086 spawned with command: ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '80000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1']
[2025-04-01 05:03:54,909] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 05:03:57,938] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.16.5, git-hash=unknown, git-branch=unknown
[2025-04-01 05:03:57,938] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-01 05:03:57,938] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-04-01 05:03:57,941] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 1
[2025-04-01 05:03:59,124] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124/fused_adam/build.ninja...
/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.19464898109436035 seconds
[2025-04-01 05:03:59,324] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-04-01 05:03:59,325] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-04-01 05:03:59,329] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-04-01 05:03:59,330] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-04-01 05:03:59,330] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2025-04-01 05:03:59,330] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000
[2025-04-01 05:03:59,330] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000
[2025-04-01 05:03:59,330] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-04-01 05:03:59,330] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-04-01 05:03:59,776] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-04-01 05:03:59,777] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 05:03:59,777] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.12 GB, percent = 6.7%
[2025-04-01 05:03:59,891] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-04-01 05:03:59,891] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 05:03:59,891] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.13 GB, percent = 6.7%
[2025-04-01 05:03:59,892] [INFO] [stage_1_and_2.py:556:__init__] optimizer state initialized
[2025-04-01 05:03:59,999] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-04-01 05:03:59,999] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 05:03:59,999] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.13 GB, percent = 6.7%
[2025-04-01 05:04:00,001] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-04-01 05:04:00,001] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-04-01 05:04:00,001] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-04-01 05:04:00,001] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 05:04:00,002] [INFO] [config.py:1000:print] DeepSpeedEngine configuration:
[2025-04-01 05:04:00,002] [INFO] [config.py:1004:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-04-01 05:04:00,002] [INFO] [config.py:1004:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-04-01 05:04:00,002] [INFO] [config.py:1004:print]   amp_enabled .................. False
[2025-04-01 05:04:00,002] [INFO] [config.py:1004:print]   amp_params ................... False
[2025-04-01 05:04:00,002] [INFO] [config.py:1004:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-01 05:04:00,002] [INFO] [config.py:1004:print]   bfloat16_enabled ............. False
[2025-04-01 05:04:00,002] [INFO] [config.py:1004:print]   bfloat16_immediate_grad_update  False
[2025-04-01 05:04:00,002] [INFO] [config.py:1004:print]   checkpoint_parallel_write_pipeline  False
[2025-04-01 05:04:00,002] [INFO] [config.py:1004:print]   checkpoint_tag_validation_enabled  True
[2025-04-01 05:04:00,002] [INFO] [config.py:1004:print]   checkpoint_tag_validation_fail  False
[2025-04-01 05:04:00,002] [INFO] [config.py:1004:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fcaa81b5b20>
[2025-04-01 05:04:00,002] [INFO] [config.py:1004:print]   communication_data_type ...... None
[2025-04-01 05:04:00,002] [INFO] [config.py:1004:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-01 05:04:00,002] [INFO] [config.py:1004:print]   curriculum_enabled_legacy .... False
[2025-04-01 05:04:00,002] [INFO] [config.py:1004:print]   curriculum_params_legacy ..... False
[2025-04-01 05:04:00,002] [INFO] [config.py:1004:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-01 05:04:00,002] [INFO] [config.py:1004:print]   data_efficiency_enabled ...... False
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   dataloader_drop_last ......... False
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   disable_allgather ............ False
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   dump_state ................... False
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   eigenvalue_enabled ........... False
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   eigenvalue_layer_num ......... 0
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   eigenvalue_max_iter .......... 100
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   eigenvalue_stability ......... 1e-06
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   eigenvalue_tol ............... 0.01
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   eigenvalue_verbose ........... False
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   elasticity_enabled ........... False
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   fp16_auto_cast ............... False
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   fp16_enabled ................. True
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   fp16_master_weights_and_gradients  False
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   global_rank .................. 0
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   grad_accum_dtype ............. None
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   gradient_accumulation_steps .. 1
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   gradient_clipping ............ 1.0
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   gradient_predivide_factor .... 1.0
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   graph_harvesting ............. False
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   initial_dynamic_scale ........ 65536
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   load_universal_checkpoint .... False
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   loss_scale ................... 0
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   memory_breakdown ............. False
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   mics_hierarchial_params_gather  False
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   mics_shard_size .............. -1
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   optimizer_legacy_fusion ...... False
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   optimizer_name ............... adamw
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   optimizer_params ............. {'lr': 0.0002, 'weight_decay': 0.01}
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   pld_enabled .................. False
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   pld_params ................... False
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   prescale_gradients ........... False
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   scheduler_name ............... None
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   scheduler_params ............. None
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   seq_parallel_communication_data_type  torch.float32
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   sparse_attention ............. None
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   sparse_gradients_enabled ..... False
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   steps_per_print .............. 50
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   timers_config ................ enabled=True synchronized=True
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   train_batch_size ............. 256
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   train_micro_batch_size_per_gpu  256
[2025-04-01 05:04:00,003] [INFO] [config.py:1004:print]   use_data_before_expert_parallel_  False
[2025-04-01 05:04:00,004] [INFO] [config.py:1004:print]   use_node_local_storage ....... False
[2025-04-01 05:04:00,004] [INFO] [config.py:1004:print]   wall_clock_breakdown ......... False
[2025-04-01 05:04:00,004] [INFO] [config.py:1004:print]   weight_quantization_config ... None
[2025-04-01 05:04:00,004] [INFO] [config.py:1004:print]   world_size ................... 1
[2025-04-01 05:04:00,004] [INFO] [config.py:1004:print]   zero_allow_untested_optimizer  False
[2025-04-01 05:04:00,004] [INFO] [config.py:1004:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-04-01 05:04:00,004] [INFO] [config.py:1004:print]   zero_enabled ................. True
[2025-04-01 05:04:00,004] [INFO] [config.py:1004:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-01 05:04:00,004] [INFO] [config.py:1004:print]   zero_optimization_stage ...... 2
[2025-04-01 05:04:00,004] [INFO] [config.py:990:print_user_config]   json = {
    "train_batch_size": 256, 
    "gradient_accumulation_steps": 1, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0002, 
            "weight_decay": 0.01
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 1000
    }, 
    "zero_optimization": {
        "stage": 2, 
        "contiguous_gradients": true, 
        "overlap_comm": true
    }, 
    "steps_per_print": 50, 
    "gradient_clipping": 1.0
}
****START TRAINING****
image_size: 8, batch_size: 256, timesteps: 1000, time_emb_dim: 256
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s][2025-04-01 05:04:01,107] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
 20%|█████████████████████▏                                                                                    | 1/5 [00:01<00:04,  1.03s/it][2025-04-01 05:04:01,179] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:01<00:00,  2.88it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.51it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.34it/s]
/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py:190: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True) # 使用 pd.concat 追加数据
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.36it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.61it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.53it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.36it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.34it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.55it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.63it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.41it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.29it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.60it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.45it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.30it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.34it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.59it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.38it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.31it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.25it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.47it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.50it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.29it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.38it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.57it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.56it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.41it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.29it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.48it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.37it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.26it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.23it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.44it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.47it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.28it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.33it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.52it/s][2025-04-01 05:04:07,146] [INFO] [logging.py:107:log_dist] [Rank 0] step=50, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 05:04:07,147] [INFO] [timer.py:264:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=2625.497871695313, CurrSamplesPerSec=3453.243432016737, MemAllocated=0.02GB, MaxMemAllocated=1.9GB
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.53it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.38it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.26it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.46it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.47it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.29it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.32it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.57it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.49it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.31it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.34it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.59it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.59it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.40it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.29it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.54it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.55it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.33it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.32it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.50it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.45it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.32it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.25it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.61it/s][2025-04-01 05:04:10,896] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.60it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.38it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.36it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.74it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.56it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.41it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.25it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.44it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.40it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.27it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.30it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.48it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.50it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.38it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.30it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.44it/s][2025-04-01 05:04:13,403] [INFO] [logging.py:107:log_dist] [Rank 0] step=100, skipped=3, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 05:04:13,404] [INFO] [timer.py:264:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=2641.8693249715448, CurrSamplesPerSec=3403.0196014898274, MemAllocated=0.02GB, MaxMemAllocated=1.9GB
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.30it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.12it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/main.py", line 9, in <module>
[rank0]:     train_deepspeed(cfg)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py", line 210, in train_deepspeed
[rank0]:     generate_during_training(model_engine, sample_dir, config, epoch, num_images=config.num_images)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/generate.py", line 227, in generate_during_training_simulation_dif
[rank0]:     x = model_engine.module.moe.sample_initial_noise(num_images, config)
[rank0]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/reparam_moe.py", line 126, in sample_initial_noise
[rank0]:     noise_4d = combined_samples.view(num_samples//(image_size), 1, image_size, image_size)
[rank0]:                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: RuntimeError: shape '[10000, 1, 8, 8]' is invalid for input of size 80000
[rank0]:[W401 05:04:14.046168715 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-04-01 05:04:15,221] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 87086
[2025-04-01 05:04:15,222] [ERROR] [launch.py:325:sigkill_handler] ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '80000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1'] exits with return code = 1
[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ bash scripts/script_normal_dif_test.sh 
[?2004l[2025-04-01 05:06:34,243] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 05:06:36,556] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 05:06:36,556] [INFO] [runner.py:605:main] cmd = /hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None main.py --image_size 8 --batch_size 256 --num_epochs 200 --timesteps 1000 --lr 2e-4 --time_emb_dim 256 --mu1 4 --sigma1 1 --num1 9000 --mu2 10 --sigma2 4 --num2 1000 --samples_dir ./samples --checkpoints_dir ./checkpoints --fp16 --mode train --model_path  --num_images 80000 --simulation_distribution normal --use_different_noise --use_moe --num_experts 4 --moe_hidden_dim 64 --moe_tau 0.1
[2025-04-01 05:06:38,278] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 05:06:40,548] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 05:06:40,548] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 05:06:40,548] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 05:06:40,548] [INFO] [launch.py:164:main] dist_world_size=1
[2025-04-01 05:06:40,548] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 05:06:40,664] [INFO] [launch.py:256:main] process 87952 spawned with command: ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '80000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1']
[2025-04-01 05:06:43,533] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 05:06:46,338] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.16.5, git-hash=unknown, git-branch=unknown
[2025-04-01 05:06:46,338] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-01 05:06:46,338] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-04-01 05:06:46,341] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 1
[2025-04-01 05:06:47,492] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124/fused_adam/build.ninja...
/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.19675540924072266 seconds
[2025-04-01 05:06:47,694] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-04-01 05:06:47,694] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-04-01 05:06:47,699] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-04-01 05:06:47,700] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-04-01 05:06:47,700] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2025-04-01 05:06:47,700] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000
[2025-04-01 05:06:47,700] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000
[2025-04-01 05:06:47,700] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-04-01 05:06:47,700] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-04-01 05:06:47,934] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-04-01 05:06:47,935] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 05:06:47,935] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.18 GB, percent = 6.7%
[2025-04-01 05:06:48,050] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-04-01 05:06:48,051] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 05:06:48,051] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.18 GB, percent = 6.7%
[2025-04-01 05:06:48,051] [INFO] [stage_1_and_2.py:556:__init__] optimizer state initialized
[2025-04-01 05:06:48,159] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-04-01 05:06:48,160] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 05:06:48,160] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.18 GB, percent = 6.7%
[2025-04-01 05:06:48,162] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-04-01 05:06:48,162] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-04-01 05:06:48,162] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-04-01 05:06:48,162] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 05:06:48,162] [INFO] [config.py:1000:print] DeepSpeedEngine configuration:
[2025-04-01 05:06:48,162] [INFO] [config.py:1004:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-04-01 05:06:48,162] [INFO] [config.py:1004:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-04-01 05:06:48,163] [INFO] [config.py:1004:print]   amp_enabled .................. False
[2025-04-01 05:06:48,163] [INFO] [config.py:1004:print]   amp_params ................... False
[2025-04-01 05:06:48,163] [INFO] [config.py:1004:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-01 05:06:48,163] [INFO] [config.py:1004:print]   bfloat16_enabled ............. False
[2025-04-01 05:06:48,163] [INFO] [config.py:1004:print]   bfloat16_immediate_grad_update  False
[2025-04-01 05:06:48,163] [INFO] [config.py:1004:print]   checkpoint_parallel_write_pipeline  False
[2025-04-01 05:06:48,163] [INFO] [config.py:1004:print]   checkpoint_tag_validation_enabled  True
[2025-04-01 05:06:48,163] [INFO] [config.py:1004:print]   checkpoint_tag_validation_fail  False
[2025-04-01 05:06:48,163] [INFO] [config.py:1004:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fa098c05b20>
[2025-04-01 05:06:48,163] [INFO] [config.py:1004:print]   communication_data_type ...... None
[2025-04-01 05:06:48,163] [INFO] [config.py:1004:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-01 05:06:48,163] [INFO] [config.py:1004:print]   curriculum_enabled_legacy .... False
[2025-04-01 05:06:48,163] [INFO] [config.py:1004:print]   curriculum_params_legacy ..... False
[2025-04-01 05:06:48,163] [INFO] [config.py:1004:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-01 05:06:48,163] [INFO] [config.py:1004:print]   data_efficiency_enabled ...... False
[2025-04-01 05:06:48,163] [INFO] [config.py:1004:print]   dataloader_drop_last ......... False
[2025-04-01 05:06:48,163] [INFO] [config.py:1004:print]   disable_allgather ............ False
[2025-04-01 05:06:48,163] [INFO] [config.py:1004:print]   dump_state ................... False
[2025-04-01 05:06:48,163] [INFO] [config.py:1004:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-04-01 05:06:48,163] [INFO] [config.py:1004:print]   eigenvalue_enabled ........... False
[2025-04-01 05:06:48,163] [INFO] [config.py:1004:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-01 05:06:48,163] [INFO] [config.py:1004:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-01 05:06:48,163] [INFO] [config.py:1004:print]   eigenvalue_layer_num ......... 0
[2025-04-01 05:06:48,163] [INFO] [config.py:1004:print]   eigenvalue_max_iter .......... 100
[2025-04-01 05:06:48,163] [INFO] [config.py:1004:print]   eigenvalue_stability ......... 1e-06
[2025-04-01 05:06:48,163] [INFO] [config.py:1004:print]   eigenvalue_tol ............... 0.01
[2025-04-01 05:06:48,163] [INFO] [config.py:1004:print]   eigenvalue_verbose ........... False
[2025-04-01 05:06:48,163] [INFO] [config.py:1004:print]   elasticity_enabled ........... False
[2025-04-01 05:06:48,163] [INFO] [config.py:1004:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-04-01 05:06:48,163] [INFO] [config.py:1004:print]   fp16_auto_cast ............... False
[2025-04-01 05:06:48,163] [INFO] [config.py:1004:print]   fp16_enabled ................. True
[2025-04-01 05:06:48,163] [INFO] [config.py:1004:print]   fp16_master_weights_and_gradients  False
[2025-04-01 05:06:48,163] [INFO] [config.py:1004:print]   global_rank .................. 0
[2025-04-01 05:06:48,163] [INFO] [config.py:1004:print]   grad_accum_dtype ............. None
[2025-04-01 05:06:48,163] [INFO] [config.py:1004:print]   gradient_accumulation_steps .. 1
[2025-04-01 05:06:48,163] [INFO] [config.py:1004:print]   gradient_clipping ............ 1.0
[2025-04-01 05:06:48,163] [INFO] [config.py:1004:print]   gradient_predivide_factor .... 1.0
[2025-04-01 05:06:48,163] [INFO] [config.py:1004:print]   graph_harvesting ............. False
[2025-04-01 05:06:48,164] [INFO] [config.py:1004:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-01 05:06:48,164] [INFO] [config.py:1004:print]   initial_dynamic_scale ........ 65536
[2025-04-01 05:06:48,164] [INFO] [config.py:1004:print]   load_universal_checkpoint .... False
[2025-04-01 05:06:48,164] [INFO] [config.py:1004:print]   loss_scale ................... 0
[2025-04-01 05:06:48,164] [INFO] [config.py:1004:print]   memory_breakdown ............. False
[2025-04-01 05:06:48,164] [INFO] [config.py:1004:print]   mics_hierarchial_params_gather  False
[2025-04-01 05:06:48,164] [INFO] [config.py:1004:print]   mics_shard_size .............. -1
[2025-04-01 05:06:48,164] [INFO] [config.py:1004:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-04-01 05:06:48,164] [INFO] [config.py:1004:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-04-01 05:06:48,164] [INFO] [config.py:1004:print]   optimizer_legacy_fusion ...... False
[2025-04-01 05:06:48,164] [INFO] [config.py:1004:print]   optimizer_name ............... adamw
[2025-04-01 05:06:48,164] [INFO] [config.py:1004:print]   optimizer_params ............. {'lr': 0.0002, 'weight_decay': 0.01}
[2025-04-01 05:06:48,164] [INFO] [config.py:1004:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-04-01 05:06:48,164] [INFO] [config.py:1004:print]   pld_enabled .................. False
[2025-04-01 05:06:48,164] [INFO] [config.py:1004:print]   pld_params ................... False
[2025-04-01 05:06:48,164] [INFO] [config.py:1004:print]   prescale_gradients ........... False
[2025-04-01 05:06:48,164] [INFO] [config.py:1004:print]   scheduler_name ............... None
[2025-04-01 05:06:48,164] [INFO] [config.py:1004:print]   scheduler_params ............. None
[2025-04-01 05:06:48,164] [INFO] [config.py:1004:print]   seq_parallel_communication_data_type  torch.float32
[2025-04-01 05:06:48,164] [INFO] [config.py:1004:print]   sparse_attention ............. None
[2025-04-01 05:06:48,164] [INFO] [config.py:1004:print]   sparse_gradients_enabled ..... False
[2025-04-01 05:06:48,164] [INFO] [config.py:1004:print]   steps_per_print .............. 50
[2025-04-01 05:06:48,164] [INFO] [config.py:1004:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-04-01 05:06:48,164] [INFO] [config.py:1004:print]   timers_config ................ enabled=True synchronized=True
[2025-04-01 05:06:48,164] [INFO] [config.py:1004:print]   train_batch_size ............. 256
[2025-04-01 05:06:48,164] [INFO] [config.py:1004:print]   train_micro_batch_size_per_gpu  256
[2025-04-01 05:06:48,164] [INFO] [config.py:1004:print]   use_data_before_expert_parallel_  False
[2025-04-01 05:06:48,164] [INFO] [config.py:1004:print]   use_node_local_storage ....... False
[2025-04-01 05:06:48,164] [INFO] [config.py:1004:print]   wall_clock_breakdown ......... False
[2025-04-01 05:06:48,164] [INFO] [config.py:1004:print]   weight_quantization_config ... None
[2025-04-01 05:06:48,164] [INFO] [config.py:1004:print]   world_size ................... 1
[2025-04-01 05:06:48,164] [INFO] [config.py:1004:print]   zero_allow_untested_optimizer  False
[2025-04-01 05:06:48,164] [INFO] [config.py:1004:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-04-01 05:06:48,164] [INFO] [config.py:1004:print]   zero_enabled ................. True
[2025-04-01 05:06:48,164] [INFO] [config.py:1004:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-01 05:06:48,164] [INFO] [config.py:1004:print]   zero_optimization_stage ...... 2
[2025-04-01 05:06:48,164] [INFO] [config.py:990:print_user_config]   json = {
    "train_batch_size": 256, 
    "gradient_accumulation_steps": 1, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0002, 
            "weight_decay": 0.01
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 1000
    }, 
    "zero_optimization": {
        "stage": 2, 
        "contiguous_gradients": true, 
        "overlap_comm": true
    }, 
    "steps_per_print": 50, 
    "gradient_clipping": 1.0
}
****START TRAINING****
image_size: 8, batch_size: 256, timesteps: 1000, time_emb_dim: 256
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s][2025-04-01 05:06:49,321] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
 20%|█████████████████████▏                                                                                    | 1/5 [00:01<00:04,  1.00s/it][2025-04-01 05:06:49,392] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:01<00:00,  2.96it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.63it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.43it/s]
/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py:190: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True) # 使用 pd.concat 追加数据
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.36it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.55it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.36it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.29it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.80it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  9.15it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.99it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.87it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.34it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.66it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.51it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.37it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.38it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.65it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.49it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.38it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.35it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.51it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.33it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.27it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.37it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.55it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.37it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.27it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.34it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.60it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.60it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.40it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.28it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.53it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.16it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.40it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.35it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.61it/s][2025-04-01 05:06:55,298] [INFO] [logging.py:107:log_dist] [Rank 0] step=50, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 05:06:55,300] [INFO] [timer.py:264:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=2642.3884765145554, CurrSamplesPerSec=3457.903091336405, MemAllocated=0.02GB, MaxMemAllocated=1.9GB
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.60it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.40it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.87it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  9.09it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 11.03it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.90it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.22it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.49it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.38it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.28it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.32it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.72it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.77it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.51it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.33it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.73it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.69it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.48it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.87it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  9.18it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.87it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.74it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.28it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.67it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.44it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.98it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.96it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  9.37it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 11.23it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.25it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.36it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.55it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.49it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.35it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.29it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.55it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.42it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.30it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.36it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.73it/s][2025-04-01 05:07:01,434] [INFO] [logging.py:107:log_dist] [Rank 0] step=100, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 05:07:01,435] [INFO] [timer.py:264:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=2684.0368576780706, CurrSamplesPerSec=3344.227556360108, MemAllocated=0.02GB, MaxMemAllocated=1.9GB
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.48it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.34it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/main.py", line 9, in <module>
[rank0]:     train_deepspeed(cfg)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py", line 210, in train_deepspeed
[rank0]:     generate_during_training(model_engine, sample_dir, config, epoch, num_images=config.num_images)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/generate.py", line 227, in generate_during_training_simulation_dif
[rank0]:     x = model_engine.module.moe.sample_initial_noise(num_images, config)
[rank0]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/reparam_moe.py", line 110, in sample_initial_noise
[rank0]:     raise ValueError(
[rank0]: ValueError: input_dim (1) 必须等于 image_size^2 (64)
[rank0]:[W401 05:07:02.078437496 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-04-01 05:07:03,667] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 87952
[2025-04-01 05:07:03,668] [ERROR] [launch.py:325:sigkill_handler] ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '80000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1'] exits with return code = 1
[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ bash scripts/script_normal_dif_test.sh 
[?2004l[2025-04-01 05:10:32,123] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 05:10:34,411] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 05:10:34,412] [INFO] [runner.py:605:main] cmd = /hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None main.py --image_size 8 --batch_size 256 --num_epochs 200 --timesteps 1000 --lr 2e-4 --time_emb_dim 256 --mu1 4 --sigma1 1 --num1 9000 --mu2 10 --sigma2 4 --num2 1000 --samples_dir ./samples --checkpoints_dir ./checkpoints --fp16 --mode train --model_path  --num_images 80000 --simulation_distribution normal --use_different_noise --use_moe --num_experts 4 --moe_hidden_dim 64 --moe_tau 0.1
[2025-04-01 05:10:36,141] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 05:10:38,462] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 05:10:38,463] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 05:10:38,463] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 05:10:38,463] [INFO] [launch.py:164:main] dist_world_size=1
[2025-04-01 05:10:38,463] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 05:10:38,580] [INFO] [launch.py:256:main] process 88817 spawned with command: ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '80000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1']
[2025-04-01 05:10:41,337] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 05:10:44,246] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.16.5, git-hash=unknown, git-branch=unknown
[2025-04-01 05:10:44,246] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-01 05:10:44,246] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-04-01 05:10:44,251] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 1
[2025-04-01 05:10:45,589] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124/fused_adam/build.ninja...
/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.19633126258850098 seconds
[2025-04-01 05:10:45,791] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-04-01 05:10:45,791] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-04-01 05:10:45,796] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-04-01 05:10:45,796] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-04-01 05:10:45,796] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2025-04-01 05:10:45,797] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000
[2025-04-01 05:10:45,797] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000
[2025-04-01 05:10:45,797] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-04-01 05:10:45,797] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-04-01 05:10:46,161] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-04-01 05:10:46,162] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 05:10:46,162] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.19 GB, percent = 6.7%
[2025-04-01 05:10:46,274] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-04-01 05:10:46,274] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 05:10:46,275] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.2 GB, percent = 6.7%
[2025-04-01 05:10:46,275] [INFO] [stage_1_and_2.py:556:__init__] optimizer state initialized
[2025-04-01 05:10:46,380] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-04-01 05:10:46,380] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 05:10:46,380] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.2 GB, percent = 6.7%
[2025-04-01 05:10:46,382] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-04-01 05:10:46,382] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-04-01 05:10:46,382] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-04-01 05:10:46,382] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 05:10:46,382] [INFO] [config.py:1000:print] DeepSpeedEngine configuration:
[2025-04-01 05:10:46,383] [INFO] [config.py:1004:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-04-01 05:10:46,383] [INFO] [config.py:1004:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-04-01 05:10:46,383] [INFO] [config.py:1004:print]   amp_enabled .................. False
[2025-04-01 05:10:46,383] [INFO] [config.py:1004:print]   amp_params ................... False
[2025-04-01 05:10:46,383] [INFO] [config.py:1004:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-01 05:10:46,383] [INFO] [config.py:1004:print]   bfloat16_enabled ............. False
[2025-04-01 05:10:46,383] [INFO] [config.py:1004:print]   bfloat16_immediate_grad_update  False
[2025-04-01 05:10:46,383] [INFO] [config.py:1004:print]   checkpoint_parallel_write_pipeline  False
[2025-04-01 05:10:46,383] [INFO] [config.py:1004:print]   checkpoint_tag_validation_enabled  True
[2025-04-01 05:10:46,383] [INFO] [config.py:1004:print]   checkpoint_tag_validation_fail  False
[2025-04-01 05:10:46,383] [INFO] [config.py:1004:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fe2230102c0>
[2025-04-01 05:10:46,383] [INFO] [config.py:1004:print]   communication_data_type ...... None
[2025-04-01 05:10:46,383] [INFO] [config.py:1004:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-01 05:10:46,383] [INFO] [config.py:1004:print]   curriculum_enabled_legacy .... False
[2025-04-01 05:10:46,383] [INFO] [config.py:1004:print]   curriculum_params_legacy ..... False
[2025-04-01 05:10:46,383] [INFO] [config.py:1004:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-01 05:10:46,383] [INFO] [config.py:1004:print]   data_efficiency_enabled ...... False
[2025-04-01 05:10:46,383] [INFO] [config.py:1004:print]   dataloader_drop_last ......... False
[2025-04-01 05:10:46,383] [INFO] [config.py:1004:print]   disable_allgather ............ False
[2025-04-01 05:10:46,383] [INFO] [config.py:1004:print]   dump_state ................... False
[2025-04-01 05:10:46,383] [INFO] [config.py:1004:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-04-01 05:10:46,383] [INFO] [config.py:1004:print]   eigenvalue_enabled ........... False
[2025-04-01 05:10:46,383] [INFO] [config.py:1004:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-01 05:10:46,383] [INFO] [config.py:1004:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-01 05:10:46,383] [INFO] [config.py:1004:print]   eigenvalue_layer_num ......... 0
[2025-04-01 05:10:46,383] [INFO] [config.py:1004:print]   eigenvalue_max_iter .......... 100
[2025-04-01 05:10:46,383] [INFO] [config.py:1004:print]   eigenvalue_stability ......... 1e-06
[2025-04-01 05:10:46,383] [INFO] [config.py:1004:print]   eigenvalue_tol ............... 0.01
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   eigenvalue_verbose ........... False
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   elasticity_enabled ........... False
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   fp16_auto_cast ............... False
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   fp16_enabled ................. True
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   fp16_master_weights_and_gradients  False
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   global_rank .................. 0
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   grad_accum_dtype ............. None
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   gradient_accumulation_steps .. 1
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   gradient_clipping ............ 1.0
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   gradient_predivide_factor .... 1.0
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   graph_harvesting ............. False
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   initial_dynamic_scale ........ 65536
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   load_universal_checkpoint .... False
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   loss_scale ................... 0
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   memory_breakdown ............. False
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   mics_hierarchial_params_gather  False
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   mics_shard_size .............. -1
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   optimizer_legacy_fusion ...... False
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   optimizer_name ............... adamw
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   optimizer_params ............. {'lr': 0.0002, 'weight_decay': 0.01}
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   pld_enabled .................. False
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   pld_params ................... False
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   prescale_gradients ........... False
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   scheduler_name ............... None
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   scheduler_params ............. None
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   seq_parallel_communication_data_type  torch.float32
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   sparse_attention ............. None
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   sparse_gradients_enabled ..... False
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   steps_per_print .............. 50
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   timers_config ................ enabled=True synchronized=True
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   train_batch_size ............. 256
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   train_micro_batch_size_per_gpu  256
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   use_data_before_expert_parallel_  False
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   use_node_local_storage ....... False
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   wall_clock_breakdown ......... False
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   weight_quantization_config ... None
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   world_size ................... 1
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   zero_allow_untested_optimizer  False
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   zero_enabled ................. True
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-01 05:10:46,384] [INFO] [config.py:1004:print]   zero_optimization_stage ...... 2
[2025-04-01 05:10:46,385] [INFO] [config.py:990:print_user_config]   json = {
    "train_batch_size": 256, 
    "gradient_accumulation_steps": 1, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0002, 
            "weight_decay": 0.01
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 1000
    }, 
    "zero_optimization": {
        "stage": 2, 
        "contiguous_gradients": true, 
        "overlap_comm": true
    }, 
    "steps_per_print": 50, 
    "gradient_clipping": 1.0
}
****START TRAINING****
image_size: 8, batch_size: 256, timesteps: 1000, time_emb_dim: 256
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s][2025-04-01 05:10:47,410] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:03,  1.02it/s][2025-04-01 05:10:47,480] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:01<00:00,  2.85it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.48it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.36it/s]
/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py:190: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True) # 使用 pd.concat 追加数据
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.35it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.68it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.42it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.37it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.47it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.72it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.69it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.52it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.31it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.51it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.33it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.28it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.31it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.71it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.61it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.46it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.36it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.70it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.60it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.45it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.32it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.71it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.48it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.40it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:01,  3.76it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.09it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.03it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.93it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.33it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.45it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.67it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.45it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.35it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.68it/s][2025-04-01 05:10:53,467] [INFO] [logging.py:107:log_dist] [Rank 0] step=50, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 05:10:53,468] [INFO] [timer.py:264:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=2590.087645635119, CurrSamplesPerSec=3458.0812750995997, MemAllocated=0.02GB, MaxMemAllocated=1.9GB
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.44it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.39it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.40it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.59it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.67it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.48it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.33it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.72it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.77it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.51it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.29it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.67it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.46it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.36it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.37it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.63it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.63it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.59it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.39it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.69it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.74it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.51it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.43it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.62it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.69it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.50it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.42it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.62it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.69it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.51it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.33it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.70it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.75it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.62it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.42it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.67it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.72it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.53it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.39it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  9.15it/s][2025-04-01 05:10:59,596] [INFO] [logging.py:107:log_dist] [Rank 0] step=100, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 05:10:59,597] [INFO] [timer.py:264:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=2651.344308810296, CurrSamplesPerSec=3457.6135818816356, MemAllocated=0.02GB, MaxMemAllocated=1.9GB
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.98it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.75it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/main.py", line 9, in <module>
[rank0]:     train_deepspeed(cfg)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py", line 210, in train_deepspeed
[rank0]:     generate_during_training(model_engine, sample_dir, config, epoch, num_images=config.num_images)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/generate.py", line 240, in generate_during_training_simulation_dif
[rank0]:     pred_noise = model_engine(x, t_batch)
[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/deepspeed/runtime/engine.py", line 2030, in forward
[rank0]:     loss = self.module(*inputs, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank0]:     return inner()
[rank0]:            ^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/unet.py", line 279, in forward
[rank0]:     x = self.up2(x, x2, t_embed)
[rank0]:         ^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/unet.py", line 70, in forward
[rank0]:     x = self.up(x)
[rank0]:         ^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/upsampling.py", line 172, in forward
[rank0]:     return F.interpolate(
[rank0]:            ^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/functional.py", line 4649, in interpolate
[rank0]:     return torch._C._nn.upsample_nearest2d(input, output_size, scale_factors)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 80.00 MiB. GPU 0 has a total capacity of 79.33 GiB of which 6.44 MiB is free. Process 1639346 has 1.55 GiB memory in use. Process 2816995 has 77.75 GiB memory in use. Of the allocated memory 76.86 GiB is allocated by PyTorch, and 62.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W401 05:11:01.920680449 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-04-01 05:11:03,584] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 88817
[2025-04-01 05:11:03,585] [ERROR] [launch.py:325:sigkill_handler] ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '80000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1'] exits with return code = 1
[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ bash scripts/script_normal_dif_test.sh 
[?2004l[2025-04-01 05:11:22,311] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 05:11:24,604] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 05:11:24,604] [INFO] [runner.py:605:main] cmd = /hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None main.py --image_size 8 --batch_size 256 --num_epochs 200 --timesteps 1000 --lr 2e-4 --time_emb_dim 256 --mu1 4 --sigma1 1 --num1 9000 --mu2 10 --sigma2 4 --num2 1000 --samples_dir ./samples --checkpoints_dir ./checkpoints --fp16 --mode train --model_path  --num_images 10000 --simulation_distribution normal --use_different_noise --use_moe --num_experts 4 --moe_hidden_dim 64 --moe_tau 0.1
[2025-04-01 05:11:26,326] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 05:11:28,555] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 05:11:28,555] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 05:11:28,555] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 05:11:28,555] [INFO] [launch.py:164:main] dist_world_size=1
[2025-04-01 05:11:28,555] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 05:11:28,672] [INFO] [launch.py:256:main] process 89645 spawned with command: ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1']
^CTraceback (most recent call last):
  File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/main.py", line 2, in <module>
    from generate import generate_samples
  File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/generate.py", line 3, in <module>
[2025-04-01 05:11:30,138] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 89645
    import torchvision
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torchvision/__init__.py", line 10, in <module>
    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils  # usort:skip
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torchvision/models/__init__.py", line 2, in <module>
    from .convnext import *
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torchvision/models/convnext.py", line 8, in <module>
    from ..ops.misc import Conv2dNormActivation, Permute
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torchvision/ops/__init__.py", line 1, in <module>
    from ._register_onnx_ops import _register_custom_op
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torchvision/ops/_register_onnx_ops.py", line 5, in <module>
    from torch.onnx import symbolic_opset11 as opset11
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/onnx/__init__.py", line 65, in <module>
    from ._type_utils import JitScalarType
  File "<frozen importlib._bootstrap>", line 1354, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1325, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 929, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 990, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1123, in get_code
  File "<frozen importlib._bootstrap_external>", line 752, in _compile_bytecode
KeyboardInterrupt
[2025-04-01 05:11:30,195] [INFO] [launch.py:328:sigkill_handler] Main process received SIGINT, exiting
^CTraceback (most recent call last):
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/subprocess.py", line 1264, in wait
[2025-04-01 05:11:30,278] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 89645
    return self._wait(timeout=timeout)
[2025-04-01 05:11:30,279] [INFO] [launch.py:328:sigkill_handler] Main process received SIGINT, exiting
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/subprocess.py", line 2046, in _wait
Exception ignored in atexit callback: <function matmul_ext_update_autotune_table at 0x7fad34dfcd60>
Traceback (most recent call last):
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 480, in matmul_ext_update_autotune_table
    fp16_matmul._update_autotune_table()
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 458, in _update_autotune_table
    (pid, sts) = self._try_wait(0)
                 ^^^^^^^^^^^^^^^^^
    TritonMatmul._update_autotune_table(__class__.__name__ + "_4d_kernel", __class__._4d_kernel)  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/subprocess.py", line 2004, in _try_wait

  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 180, in _update_autotune_table
    cache_manager = AutotuneCacheManager(cache_key)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 90, in __init__
    TritonCacheDir.warn_if_nfs(self.cache_dir)
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 47, in warn_if_nfs
    if is_nfs_path(cache_dir) and not TritonCacheDir._warning_printed:
       ^^^^^^^^^^^^^^^^^^^^^^
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 30, in is_nfs_path
    (pid, sts) = os.waitpid(self.pid, wait_flags)
    output = subprocess.check_output(['df', '-T', path], encoding='utf-8')
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt

During handling of the above exception, another exception occurred:

   Traceback (most recent call last):
     File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/deepspeed", line 6, in <module>
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^    main()
^^^^^^^^^^  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/deepspeed/launcher/runner.py", line 621, in main
^^^^^^^^^^^^^^^^^
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/subprocess.py", line 466, in check_output
    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,
           ^^^^^^^^^^    result.wait()
^^^^^^^^^^^^^^^^^^^^^  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/subprocess.py", line 1277, in wait
^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/subprocess.py", line 548, in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/subprocess.py", line 1026, in __init__
    self._wait(timeout=sigint_timeout)
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/subprocess.py", line 2040, in _wait
    self._execute_child(args, executable, preexec_fn, close_fds,
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/subprocess.py", line 1906, in _execute_child
    time.sleep(delay)
    part = os.read(errpipe_read, 50000)
KeyboardInterrupt
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/deepspeed/launcher/launch.py", line 332, in sigkill_handler
    sys.exit(1)
SystemExit: 1
^C
[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ ^C[?2004l[?2004h[?2004l
[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ ^C[?2004l[?2004h[?2004l
[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ ^C[?2004l[?2004h[?2004l
[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ bash scripts/script_normal_dif_test.sh 
[?2004l[2025-04-01 05:11:42,613] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 05:11:44,907] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 05:11:44,908] [INFO] [runner.py:605:main] cmd = /hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None main.py --image_size 8 --batch_size 256 --num_epochs 200 --timesteps 1000 --lr 2e-4 --time_emb_dim 256 --mu1 4 --sigma1 1 --num1 9000 --mu2 10 --sigma2 4 --num2 1000 --samples_dir ./samples --checkpoints_dir ./checkpoints --fp16 --mode train --model_path  --num_images 10000 --simulation_distribution normal --use_different_noise --use_moe --num_experts 4 --moe_hidden_dim 64 --moe_tau 0.1
[2025-04-01 05:11:46,638] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 05:11:48,777] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 05:11:48,777] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 05:11:48,778] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 05:11:48,778] [INFO] [launch.py:164:main] dist_world_size=1
[2025-04-01 05:11:48,778] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 05:11:48,894] [INFO] [launch.py:256:main] process 90013 spawned with command: ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1']
[2025-04-01 05:11:51,594] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 05:11:54,441] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.16.5, git-hash=unknown, git-branch=unknown
[2025-04-01 05:11:54,441] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-01 05:11:54,442] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-04-01 05:11:54,447] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 1
[2025-04-01 05:11:55,632] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124/fused_adam/build.ninja...
/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.18758893013000488 seconds
[2025-04-01 05:11:55,825] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-04-01 05:11:55,825] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-04-01 05:11:55,830] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-04-01 05:11:55,830] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-04-01 05:11:55,830] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2025-04-01 05:11:55,831] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000
[2025-04-01 05:11:55,831] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000
[2025-04-01 05:11:55,831] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-04-01 05:11:55,831] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-04-01 05:11:56,200] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-04-01 05:11:56,200] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 05:11:56,201] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.1 GB, percent = 6.7%
[2025-04-01 05:11:56,315] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-04-01 05:11:56,316] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 05:11:56,316] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.1 GB, percent = 6.7%
[2025-04-01 05:11:56,316] [INFO] [stage_1_and_2.py:556:__init__] optimizer state initialized
[2025-04-01 05:11:56,424] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-04-01 05:11:56,424] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 05:11:56,424] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.1 GB, percent = 6.7%
[2025-04-01 05:11:56,426] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-04-01 05:11:56,426] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-04-01 05:11:56,426] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-04-01 05:11:56,426] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 05:11:56,426] [INFO] [config.py:1000:print] DeepSpeedEngine configuration:
[2025-04-01 05:11:56,427] [INFO] [config.py:1004:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-04-01 05:11:56,427] [INFO] [config.py:1004:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-04-01 05:11:56,427] [INFO] [config.py:1004:print]   amp_enabled .................. False
[2025-04-01 05:11:56,427] [INFO] [config.py:1004:print]   amp_params ................... False
[2025-04-01 05:11:56,427] [INFO] [config.py:1004:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-01 05:11:56,427] [INFO] [config.py:1004:print]   bfloat16_enabled ............. False
[2025-04-01 05:11:56,427] [INFO] [config.py:1004:print]   bfloat16_immediate_grad_update  False
[2025-04-01 05:11:56,427] [INFO] [config.py:1004:print]   checkpoint_parallel_write_pipeline  False
[2025-04-01 05:11:56,427] [INFO] [config.py:1004:print]   checkpoint_tag_validation_enabled  True
[2025-04-01 05:11:56,427] [INFO] [config.py:1004:print]   checkpoint_tag_validation_fail  False
[2025-04-01 05:11:56,427] [INFO] [config.py:1004:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f3cff3ad700>
[2025-04-01 05:11:56,427] [INFO] [config.py:1004:print]   communication_data_type ...... None
[2025-04-01 05:11:56,427] [INFO] [config.py:1004:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-01 05:11:56,427] [INFO] [config.py:1004:print]   curriculum_enabled_legacy .... False
[2025-04-01 05:11:56,427] [INFO] [config.py:1004:print]   curriculum_params_legacy ..... False
[2025-04-01 05:11:56,427] [INFO] [config.py:1004:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-01 05:11:56,427] [INFO] [config.py:1004:print]   data_efficiency_enabled ...... False
[2025-04-01 05:11:56,427] [INFO] [config.py:1004:print]   dataloader_drop_last ......... False
[2025-04-01 05:11:56,427] [INFO] [config.py:1004:print]   disable_allgather ............ False
[2025-04-01 05:11:56,427] [INFO] [config.py:1004:print]   dump_state ................... False
[2025-04-01 05:11:56,427] [INFO] [config.py:1004:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-04-01 05:11:56,427] [INFO] [config.py:1004:print]   eigenvalue_enabled ........... False
[2025-04-01 05:11:56,427] [INFO] [config.py:1004:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-01 05:11:56,427] [INFO] [config.py:1004:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-01 05:11:56,427] [INFO] [config.py:1004:print]   eigenvalue_layer_num ......... 0
[2025-04-01 05:11:56,427] [INFO] [config.py:1004:print]   eigenvalue_max_iter .......... 100
[2025-04-01 05:11:56,427] [INFO] [config.py:1004:print]   eigenvalue_stability ......... 1e-06
[2025-04-01 05:11:56,427] [INFO] [config.py:1004:print]   eigenvalue_tol ............... 0.01
[2025-04-01 05:11:56,427] [INFO] [config.py:1004:print]   eigenvalue_verbose ........... False
[2025-04-01 05:11:56,427] [INFO] [config.py:1004:print]   elasticity_enabled ........... False
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   fp16_auto_cast ............... False
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   fp16_enabled ................. True
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   fp16_master_weights_and_gradients  False
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   global_rank .................. 0
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   grad_accum_dtype ............. None
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   gradient_accumulation_steps .. 1
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   gradient_clipping ............ 1.0
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   gradient_predivide_factor .... 1.0
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   graph_harvesting ............. False
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   initial_dynamic_scale ........ 65536
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   load_universal_checkpoint .... False
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   loss_scale ................... 0
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   memory_breakdown ............. False
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   mics_hierarchial_params_gather  False
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   mics_shard_size .............. -1
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   optimizer_legacy_fusion ...... False
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   optimizer_name ............... adamw
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   optimizer_params ............. {'lr': 0.0002, 'weight_decay': 0.01}
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   pld_enabled .................. False
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   pld_params ................... False
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   prescale_gradients ........... False
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   scheduler_name ............... None
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   scheduler_params ............. None
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   seq_parallel_communication_data_type  torch.float32
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   sparse_attention ............. None
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   sparse_gradients_enabled ..... False
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   steps_per_print .............. 50
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   timers_config ................ enabled=True synchronized=True
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   train_batch_size ............. 256
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   train_micro_batch_size_per_gpu  256
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   use_data_before_expert_parallel_  False
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   use_node_local_storage ....... False
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   wall_clock_breakdown ......... False
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   weight_quantization_config ... None
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   world_size ................... 1
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   zero_allow_untested_optimizer  False
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   zero_enabled ................. True
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-01 05:11:56,428] [INFO] [config.py:1004:print]   zero_optimization_stage ...... 2
[2025-04-01 05:11:56,428] [INFO] [config.py:990:print_user_config]   json = {
    "train_batch_size": 256, 
    "gradient_accumulation_steps": 1, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0002, 
            "weight_decay": 0.01
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 1000
    }, 
    "zero_optimization": {
        "stage": 2, 
        "contiguous_gradients": true, 
        "overlap_comm": true
    }, 
    "steps_per_print": 50, 
    "gradient_clipping": 1.0
}
****START TRAINING****
image_size: 8, batch_size: 256, timesteps: 1000, time_emb_dim: 256
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s][2025-04-01 05:11:57,473] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:03,  1.00it/s][2025-04-01 05:11:57,545] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:01<00:00,  2.95it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.64it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.45it/s]
/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py:190: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True) # 使用 pd.concat 追加数据
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.25it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.44it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.36it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.26it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.33it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.47it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.32it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.24it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.26it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.47it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.52it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.35it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.31it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.52it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.55it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.36it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.32it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.60it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.41it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.41it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.28it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.36it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.24it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.19it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.28it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.69it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.60it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.40it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.21it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.42it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.41it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.24it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:01,  3.68it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.00it/s][2025-04-01 05:12:03,545] [INFO] [logging.py:107:log_dist] [Rank 0] step=50, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 05:12:03,546] [INFO] [timer.py:264:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=2594.989082837073, CurrSamplesPerSec=3260.2817468701505, MemAllocated=0.02GB, MaxMemAllocated=1.9GB
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.96it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.84it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.85it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  9.48it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 11.09it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.93it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.81it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  9.30it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.90it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.76it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.34it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.62it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.42it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.35it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.32it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.53it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.43it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.31it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.38it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.56it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.39it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.31it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.28it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.48it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.33it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.25it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.32it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.66it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.46it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.35it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.31it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.58it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.47it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.36it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.30it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.51it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.28it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.23it/s]
  0%|                                                                                                                  | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▏                                                                                    | 1/5 [00:00<00:00,  4.34it/s] 60%|███████████████████████████████████████████████████████████████▌                                          | 3/5 [00:00<00:00,  8.41it/s][2025-04-01 05:12:09,761] [INFO] [logging.py:107:log_dist] [Rank 0] step=100, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 05:12:09,763] [INFO] [timer.py:264:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=2636.2450383337286, CurrSamplesPerSec=3352.476318132531, MemAllocated=0.02GB, MaxMemAllocated=1.9GB
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.26it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.22it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/main.py", line 9, in <module>
[rank0]:     train_deepspeed(cfg)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py", line 210, in train_deepspeed
[rank0]:     generate_during_training(model_engine, sample_dir, config, epoch, num_images=config.num_images//config.image_size)
[rank0]:   File "/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/generate.py", line 249, in generate_during_training_simulation_dif
[rank0]:     mus = torch.stack([expert(dummy_input) for expert in moe.expert_mu]).squeeze(1)  # [num_experts, input_dim]
[rank0]:                        ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
[rank0]:     input = module(input)
[rank0]:             ^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
[rank0]:     return F.linear(input, self.weight, self.bias)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: RuntimeError: mat1 and mat2 must have the same dtype, but got Float and Half
[rank0]:[W401 05:12:11.124079857 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-04-01 05:12:11,897] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 90013
[2025-04-01 05:12:11,898] [ERROR] [launch.py:325:sigkill_handler] ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1'] exits with return code = 1
[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ bash scripts/script_normal_dif_test.sh 
[?2004l[2025-04-01 05:15:20,146] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 05:15:22,478] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 05:15:22,478] [INFO] [runner.py:605:main] cmd = /hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None main.py --image_size 8 --batch_size 256 --num_epochs 200 --timesteps 1000 --lr 2e-4 --time_emb_dim 256 --mu1 4 --sigma1 1 --num1 9000 --mu2 10 --sigma2 4 --num2 1000 --samples_dir ./samples --checkpoints_dir ./checkpoints --fp16 --mode train --model_path  --num_images 10000 --simulation_distribution normal --use_different_noise --use_moe --num_experts 4 --moe_hidden_dim 64 --moe_tau 0.1
[2025-04-01 05:15:24,262] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 05:15:26,449] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 05:15:26,449] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 05:15:26,449] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 05:15:26,449] [INFO] [launch.py:164:main] dist_world_size=1
[2025-04-01 05:15:26,449] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 05:15:26,563] [INFO] [launch.py:256:main] process 90937 spawned with command: ['/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python', '-u', 'main.py', '--local_rank=0', '--image_size', '8', '--batch_size', '256', '--num_epochs', '200', '--timesteps', '1000', '--lr', '2e-4', '--time_emb_dim', '256', '--mu1', '4', '--sigma1', '1', '--num1', '9000', '--mu2', '10', '--sigma2', '4', '--num2', '1000', '--samples_dir', './samples', '--checkpoints_dir', './checkpoints', '--fp16', '--mode', 'train', '--model_path', '', '--num_images', '10000', '--simulation_distribution', 'normal', '--use_different_noise', '--use_moe', '--num_experts', '4', '--moe_hidden_dim', '64', '--moe_tau', '0.1']
[2025-04-01 05:15:29,271] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 05:15:32,240] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.16.5, git-hash=unknown, git-branch=unknown
[2025-04-01 05:15:32,240] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-01 05:15:32,240] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-04-01 05:15:32,243] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 1
[2025-04-01 05:15:33,421] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /hpc2hdd/home/aliu789/.cache/torch_extensions/py312_cu124/fused_adam/build.ninja...
/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.19634389877319336 seconds
[2025-04-01 05:15:33,622] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-04-01 05:15:33,623] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-04-01 05:15:33,627] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-04-01 05:15:33,628] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-04-01 05:15:33,628] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2025-04-01 05:15:33,628] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000
[2025-04-01 05:15:33,628] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000
[2025-04-01 05:15:33,628] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-04-01 05:15:33,628] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-04-01 05:15:33,979] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-04-01 05:15:33,980] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 05:15:33,980] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.15 GB, percent = 6.7%
[2025-04-01 05:15:34,092] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-04-01 05:15:34,092] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 05:15:34,093] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.16 GB, percent = 6.7%
[2025-04-01 05:15:34,093] [INFO] [stage_1_and_2.py:556:__init__] optimizer state initialized
[2025-04-01 05:15:34,197] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-04-01 05:15:34,198] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB 
[2025-04-01 05:15:34,198] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.16 GB, percent = 6.7%
[2025-04-01 05:15:34,200] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-04-01 05:15:34,200] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-04-01 05:15:34,200] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-04-01 05:15:34,200] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 05:15:34,200] [INFO] [config.py:1000:print] DeepSpeedEngine configuration:
[2025-04-01 05:15:34,200] [INFO] [config.py:1004:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-04-01 05:15:34,200] [INFO] [config.py:1004:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-04-01 05:15:34,200] [INFO] [config.py:1004:print]   amp_enabled .................. False
[2025-04-01 05:15:34,200] [INFO] [config.py:1004:print]   amp_params ................... False
[2025-04-01 05:15:34,201] [INFO] [config.py:1004:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-01 05:15:34,201] [INFO] [config.py:1004:print]   bfloat16_enabled ............. False
[2025-04-01 05:15:34,201] [INFO] [config.py:1004:print]   bfloat16_immediate_grad_update  False
[2025-04-01 05:15:34,201] [INFO] [config.py:1004:print]   checkpoint_parallel_write_pipeline  False
[2025-04-01 05:15:34,201] [INFO] [config.py:1004:print]   checkpoint_tag_validation_enabled  True
[2025-04-01 05:15:34,201] [INFO] [config.py:1004:print]   checkpoint_tag_validation_fail  False
[2025-04-01 05:15:34,201] [INFO] [config.py:1004:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f88ef1f4800>
[2025-04-01 05:15:34,201] [INFO] [config.py:1004:print]   communication_data_type ...... None
[2025-04-01 05:15:34,201] [INFO] [config.py:1004:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-01 05:15:34,201] [INFO] [config.py:1004:print]   curriculum_enabled_legacy .... False
[2025-04-01 05:15:34,201] [INFO] [config.py:1004:print]   curriculum_params_legacy ..... False
[2025-04-01 05:15:34,201] [INFO] [config.py:1004:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-01 05:15:34,201] [INFO] [config.py:1004:print]   data_efficiency_enabled ...... False
[2025-04-01 05:15:34,201] [INFO] [config.py:1004:print]   dataloader_drop_last ......... False
[2025-04-01 05:15:34,201] [INFO] [config.py:1004:print]   disable_allgather ............ False
[2025-04-01 05:15:34,201] [INFO] [config.py:1004:print]   dump_state ................... False
[2025-04-01 05:15:34,201] [INFO] [config.py:1004:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-04-01 05:15:34,201] [INFO] [config.py:1004:print]   eigenvalue_enabled ........... False
[2025-04-01 05:15:34,201] [INFO] [config.py:1004:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-01 05:15:34,201] [INFO] [config.py:1004:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-01 05:15:34,201] [INFO] [config.py:1004:print]   eigenvalue_layer_num ......... 0
[2025-04-01 05:15:34,201] [INFO] [config.py:1004:print]   eigenvalue_max_iter .......... 100
[2025-04-01 05:15:34,201] [INFO] [config.py:1004:print]   eigenvalue_stability ......... 1e-06
[2025-04-01 05:15:34,201] [INFO] [config.py:1004:print]   eigenvalue_tol ............... 0.01
[2025-04-01 05:15:34,201] [INFO] [config.py:1004:print]   eigenvalue_verbose ........... False
[2025-04-01 05:15:34,201] [INFO] [config.py:1004:print]   elasticity_enabled ........... False
[2025-04-01 05:15:34,201] [INFO] [config.py:1004:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-04-01 05:15:34,201] [INFO] [config.py:1004:print]   fp16_auto_cast ............... False
[2025-04-01 05:15:34,201] [INFO] [config.py:1004:print]   fp16_enabled ................. True
[2025-04-01 05:15:34,201] [INFO] [config.py:1004:print]   fp16_master_weights_and_gradients  False
[2025-04-01 05:15:34,201] [INFO] [config.py:1004:print]   global_rank .................. 0
[2025-04-01 05:15:34,201] [INFO] [config.py:1004:print]   grad_accum_dtype ............. None
[2025-04-01 05:15:34,201] [INFO] [config.py:1004:print]   gradient_accumulation_steps .. 1
[2025-04-01 05:15:34,201] [INFO] [config.py:1004:print]   gradient_clipping ............ 1.0
[2025-04-01 05:15:34,201] [INFO] [config.py:1004:print]   gradient_predivide_factor .... 1.0
[2025-04-01 05:15:34,201] [INFO] [config.py:1004:print]   graph_harvesting ............. False
[2025-04-01 05:15:34,201] [INFO] [config.py:1004:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-01 05:15:34,201] [INFO] [config.py:1004:print]   initial_dynamic_scale ........ 65536
[2025-04-01 05:15:34,201] [INFO] [config.py:1004:print]   load_universal_checkpoint .... False
[2025-04-01 05:15:34,201] [INFO] [config.py:1004:print]   loss_scale ................... 0
[2025-04-01 05:15:34,201] [INFO] [config.py:1004:print]   memory_breakdown ............. False
[2025-04-01 05:15:34,201] [INFO] [config.py:1004:print]   mics_hierarchial_params_gather  False
[2025-04-01 05:15:34,202] [INFO] [config.py:1004:print]   mics_shard_size .............. -1
[2025-04-01 05:15:34,202] [INFO] [config.py:1004:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-04-01 05:15:34,202] [INFO] [config.py:1004:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-04-01 05:15:34,202] [INFO] [config.py:1004:print]   optimizer_legacy_fusion ...... False
[2025-04-01 05:15:34,202] [INFO] [config.py:1004:print]   optimizer_name ............... adamw
[2025-04-01 05:15:34,202] [INFO] [config.py:1004:print]   optimizer_params ............. {'lr': 0.0002, 'weight_decay': 0.01}
[2025-04-01 05:15:34,202] [INFO] [config.py:1004:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-04-01 05:15:34,202] [INFO] [config.py:1004:print]   pld_enabled .................. False
[2025-04-01 05:15:34,202] [INFO] [config.py:1004:print]   pld_params ................... False
[2025-04-01 05:15:34,202] [INFO] [config.py:1004:print]   prescale_gradients ........... False
[2025-04-01 05:15:34,202] [INFO] [config.py:1004:print]   scheduler_name ............... None
[2025-04-01 05:15:34,202] [INFO] [config.py:1004:print]   scheduler_params ............. None
[2025-04-01 05:15:34,202] [INFO] [config.py:1004:print]   seq_parallel_communication_data_type  torch.float32
[2025-04-01 05:15:34,202] [INFO] [config.py:1004:print]   sparse_attention ............. None
[2025-04-01 05:15:34,202] [INFO] [config.py:1004:print]   sparse_gradients_enabled ..... False
[2025-04-01 05:15:34,202] [INFO] [config.py:1004:print]   steps_per_print .............. 50
[2025-04-01 05:15:34,202] [INFO] [config.py:1004:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-04-01 05:15:34,202] [INFO] [config.py:1004:print]   timers_config ................ enabled=True synchronized=True
[2025-04-01 05:15:34,202] [INFO] [config.py:1004:print]   train_batch_size ............. 256
[2025-04-01 05:15:34,202] [INFO] [config.py:1004:print]   train_micro_batch_size_per_gpu  256
[2025-04-01 05:15:34,202] [INFO] [config.py:1004:print]   use_data_before_expert_parallel_  False
[2025-04-01 05:15:34,202] [INFO] [config.py:1004:print]   use_node_local_storage ....... False
[2025-04-01 05:15:34,202] [INFO] [config.py:1004:print]   wall_clock_breakdown ......... False
[2025-04-01 05:15:34,202] [INFO] [config.py:1004:print]   weight_quantization_config ... None
[2025-04-01 05:15:34,202] [INFO] [config.py:1004:print]   world_size ................... 1
[2025-04-01 05:15:34,202] [INFO] [config.py:1004:print]   zero_allow_untested_optimizer  False
[2025-04-01 05:15:34,202] [INFO] [config.py:1004:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-04-01 05:15:34,202] [INFO] [config.py:1004:print]   zero_enabled ................. True
[2025-04-01 05:15:34,202] [INFO] [config.py:1004:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-01 05:15:34,202] [INFO] [config.py:1004:print]   zero_optimization_stage ...... 2
[2025-04-01 05:15:34,202] [INFO] [config.py:990:print_user_config]   json = {
    "train_batch_size": 256, 
    "gradient_accumulation_steps": 1, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0002, 
            "weight_decay": 0.01
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 1000
    }, 
    "zero_optimization": {
        "stage": 2, 
        "contiguous_gradients": true, 
        "overlap_comm": true
    }, 
    "steps_per_print": 50, 
    "gradient_clipping": 1.0
}
****START TRAINING****
image_size: 8, batch_size: 256, timesteps: 1000, time_emb_dim: 256
  0%|                                                                                                                   | 0/5 [00:00<?, ?it/s][2025-04-01 05:15:35,335] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
 20%|█████████████████████▍                                                                                     | 1/5 [00:01<00:04,  1.09s/it][2025-04-01 05:15:35,403] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
 60%|████████████████████████████████████████████████████████████████▏                                          | 3/5 [00:01<00:00,  2.80it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.44it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.27it/s]
/hpc2hdd/home/aliu789/data/ywj/diffusion_noise_agent/train.py:190: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True) # 使用 pd.concat 追加数据
  0%|                                                                                                                   | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▍                                                                                     | 1/5 [00:00<00:00,  4.39it/s] 60%|████████████████████████████████████████████████████████████████▏                                          | 3/5 [00:00<00:00,  8.59it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.52it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.36it/s]
  0%|                                                                                                                   | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▍                                                                                     | 1/5 [00:00<00:00,  4.28it/s] 60%|████████████████████████████████████████████████████████████████▏                                          | 3/5 [00:00<00:00,  8.60it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.39it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.31it/s]
  0%|                                                                                                                   | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▍                                                                                     | 1/5 [00:00<00:00,  4.36it/s] 60%|████████████████████████████████████████████████████████████████▏                                          | 3/5 [00:00<00:00,  8.61it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.54it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.39it/s]
  0%|                                                                                                                   | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▍                                                                                     | 1/5 [00:00<00:00,  4.29it/s] 60%|████████████████████████████████████████████████████████████████▏                                          | 3/5 [00:00<00:00,  8.48it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.38it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.27it/s]
  0%|                                                                                                                   | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▍                                                                                     | 1/5 [00:00<00:00,  4.35it/s] 60%|████████████████████████████████████████████████████████████████▏                                          | 3/5 [00:00<00:00,  8.81it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.75it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.51it/s]
  0%|                                                                                                                   | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▍                                                                                     | 1/5 [00:00<00:00,  4.35it/s] 60%|████████████████████████████████████████████████████████████████▏                                          | 3/5 [00:00<00:00,  8.53it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.37it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.30it/s]
  0%|                                                                                                                   | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▍                                                                                     | 1/5 [00:00<00:00,  4.34it/s] 60%|████████████████████████████████████████████████████████████████▏                                          | 3/5 [00:00<00:00,  8.65it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.62it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.52it/s]
  0%|                                                                                                                   | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▍                                                                                     | 1/5 [00:00<00:00,  4.22it/s] 60%|████████████████████████████████████████████████████████████████▏                                          | 3/5 [00:00<00:00,  8.41it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.39it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.34it/s]
  0%|                                                                                                                   | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▍                                                                                     | 1/5 [00:00<00:00,  4.41it/s] 60%|████████████████████████████████████████████████████████████████▏                                          | 3/5 [00:00<00:00,  8.60it/s][2025-04-01 05:15:41,311] [INFO] [logging.py:107:log_dist] [Rank 0] step=50, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 05:15:41,313] [INFO] [timer.py:264:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=2638.711043427709, CurrSamplesPerSec=3341.500847193799, MemAllocated=0.02GB, MaxMemAllocated=1.9GB
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.45it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.41it/s]
  0%|                                                                                                                   | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▍                                                                                     | 1/5 [00:00<00:00,  4.77it/s] 60%|████████████████████████████████████████████████████████████████▏                                          | 3/5 [00:00<00:00,  8.98it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.66it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.59it/s]
  0%|                                                                                                                   | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▍                                                                                     | 1/5 [00:00<00:00,  4.87it/s] 60%|████████████████████████████████████████████████████████████████▏                                          | 3/5 [00:00<00:00,  9.19it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.87it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.83it/s]
  0%|                                                                                                                   | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▍                                                                                     | 1/5 [00:00<00:00,  4.38it/s] 60%|████████████████████████████████████████████████████████████████▏                                          | 3/5 [00:00<00:00,  8.57it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.65it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.45it/s]
  0%|                                                                                                                   | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▍                                                                                     | 1/5 [00:00<00:00,  4.31it/s] 60%|████████████████████████████████████████████████████████████████▏                                          | 3/5 [00:00<00:00,  8.50it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.39it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.38it/s]
  0%|                                                                                                                   | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▍                                                                                     | 1/5 [00:00<00:00,  4.32it/s] 60%|████████████████████████████████████████████████████████████████▏                                          | 3/5 [00:00<00:00,  8.71it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.75it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.56it/s]
  0%|                                                                                                                   | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▍                                                                                     | 1/5 [00:00<00:00,  4.68it/s] 60%|████████████████████████████████████████████████████████████████▏                                          | 3/5 [00:00<00:00,  8.78it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.80it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.81it/s]
  0%|                                                                                                                   | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▍                                                                                     | 1/5 [00:00<00:00,  4.35it/s] 60%|████████████████████████████████████████████████████████████████▏                                          | 3/5 [00:00<00:00,  8.71it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.47it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.39it/s]
  0%|                                                                                                                   | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▍                                                                                     | 1/5 [00:00<00:00,  4.35it/s] 60%|████████████████████████████████████████████████████████████████▏                                          | 3/5 [00:00<00:00,  8.60it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.60it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.42it/s]
  0%|                                                                                                                   | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▍                                                                                     | 1/5 [00:00<00:00,  4.30it/s] 60%|████████████████████████████████████████████████████████████████▏                                          | 3/5 [00:00<00:00,  8.48it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.50it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.34it/s]
  0%|                                                                                                                   | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▍                                                                                     | 1/5 [00:00<00:00,  4.74it/s] 60%|████████████████████████████████████████████████████████████████▏                                          | 3/5 [00:00<00:00,  9.03it/s][2025-04-01 05:15:47,413] [INFO] [logging.py:107:log_dist] [Rank 0] step=100, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 05:15:47,414] [INFO] [timer.py:264:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=2678.889284165241, CurrSamplesPerSec=3423.559251701011, MemAllocated=0.02GB, MaxMemAllocated=1.9GB
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.82it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.66it/s]
  0%|                                                                                                                   | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▍                                                                                     | 1/5 [00:00<00:01,  3.47it/s] 60%|████████████████████████████████████████████████████████████████▏                                          | 3/5 [00:00<00:00,  7.47it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.59it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.44it/s]
  0%|                                                                                                                   | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▍                                                                                     | 1/5 [00:00<00:01,  3.94it/s] 60%|████████████████████████████████████████████████████████████████▏                                          | 3/5 [00:00<00:00,  7.92it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.83it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.75it/s]
  0%|                                                                                                                   | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▍                                                                                     | 1/5 [00:00<00:01,  3.93it/s] 60%|████████████████████████████████████████████████████████████████▏                                          | 3/5 [00:00<00:00,  8.04it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.89it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.87it/s]
  0%|                                                                                                                   | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▍                                                                                     | 1/5 [00:00<00:01,  3.88it/s] 60%|████████████████████████████████████████████████████████████████▏                                          | 3/5 [00:00<00:00,  7.97it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.95it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.80it/s]
  0%|                                                                                                                   | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▍                                                                                     | 1/5 [00:00<00:01,  3.85it/s] 60%|████████████████████████████████████████████████████████████████▏                                          | 3/5 [00:00<00:00,  7.90it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.88it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.90it/s]
  0%|                                                                                                                   | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▍                                                                                     | 1/5 [00:00<00:01,  3.46it/s] 60%|████████████████████████████████████████████████████████████████▏                                          | 3/5 [00:00<00:00,  7.40it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.44it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.37it/s]
  0%|                                                                                                                   | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▍                                                                                     | 1/5 [00:00<00:01,  3.94it/s] 60%|████████████████████████████████████████████████████████████████▏                                          | 3/5 [00:00<00:00,  8.00it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.07it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.87it/s]
  0%|                                                                                                                   | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▍                                                                                     | 1/5 [00:00<00:01,  3.90it/s] 60%|████████████████████████████████████████████████████████████████▏                                          | 3/5 [00:00<00:00,  8.02it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.91it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.78it/s]
  0%|                                                                                                                   | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▍                                                                                     | 1/5 [00:00<00:01,  3.88it/s] 60%|████████████████████████████████████████████████████████████████▏                                          | 3/5 [00:00<00:00,  8.05it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.95it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.81it/s]
  0%|                                                                                                                   | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▍                                                                                     | 1/5 [00:00<00:01,  3.85it/s] 60%|████████████████████████████████████████████████████████████████▏                                          | 3/5 [00:00<00:00,  7.88it/s][2025-04-01 05:16:03,219] [INFO] [logging.py:107:log_dist] [Rank 0] step=150, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 05:16:03,221] [INFO] [timer.py:264:stop] epoch=0/micro_step=150/global_step=150, RunningAvgSamplesPerSec=2615.874586661014, CurrSamplesPerSec=3345.1652393460504, MemAllocated=0.02GB, MaxMemAllocated=25.12GB
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.80it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.70it/s]
  0%|                                                                                                                   | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▍                                                                                     | 1/5 [00:00<00:01,  3.86it/s] 60%|████████████████████████████████████████████████████████████████▏                                          | 3/5 [00:00<00:00,  7.91it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.76it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.68it/s]
  0%|                                                                                                                   | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▍                                                                                     | 1/5 [00:00<00:01,  3.85it/s] 60%|████████████████████████████████████████████████████████████████▏                                          | 3/5 [00:00<00:00,  7.84it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.83it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.93it/s]
  0%|                                                                                                                   | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▍                                                                                     | 1/5 [00:00<00:01,  3.85it/s] 60%|████████████████████████████████████████████████████████████████▏                                          | 3/5 [00:00<00:00,  7.90it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.88it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.75it/s]
  0%|                                                                                                                   | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▍                                                                                     | 1/5 [00:00<00:01,  3.84it/s] 60%|████████████████████████████████████████████████████████████████▏                                          | 3/5 [00:00<00:00,  7.84it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.76it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.67it/s]
  0%|                                                                                                                   | 0/5 [00:00<?, ?it/s] 20%|█████████████████████▍                                                                                     | 1/5 [00:00<00:01,  3.94it/s] 60%|████████████████████████████████████████████████████████████████▏                                          | 3/5 [00:00<00:00,  8.06it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.93it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.82it/s]
  0%|                                                                                              | 0/5 [00:00<?, ?it/s] 20%|█████████████████▏                                                                    | 1/5 [00:00<00:01,  3.88it/s] 60%|███████████████████████████████████████████████████▌                                  | 3/5 [00:00<00:00,  7.94it/s]100%|██████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.99it/s]100%|██████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.84it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.33it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  7.29it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.41it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.28it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.95it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  8.06it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.94it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.81it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.91it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  8.05it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.05it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.89it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.86it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  7.92it/s][2025-04-01 05:16:09,946] [INFO] [logging.py:107:log_dist] [Rank 0] step=200, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 05:16:09,948] [INFO] [timer.py:264:stop] epoch=0/micro_step=200/global_step=200, RunningAvgSamplesPerSec=2580.267575939806, CurrSamplesPerSec=3317.4153126442375, MemAllocated=0.02GB, MaxMemAllocated=25.12GB
100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.87it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.75it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.32it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  7.29it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.29it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.19it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.94it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  8.05it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.81it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.74it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.94it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  8.06it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.00it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.84it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.89it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  8.05it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.18it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.91it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.96it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  8.25it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.14it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.94it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.83it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  7.88it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.86it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.71it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.85it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  7.95it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.86it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.71it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.87it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  7.97it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.87it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.74it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.79it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  7.83it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.89it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.74it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.92it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  7.92it/s][2025-04-01 05:16:27,092] [INFO] [logging.py:107:log_dist] [Rank 0] step=250, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 05:16:27,093] [INFO] [timer.py:264:stop] epoch=0/micro_step=250/global_step=250, RunningAvgSamplesPerSec=2567.9883427543446, CurrSamplesPerSec=3119.8080530460793, MemAllocated=0.02GB, MaxMemAllocated=25.12GB
100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.82it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.73it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.98it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  8.10it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.98it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.87it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.90it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  8.10it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.99it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.52it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.84it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  7.89it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.69it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.64it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.90it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  7.90it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.75it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.66it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.96it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  8.02it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.91it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.79it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.95it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  8.01it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.86it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.75it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.89it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  8.00it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.89it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.76it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.88it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  7.99it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.94it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.77it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.93it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  8.10it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.16it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.92it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.42it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  7.45it/s][2025-04-01 05:16:33,844] [INFO] [logging.py:107:log_dist] [Rank 0] step=300, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 05:16:33,845] [INFO] [timer.py:264:stop] epoch=0/micro_step=300/global_step=300, RunningAvgSamplesPerSec=2557.245063652872, CurrSamplesPerSec=3122.3664090579673, MemAllocated=0.02GB, MaxMemAllocated=25.12GB
100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.47it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.36it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.52it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  7.63it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.45it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.34it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:00,  4.28it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  8.47it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.16it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.06it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.85it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  7.90it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.88it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.71it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.88it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  7.99it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.88it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.75it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.92it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  8.12it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.98it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.81it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.84it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  7.84it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.83it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.71it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.89it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  8.05it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.97it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.79it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.78it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  7.81it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.67it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.60it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:00,  4.32it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  8.45it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.22it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.07it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.75it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  7.84it/s][2025-04-01 05:16:51,192] [INFO] [logging.py:107:log_dist] [Rank 0] step=350, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 05:16:51,194] [INFO] [timer.py:264:stop] epoch=0/micro_step=350/global_step=350, RunningAvgSamplesPerSec=2555.450338399282, CurrSamplesPerSec=3326.8407459048485, MemAllocated=0.02GB, MaxMemAllocated=25.12GB
100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.82it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.70it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:00,  4.37it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  8.63it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.41it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.34it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.87it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  7.97it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.87it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.74it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.85it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  7.89it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.81it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.69it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.92it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  7.88it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.80it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.71it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.86it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  7.91it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.77it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.68it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.81it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  7.81it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.75it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.66it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.86it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  7.91it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.83it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.70it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.90it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  8.01it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.84it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.74it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.86it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  7.85it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.78it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.67it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.90it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  8.06it/s][2025-04-01 05:16:57,902] [INFO] [logging.py:107:log_dist] [Rank 0] step=400, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 05:16:57,903] [INFO] [timer.py:264:stop] epoch=0/micro_step=400/global_step=400, RunningAvgSamplesPerSec=2549.041950433116, CurrSamplesPerSec=3333.895873575444, MemAllocated=0.02GB, MaxMemAllocated=25.12GB
100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.92it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.77it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.24it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  7.27it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.17it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.11it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.86it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  7.96it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.93it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.76it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.81it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  7.81it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.75it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.63it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.81it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  7.91it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.76it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.86it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.91it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  7.96it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.98it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.80it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.79it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  7.77it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.77it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.64it/s]
  0%|                                                                                               | 0/5 [00:00<?, ?it/s] 20%|█████████████████▍                                                                     | 1/5 [00:00<00:01,  3.90it/s] 60%|████████████████████████████████████████████████████▏                                  | 3/5 [00:00<00:00,  8.06it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.94it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.80it/s]
  0%|                                                                                                       | 0/5 [00:00<?, ?it/s] 20%|███████████████████                                                                            | 1/5 [00:00<00:01,  3.82it/s] 60%|█████████████████████████████████████████████████████████                                      | 3/5 [00:00<00:00,  7.97it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.86it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.76it/s]
  0%|                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|███████████████████▍                                                                             | 1/5 [00:00<00:01,  3.88it/s] 60%|██████████████████████████████████████████████████████████▏                                      | 3/5 [00:00<00:00,  8.00it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.01it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.58it/s]
  0%|                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|███████████████████▍                                                                             | 1/5 [00:00<00:01,  3.88it/s] 60%|██████████████████████████████████████████████████████████▏                                      | 3/5 [00:00<00:00,  7.99it/s][2025-04-01 05:17:15,414] [INFO] [logging.py:107:log_dist] [Rank 0] step=450, skipped=2, lr=[0.0002], mom=[(0.9, 0.999)]
[2025-04-01 05:17:15,415] [INFO] [timer.py:264:stop] epoch=0/micro_step=450/global_step=450, RunningAvgSamplesPerSec=2544.25007693395, CurrSamplesPerSec=3264.2661831105665, MemAllocated=0.02GB, MaxMemAllocated=25.12GB
100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.88it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.80it/s]
  0%|                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|███████████████████▍                                                                             | 1/5 [00:00<00:01,  3.94it/s] 60%|██████████████████████████████████████████████████████████▏                                      | 3/5 [00:00<00:00,  7.71it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.80it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.67it/s]
  0%|                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|███████████████████▍                                                                             | 1/5 [00:00<00:01,  3.84it/s] 60%|██████████████████████████████████████████████████████████▏                                      | 3/5 [00:00<00:00,  7.83it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.54it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.24it/s]
  0%|                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|███████████████████▍                                                                             | 1/5 [00:00<00:01,  3.89it/s] 60%|██████████████████████████████████████████████████████████▏                                      | 3/5 [00:00<00:00,  7.90it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.88it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.73it/s]
  0%|                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|███████████████████▍                                                                             | 1/5 [00:00<00:01,  3.92it/s] 60%|██████████████████████████████████████████████████████████▏                                      | 3/5 [00:00<00:00,  8.04it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.01it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.82it/s]
  0%|                                                                                                         | 0/5 [00:00<?, ?it/s] 20%|███████████████████▍                                                                             | 1/5 [00:00<00:01,  3.89it/s] 60%|██████████████████████████████████████████████████████████▏                                      | 3/5 [00:00<00:00,  7.77it/s]^C[2025-04-01 05:17:18,712] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 90937
^CTraceback (most recent call last):
[2025-04-01 05:17:18,869] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 90937
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/subprocess.py", line 1264, in wait
    return self._wait(timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/subprocess.py", line 2046, in _wait
    (pid, sts) = self._try_wait(0)
                 ^^^^^^^^^^^^^^^^^
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/subprocess.py", line 2004, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/deepspeed", line 6, in <module>
    main()
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/deepspeed/launcher/runner.py", line 621, in main
    result.wait()
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/subprocess.py", line 1277, in wait
    self._wait(timeout=sigint_timeout)
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/subprocess.py", line 2040, in _wait
    time.sleep(delay)
KeyboardInterrupt
^CException ignored in atexit callback: <bound method finalize._exitfunc of <class 'weakref.finalize'>>
Traceback (most recent call last):
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/weakref.py", line 666, in _exitfunc
[2025-04-01 05:17:19,009] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 90937
    f()
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/weakref.py", line 590, in __call__
    return info.func(*info.args, **(info.kwargs or {}))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/library.py", line 433, in _del_library
    handle.destroy()
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/_library/utils.py", line 41, in destroy
    self._on_destroy()
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/_library/fake_impl.py", line 67, in deregister_fake_class
    self.lib._destroy()
  File "/hpc2hdd/home/aliu789/anaconda3/envs/ywj/lib/python3.12/site-packages/torch/library.py", line 398, in _destroy
    self.m.reset()
KeyboardInterrupt: 
^C[2025-04-01 05:17:19,143] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 90937
^C[2025-04-01 05:17:19,277] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 90937

[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [2025-04-01 05:17:19,452] [INFO] [launch.py:328:sigkill_handler] Main process received SIGINT, exiting
^C[?2004l[?2004h[?2004l
[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ ^C[?2004l[?2004h[?2004l
[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ ^C[?2004l[?2004h[?2004l
[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ ^C[?2004l[?2004h[?2004l
[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ ^C[?2004l[?2004h[?2004l
[?2004h(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ [K(ywj) aliu789@1d455874780c:~/data/ywj/diffusion_noise_agent$ bash scripts/script_normal_dif_test.sh [K[K[K[K[K[K[K[K[K.sh 
[?2004l[2025-04-01 05:19:53,451] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 05:19:55,750] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 05:19:55,750] [INFO] [runner.py:605:main] cmd = /hpc2hdd/home/aliu789/anaconda3/envs/ywj/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None main.py --image_size 8 --batch_size 256 --num_epochs 1000 --timesteps 1000 --lr 2e-4 --time_emb_dim 256 --mu1 4 --sigma1 1 --num1 9000 --mu2 10 --sigma2 4 --num2 1000 --samples_dir ./samples --checkpoints_dir ./checkpoints --fp16 --mode train --model_path  --num_images 10000 --simulation_distribution normal --use_different_noise --use_moe --num_experts 4 --moe_hidden_dim 64 --moe_tau 0.1
[2025-04-01 05:19:57,503] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
